{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "\n",
    "In this notebook, we attempt to train an inverse dynamics model (IDM). In Reinforcement Learning parlance, an IDM learns to predict $a_t$ given $s_t$ and $s_{t+1}$. In our case, we have access to the MetaDrive simulator and the Waymo dataset. We want to predict the action the car should take in MetaDrive so that the successor state in the simulator is as close as possible to the successor state in the Waymo dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    heading: float\n",
    "    velocity: npt.NDArray[np.float64]\n",
    "\n",
    "\n",
    "Observation: typing.TypeAlias = tuple[State, State]\n",
    "Action: typing.TypeAlias = tuple[float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metadrive\n",
    "from metadrive import MetaDriveEnv\n",
    "import gymnasium as gym\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def deviceof(m: nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the device of the given module\n",
    "    \"\"\"\n",
    "    return next(m.parameters()).device\n",
    "\n",
    "def normalize_angle(angle: float) -> float:\n",
    "    \"\"\"\n",
    "    Normalize the angle to [-pi, pi)\n",
    "    \"\"\"\n",
    "    return (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def get_metadrive_state(env: MetaDriveEnv) -> State:\n",
    "    return State(heading=env.vehicle.heading_theta, velocity=env.vehicle.velocity[:2])\n",
    "\n",
    "def next_state(env: MetaDriveEnv, s: State, a: Action) -> State:\n",
    "    \"\"\"\n",
    "    runs the policy and returns the total reward\n",
    "    \"\"\"\n",
    "    # reset\n",
    "    env.reset()\n",
    "    env.vehicle.set_position(env.vehicle.position, height=0.49)\n",
    "\n",
    "    # allow car to settle\n",
    "    for _ in range(5):\n",
    "        env.step([0,0])\n",
    "\n",
    "    # set the initial state\n",
    "    env.vehicle.set_velocity(s.velocity)\n",
    "    env.vehicle.set_heading_theta(s.heading)\n",
    "    \n",
    "    # run the simulator\n",
    "    env.step(a)\n",
    "\n",
    "    # get the new state\n",
    "    s_prime = get_metadrive_state(env)\n",
    "\n",
    "    # allow car to settle (if rendering)\n",
    "    if env.config.use_render:\n",
    "        for _ in range(10):\n",
    "            env.step([0,0])\n",
    "\n",
    "    return s_prime\n",
    "\n",
    "def gen_scenario() -> tuple[State, Action]:\n",
    "    \"\"\"\n",
    "    Generates a random scenario\n",
    "    \"\"\"\n",
    "    # generate a random state\n",
    "    velocity = np.random.multivariate_normal([0, 0], np.eye(2) * 100)\n",
    "    heading = normalize_angle(np.arctan2(velocity[1], velocity[0]) + np.random.normal(0, np.pi/4))\n",
    "\n",
    "    s = State(heading=heading, velocity=velocity)\n",
    "\n",
    "    # generate a random action\n",
    "    steer = np.random.uniform(-1, 1)\n",
    "    throttle = np.random.uniform(-1, 1)\n",
    "    a = (steer, throttle)\n",
    "\n",
    "    return s, a\n",
    "\n",
    "def state_batch_to_tensor(states: list[State], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the state from State to a tensor of shape (batch_size, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(np.stack([\n",
    "        [st.velocity[0], st.velocity[1], st.heading] for st in states\n",
    "    ]), dtype=torch.float32, device=device)\n",
    "\n",
    "def action_batch_to_tensor(actions: list[Action], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the action from Action to a tensor of shape (batch_size, 2)\n",
    "    \"\"\"\n",
    "    return torch.tensor(np.stack(actions), dtype=torch.float32, device=device)\n",
    "\n",
    "def obs_batch_to_tensor(obs: list[Observation], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the observation from tuple[State, State] to a tensor of shape (batch_size, 3, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    for st0, st1 in obs:\n",
    "        observations.append(np.array([\n",
    "            [st0.velocity[0], st1.velocity[0]], \n",
    "            [st0.velocity[1], st1.velocity[1]],\n",
    "            [st0.heading, st1.heading]\n",
    "        ]))\n",
    "\n",
    "    return torch.tensor(np.stack(observations), dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task may take a few minutes to run the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): (warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain :task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "default for AsyncTaskManager TaskManager\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from metadrive import MetaDriveEnv\n",
    "\n",
    "MAX_WORKERS = 16\n",
    "DATASET_SIZE = 10000\n",
    "\n",
    "def generate_data(n_scenarios: int) -> list[tuple[State, Action, State]]:\n",
    "    env = MetaDriveEnv(config={\"on_continuous_line_done\": False, \"use_render\": False})\n",
    "    dataset: list[tuple[State, Action, State]] = []\n",
    "    for _ in range(n_scenarios):\n",
    "        s0, a = gen_scenario()\n",
    "        s1 = next_state(env, s0, a)\n",
    "        dataset.append((s0, a, s1))\n",
    "    env.close()\n",
    "    return dataset\n",
    "\n",
    "mm_train_data: list[tuple[State, Action, State]] = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    batch_size, leftover_size = divmod(DATASET_SIZE, MAX_WORKERS)\n",
    "    for batch in executor.map(generate_data, [*[batch_size]*MAX_WORKERS, leftover_size]):\n",
    "        mm_train_data.extend(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    }
   ],
   "source": [
    "mm_validation_data = generate_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mm_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining data:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(mm_train_data))\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mvalidation data:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(mm_validation_data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mm_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"training data:\", len(mm_train_data))\n",
    "print(\"validation data:\", len(mm_validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create a model that attempts to predict the next state given the current state and the action: (throttle and steering)\n",
    "# each state contains: velocity_x, velocity_y, and heading\n",
    "class MetadriveModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input shape: (batch_size, 3) + (batch_size, 2) = (batch_size, 5)\n",
    "        # output shape: (batch_size, 3)\n",
    "        self.fc1 = nn.Linear(5, 512) # Bx5 -> Bx512\n",
    "        self.fc2 = nn.Linear(512, 512) # Bx512 -> Bx256\n",
    "        self.fc3 = nn.Linear(512, 3) # Bx256 -> Bx3\n",
    "    \n",
    "    def forward(self, states: torch.Tensor, actions: torch.Tensor):\n",
    "        x = torch.cat([states, actions], dim=1) # Bx5\n",
    "        x = F.relu(self.fc1(x)) # Bx512\n",
    "        x = F.relu(self.fc2(x)) # Bx512\n",
    "        x = self.fc3(x) # Bx3\n",
    "        return x\n",
    "\n",
    "def metadrive_model_train_batch(\n",
    "    mm: MetadriveModel,\n",
    "    mm_optimizer: torch.optim.Optimizer,\n",
    "    s0_batch: list[State],\n",
    "    a_batch: list[Action],\n",
    "    s1_batch: list[State],\n",
    ") -> float: \n",
    "    device = deviceof(mm)\n",
    "\n",
    "    s0_tensor = state_batch_to_tensor(s0_batch, device) \n",
    "    a_tensor = action_batch_to_tensor(a_batch, device)\n",
    "    s1_tensor = state_batch_to_tensor(s1_batch, device)\n",
    "\n",
    "    mm_optimizer.zero_grad()\n",
    "    s1_pred_tensor = mm(s0_tensor, a_tensor)\n",
    "    loss = F.mse_loss(s1_pred_tensor, s1_tensor)\n",
    "    loss.backward()\n",
    "    mm_optimizer.step()\n",
    "    return float(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optimizer: torch.optim.Optimizer, lr: float) -> None:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# make sure we don't run out of data\n",
    "mm_train_iter = itertools.cycle(mm_train_data)\n",
    "\n",
    "mm = MetadriveModel().to(device)\n",
    "\n",
    "mm_optimizer = torch.optim.Adam(mm.parameters())\n",
    "\n",
    "mm_step = 0\n",
    "mm_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 63.981\n",
      "Step: 2, Loss: 64.212\n",
      "Step: 3, Loss: 63.873\n",
      "Step: 4, Loss: 64.268\n",
      "Step: 5, Loss: 61.975\n",
      "Step: 6, Loss: 63.006\n",
      "Step: 7, Loss: 62.846\n",
      "Step: 8, Loss: 62.234\n",
      "Step: 9, Loss: 61.594\n",
      "Step: 10, Loss: 61.173\n",
      "Step: 11, Loss: 61.655\n",
      "Step: 12, Loss: 60.016\n",
      "Step: 13, Loss: 60.299\n",
      "Step: 14, Loss: 60.424\n",
      "Step: 15, Loss: 59.961\n",
      "Step: 16, Loss: 58.798\n",
      "Step: 17, Loss: 59.180\n",
      "Step: 18, Loss: 58.973\n",
      "Step: 19, Loss: 58.128\n",
      "Step: 20, Loss: 57.874\n",
      "Step: 21, Loss: 57.767\n",
      "Step: 22, Loss: 57.869\n",
      "Step: 23, Loss: 56.185\n",
      "Step: 24, Loss: 56.960\n",
      "Step: 25, Loss: 56.608\n",
      "Step: 26, Loss: 56.340\n",
      "Step: 27, Loss: 55.405\n",
      "Step: 28, Loss: 55.216\n",
      "Step: 29, Loss: 55.582\n",
      "Step: 30, Loss: 54.201\n",
      "Step: 31, Loss: 54.504\n",
      "Step: 32, Loss: 54.266\n",
      "Step: 33, Loss: 54.140\n",
      "Step: 34, Loss: 52.700\n",
      "Step: 35, Loss: 53.540\n",
      "Step: 36, Loss: 52.912\n",
      "Step: 37, Loss: 52.653\n",
      "Step: 38, Loss: 52.160\n",
      "Step: 39, Loss: 51.739\n",
      "Step: 40, Loss: 52.050\n",
      "Step: 41, Loss: 50.691\n",
      "Step: 42, Loss: 50.995\n",
      "Step: 43, Loss: 51.039\n",
      "Step: 44, Loss: 50.677\n",
      "Step: 45, Loss: 49.689\n",
      "Step: 46, Loss: 49.672\n",
      "Step: 47, Loss: 49.928\n",
      "Step: 48, Loss: 48.679\n",
      "Step: 49, Loss: 48.806\n",
      "Step: 50, Loss: 48.627\n",
      "Step: 51, Loss: 48.795\n",
      "Step: 52, Loss: 47.110\n",
      "Step: 53, Loss: 47.892\n",
      "Step: 54, Loss: 47.580\n",
      "Step: 55, Loss: 47.237\n",
      "Step: 56, Loss: 46.681\n",
      "Step: 57, Loss: 46.194\n",
      "Step: 58, Loss: 46.776\n",
      "Step: 59, Loss: 45.231\n",
      "Step: 60, Loss: 45.653\n",
      "Step: 61, Loss: 45.568\n",
      "Step: 62, Loss: 45.273\n",
      "Step: 63, Loss: 44.272\n",
      "Step: 64, Loss: 44.478\n",
      "Step: 65, Loss: 44.409\n",
      "Step: 66, Loss: 43.579\n",
      "Step: 67, Loss: 43.542\n",
      "Step: 68, Loss: 43.324\n",
      "Step: 69, Loss: 43.405\n",
      "Step: 70, Loss: 42.063\n",
      "Step: 71, Loss: 42.617\n",
      "Step: 72, Loss: 42.390\n",
      "Step: 73, Loss: 41.954\n",
      "Step: 74, Loss: 41.365\n",
      "Step: 75, Loss: 41.204\n",
      "Step: 76, Loss: 41.411\n",
      "Step: 77, Loss: 40.292\n",
      "Step: 78, Loss: 40.450\n",
      "Step: 79, Loss: 40.338\n",
      "Step: 80, Loss: 40.180\n",
      "Step: 81, Loss: 39.076\n",
      "Step: 82, Loss: 39.587\n",
      "Step: 83, Loss: 39.226\n",
      "Step: 84, Loss: 38.771\n",
      "Step: 85, Loss: 38.459\n",
      "Step: 86, Loss: 38.221\n",
      "Step: 87, Loss: 38.323\n",
      "Step: 88, Loss: 37.196\n",
      "Step: 89, Loss: 37.570\n",
      "Step: 90, Loss: 37.351\n",
      "Step: 91, Loss: 37.088\n",
      "Step: 92, Loss: 36.428\n",
      "Step: 93, Loss: 36.301\n",
      "Step: 94, Loss: 36.383\n",
      "Step: 95, Loss: 35.465\n",
      "Step: 96, Loss: 35.658\n",
      "Step: 97, Loss: 35.286\n",
      "Step: 98, Loss: 35.330\n",
      "Step: 99, Loss: 34.251\n",
      "Step: 100, Loss: 34.704\n",
      "Step: 101, Loss: 34.319\n",
      "Step: 102, Loss: 34.069\n",
      "Step: 103, Loss: 33.657\n",
      "Step: 104, Loss: 33.338\n",
      "Step: 105, Loss: 33.516\n",
      "Step: 106, Loss: 32.476\n",
      "Step: 107, Loss: 32.710\n",
      "Step: 108, Loss: 32.637\n",
      "Step: 109, Loss: 32.349\n",
      "Step: 110, Loss: 31.605\n",
      "Step: 111, Loss: 31.647\n",
      "Step: 112, Loss: 31.621\n",
      "Step: 113, Loss: 30.850\n",
      "Step: 114, Loss: 30.916\n",
      "Step: 115, Loss: 30.656\n",
      "Step: 116, Loss: 30.764\n",
      "Step: 117, Loss: 29.572\n",
      "Step: 118, Loss: 30.039\n",
      "Step: 119, Loss: 29.808\n",
      "Step: 120, Loss: 29.450\n",
      "Step: 121, Loss: 29.080\n",
      "Step: 122, Loss: 28.802\n",
      "Step: 123, Loss: 28.937\n",
      "Step: 124, Loss: 28.082\n",
      "Step: 125, Loss: 28.216\n",
      "Step: 126, Loss: 28.056\n",
      "Step: 127, Loss: 27.828\n",
      "Step: 128, Loss: 27.186\n",
      "Step: 129, Loss: 27.309\n",
      "Step: 130, Loss: 27.110\n",
      "Step: 131, Loss: 26.654\n",
      "Step: 132, Loss: 26.457\n",
      "Step: 133, Loss: 26.325\n",
      "Step: 134, Loss: 26.309\n",
      "Step: 135, Loss: 25.419\n",
      "Step: 136, Loss: 25.743\n",
      "Step: 137, Loss: 25.477\n",
      "Step: 138, Loss: 25.254\n",
      "Step: 139, Loss: 24.778\n",
      "Step: 140, Loss: 24.627\n",
      "Step: 141, Loss: 24.715\n",
      "Step: 142, Loss: 23.987\n",
      "Step: 143, Loss: 24.072\n",
      "Step: 144, Loss: 23.884\n",
      "Step: 145, Loss: 23.735\n",
      "Step: 146, Loss: 23.062\n",
      "Step: 147, Loss: 23.322\n",
      "Step: 148, Loss: 23.004\n",
      "Step: 149, Loss: 22.759\n",
      "Step: 150, Loss: 22.495\n",
      "Step: 151, Loss: 22.239\n",
      "Step: 152, Loss: 22.310\n",
      "Step: 153, Loss: 21.635\n",
      "Step: 154, Loss: 21.725\n",
      "Step: 155, Loss: 21.616\n",
      "Step: 156, Loss: 21.393\n",
      "Step: 157, Loss: 20.951\n",
      "Step: 158, Loss: 20.816\n",
      "Step: 159, Loss: 20.869\n",
      "Step: 160, Loss: 20.268\n",
      "Step: 161, Loss: 20.274\n",
      "Step: 162, Loss: 20.095\n",
      "Step: 163, Loss: 20.108\n",
      "Step: 164, Loss: 19.340\n",
      "Step: 165, Loss: 19.600\n",
      "Step: 166, Loss: 19.394\n",
      "Step: 167, Loss: 19.161\n",
      "Step: 168, Loss: 18.886\n",
      "Step: 169, Loss: 18.622\n",
      "Step: 170, Loss: 18.782\n",
      "Step: 171, Loss: 18.087\n",
      "Step: 172, Loss: 18.207\n",
      "Step: 173, Loss: 18.087\n",
      "Step: 174, Loss: 17.898\n",
      "Step: 175, Loss: 17.471\n",
      "Step: 176, Loss: 17.443\n",
      "Step: 177, Loss: 17.361\n",
      "Step: 178, Loss: 16.961\n",
      "Step: 179, Loss: 16.910\n",
      "Step: 180, Loss: 16.743\n",
      "Step: 181, Loss: 16.721\n",
      "Step: 182, Loss: 16.121\n",
      "Step: 183, Loss: 16.295\n",
      "Step: 184, Loss: 16.134\n",
      "Step: 185, Loss: 15.906\n",
      "Step: 186, Loss: 15.637\n",
      "Step: 187, Loss: 15.487\n",
      "Step: 188, Loss: 15.525\n",
      "Step: 189, Loss: 15.037\n",
      "Step: 190, Loss: 15.033\n",
      "Step: 191, Loss: 14.938\n",
      "Step: 192, Loss: 14.821\n",
      "Step: 193, Loss: 14.389\n",
      "Step: 194, Loss: 14.500\n",
      "Step: 195, Loss: 14.261\n",
      "Step: 196, Loss: 14.072\n",
      "Step: 197, Loss: 13.924\n",
      "Step: 198, Loss: 13.770\n",
      "Step: 199, Loss: 13.761\n",
      "Step: 200, Loss: 13.278\n",
      "Step: 201, Loss: 13.386\n",
      "Step: 202, Loss: 13.239\n",
      "Step: 203, Loss: 13.090\n",
      "Step: 204, Loss: 12.830\n",
      "Step: 205, Loss: 12.712\n",
      "Step: 206, Loss: 12.705\n",
      "Step: 207, Loss: 12.323\n",
      "Step: 208, Loss: 12.355\n",
      "Step: 209, Loss: 12.172\n",
      "Step: 210, Loss: 12.136\n",
      "Step: 211, Loss: 11.720\n",
      "Step: 212, Loss: 11.844\n",
      "Step: 213, Loss: 11.656\n",
      "Step: 214, Loss: 11.521\n",
      "Step: 215, Loss: 11.344\n",
      "Step: 216, Loss: 11.194\n",
      "Step: 217, Loss: 11.210\n",
      "Step: 218, Loss: 10.813\n",
      "Step: 219, Loss: 10.861\n",
      "Step: 220, Loss: 10.792\n",
      "Step: 221, Loss: 10.644\n",
      "Step: 222, Loss: 10.370\n",
      "Step: 223, Loss: 10.347\n",
      "Step: 224, Loss: 10.308\n",
      "Step: 225, Loss: 9.996\n",
      "Step: 226, Loss: 9.980\n",
      "Step: 227, Loss: 9.871\n",
      "Step: 228, Loss: 9.852\n",
      "Step: 229, Loss: 9.437\n",
      "Step: 230, Loss: 9.568\n",
      "Step: 231, Loss: 9.440\n",
      "Step: 232, Loss: 9.290\n",
      "Step: 233, Loss: 9.145\n",
      "Step: 234, Loss: 9.025\n",
      "Step: 235, Loss: 9.037\n",
      "Step: 236, Loss: 8.711\n",
      "Step: 237, Loss: 8.745\n",
      "Step: 238, Loss: 8.651\n",
      "Step: 239, Loss: 8.552\n",
      "Step: 240, Loss: 8.310\n",
      "Step: 241, Loss: 8.337\n",
      "Step: 242, Loss: 8.231\n",
      "Step: 243, Loss: 8.058\n",
      "Step: 244, Loss: 7.981\n",
      "Step: 245, Loss: 7.908\n",
      "Step: 246, Loss: 7.873\n",
      "Step: 247, Loss: 7.563\n",
      "Step: 248, Loss: 7.652\n",
      "Step: 249, Loss: 7.532\n",
      "Step: 250, Loss: 7.430\n",
      "Step: 251, Loss: 7.267\n",
      "Step: 252, Loss: 7.218\n",
      "Step: 253, Loss: 7.185\n",
      "Step: 254, Loss: 6.952\n",
      "Step: 255, Loss: 6.964\n",
      "Step: 256, Loss: 6.877\n",
      "Step: 257, Loss: 6.815\n",
      "Step: 258, Loss: 6.572\n",
      "Step: 259, Loss: 6.647\n",
      "Step: 260, Loss: 6.527\n",
      "Step: 261, Loss: 6.419\n",
      "Step: 262, Loss: 6.334\n",
      "Step: 263, Loss: 6.242\n",
      "Step: 264, Loss: 6.230\n",
      "Step: 265, Loss: 6.016\n",
      "Step: 266, Loss: 6.038\n",
      "Step: 267, Loss: 5.969\n",
      "Step: 268, Loss: 5.881\n",
      "Step: 269, Loss: 5.739\n",
      "Step: 270, Loss: 5.704\n",
      "Step: 271, Loss: 5.678\n",
      "Step: 272, Loss: 5.488\n",
      "Step: 273, Loss: 5.482\n",
      "Step: 274, Loss: 5.417\n",
      "Step: 275, Loss: 5.390\n",
      "Step: 276, Loss: 5.171\n",
      "Step: 277, Loss: 5.229\n",
      "Step: 278, Loss: 5.148\n",
      "Step: 279, Loss: 5.060\n",
      "Step: 280, Loss: 4.979\n",
      "Step: 281, Loss: 4.906\n",
      "Step: 282, Loss: 4.904\n",
      "Step: 283, Loss: 4.713\n",
      "Step: 284, Loss: 4.737\n",
      "Step: 285, Loss: 4.689\n",
      "Step: 286, Loss: 4.616\n",
      "Step: 287, Loss: 4.490\n",
      "Step: 288, Loss: 4.483\n",
      "Step: 289, Loss: 4.436\n",
      "Step: 290, Loss: 4.313\n",
      "Step: 291, Loss: 4.299\n",
      "Step: 292, Loss: 4.240\n",
      "Step: 293, Loss: 4.216\n",
      "Step: 294, Loss: 4.048\n",
      "Step: 295, Loss: 4.092\n",
      "Step: 296, Loss: 4.027\n",
      "Step: 297, Loss: 3.958\n",
      "Step: 298, Loss: 3.879\n",
      "Step: 299, Loss: 3.840\n",
      "Step: 300, Loss: 3.827\n",
      "Step: 301, Loss: 3.694\n",
      "Step: 302, Loss: 3.690\n",
      "Step: 303, Loss: 3.655\n",
      "Step: 304, Loss: 3.608\n",
      "Step: 305, Loss: 3.497\n",
      "Step: 306, Loss: 3.514\n",
      "Step: 307, Loss: 3.446\n",
      "Step: 308, Loss: 3.387\n",
      "Step: 309, Loss: 3.345\n",
      "Step: 310, Loss: 3.305\n",
      "Step: 311, Loss: 3.286\n",
      "Step: 312, Loss: 3.163\n",
      "Step: 313, Loss: 3.186\n",
      "Step: 314, Loss: 3.136\n",
      "Step: 315, Loss: 3.095\n",
      "Step: 316, Loss: 3.020\n",
      "Step: 317, Loss: 3.000\n",
      "Step: 318, Loss: 2.983\n",
      "Step: 319, Loss: 2.880\n",
      "Step: 320, Loss: 2.890\n",
      "Step: 321, Loss: 2.844\n",
      "Step: 322, Loss: 2.816\n",
      "Step: 323, Loss: 2.716\n",
      "Step: 324, Loss: 2.750\n",
      "Step: 325, Loss: 2.693\n",
      "Step: 326, Loss: 2.654\n",
      "Step: 327, Loss: 2.609\n",
      "Step: 328, Loss: 2.579\n",
      "Step: 329, Loss: 2.569\n",
      "Step: 330, Loss: 2.475\n",
      "Step: 331, Loss: 2.487\n",
      "Step: 332, Loss: 2.461\n",
      "Step: 333, Loss: 2.418\n",
      "Step: 334, Loss: 2.359\n",
      "Step: 335, Loss: 2.356\n",
      "Step: 336, Loss: 2.337\n",
      "Step: 337, Loss: 2.260\n",
      "Step: 338, Loss: 2.259\n",
      "Step: 339, Loss: 2.233\n",
      "Step: 340, Loss: 2.219\n",
      "Step: 341, Loss: 2.127\n",
      "Step: 342, Loss: 2.161\n",
      "Step: 343, Loss: 2.121\n",
      "Step: 344, Loss: 2.084\n",
      "Step: 345, Loss: 2.050\n",
      "Step: 346, Loss: 2.033\n",
      "Step: 347, Loss: 2.024\n",
      "Step: 348, Loss: 1.953\n",
      "Step: 349, Loss: 1.962\n",
      "Step: 350, Loss: 1.944\n",
      "Step: 351, Loss: 1.904\n",
      "Step: 352, Loss: 1.858\n",
      "Step: 353, Loss: 1.873\n",
      "Step: 354, Loss: 1.836\n",
      "Step: 355, Loss: 1.803\n",
      "Step: 356, Loss: 1.787\n",
      "Step: 357, Loss: 1.773\n",
      "Step: 358, Loss: 1.756\n",
      "Step: 359, Loss: 1.694\n",
      "Step: 360, Loss: 1.716\n",
      "Step: 361, Loss: 1.686\n",
      "Step: 362, Loss: 1.661\n",
      "Step: 363, Loss: 1.629\n",
      "Step: 364, Loss: 1.625\n",
      "Step: 365, Loss: 1.613\n",
      "Step: 366, Loss: 1.562\n",
      "Step: 367, Loss: 1.566\n",
      "Step: 368, Loss: 1.551\n",
      "Step: 369, Loss: 1.531\n",
      "Step: 370, Loss: 1.485\n",
      "Step: 371, Loss: 1.505\n",
      "Step: 372, Loss: 1.476\n",
      "Step: 373, Loss: 1.453\n",
      "Step: 374, Loss: 1.436\n",
      "Step: 375, Loss: 1.424\n",
      "Step: 376, Loss: 1.415\n",
      "Step: 377, Loss: 1.374\n",
      "Step: 378, Loss: 1.382\n",
      "Step: 379, Loss: 1.365\n",
      "Step: 380, Loss: 1.346\n",
      "Step: 381, Loss: 1.317\n",
      "Step: 382, Loss: 1.322\n",
      "Step: 383, Loss: 1.309\n",
      "Step: 384, Loss: 1.270\n",
      "Step: 385, Loss: 1.274\n",
      "Step: 386, Loss: 1.262\n",
      "Step: 387, Loss: 1.253\n",
      "Step: 388, Loss: 1.211\n",
      "Step: 389, Loss: 1.232\n",
      "Step: 390, Loss: 1.209\n",
      "Step: 391, Loss: 1.193\n",
      "Step: 392, Loss: 1.175\n",
      "Step: 393, Loss: 1.172\n",
      "Step: 394, Loss: 1.165\n",
      "Step: 395, Loss: 1.129\n",
      "Step: 396, Loss: 1.137\n",
      "Step: 397, Loss: 1.132\n",
      "Step: 398, Loss: 1.111\n",
      "Step: 399, Loss: 1.088\n",
      "Step: 400, Loss: 1.098\n",
      "Step: 401, Loss: 1.081\n",
      "Step: 402, Loss: 1.060\n",
      "Step: 403, Loss: 1.058\n",
      "Step: 404, Loss: 1.053\n",
      "Step: 405, Loss: 1.045\n",
      "Step: 406, Loss: 1.012\n",
      "Step: 407, Loss: 1.028\n",
      "Step: 408, Loss: 1.011\n",
      "Step: 409, Loss: 0.999\n",
      "Step: 410, Loss: 0.984\n",
      "Step: 411, Loss: 0.986\n",
      "Step: 412, Loss: 0.978\n",
      "Step: 413, Loss: 0.955\n",
      "Step: 414, Loss: 0.956\n",
      "Step: 415, Loss: 0.951\n",
      "Step: 416, Loss: 0.941\n",
      "Step: 417, Loss: 0.921\n",
      "Step: 418, Loss: 0.930\n",
      "Step: 419, Loss: 0.914\n",
      "Step: 420, Loss: 0.906\n",
      "Step: 421, Loss: 0.896\n",
      "Step: 422, Loss: 0.898\n",
      "Step: 423, Loss: 0.886\n",
      "Step: 424, Loss: 0.867\n",
      "Step: 425, Loss: 0.873\n",
      "Step: 426, Loss: 0.866\n",
      "Step: 427, Loss: 0.856\n",
      "Step: 428, Loss: 0.844\n",
      "Step: 429, Loss: 0.846\n",
      "Step: 430, Loss: 0.839\n",
      "Step: 431, Loss: 0.823\n",
      "Step: 432, Loss: 0.824\n",
      "Step: 433, Loss: 0.819\n",
      "Step: 434, Loss: 0.813\n",
      "Step: 435, Loss: 0.794\n",
      "Step: 436, Loss: 0.807\n",
      "Step: 437, Loss: 0.794\n",
      "Step: 438, Loss: 0.788\n",
      "Step: 439, Loss: 0.777\n",
      "Step: 440, Loss: 0.779\n",
      "Step: 441, Loss: 0.774\n",
      "Step: 442, Loss: 0.757\n",
      "Step: 443, Loss: 0.763\n",
      "Step: 444, Loss: 0.760\n",
      "Step: 445, Loss: 0.749\n",
      "Step: 446, Loss: 0.738\n",
      "Step: 447, Loss: 0.744\n",
      "Step: 448, Loss: 0.737\n",
      "Step: 449, Loss: 0.725\n",
      "Step: 450, Loss: 0.725\n",
      "Step: 451, Loss: 0.723\n",
      "Step: 452, Loss: 0.718\n",
      "Step: 453, Loss: 0.702\n",
      "Step: 454, Loss: 0.714\n",
      "Step: 455, Loss: 0.703\n",
      "Step: 456, Loss: 0.698\n",
      "Step: 457, Loss: 0.689\n",
      "Step: 458, Loss: 0.693\n",
      "Step: 459, Loss: 0.687\n",
      "Step: 460, Loss: 0.676\n",
      "Step: 461, Loss: 0.678\n",
      "Step: 462, Loss: 0.679\n",
      "Step: 463, Loss: 0.667\n",
      "Step: 464, Loss: 0.659\n",
      "Step: 465, Loss: 0.668\n",
      "Step: 466, Loss: 0.655\n",
      "Step: 467, Loss: 0.655\n",
      "Step: 468, Loss: 0.648\n",
      "Step: 469, Loss: 0.652\n",
      "Step: 470, Loss: 0.643\n",
      "Step: 471, Loss: 0.635\n",
      "Step: 472, Loss: 0.640\n",
      "Step: 473, Loss: 0.635\n",
      "Step: 474, Loss: 0.630\n",
      "Step: 475, Loss: 0.622\n",
      "Step: 476, Loss: 0.627\n",
      "Step: 477, Loss: 0.622\n",
      "Step: 478, Loss: 0.614\n",
      "Step: 479, Loss: 0.614\n",
      "Step: 480, Loss: 0.615\n",
      "Step: 481, Loss: 0.609\n",
      "Step: 482, Loss: 0.599\n",
      "Step: 483, Loss: 0.608\n",
      "Step: 484, Loss: 0.599\n",
      "Step: 485, Loss: 0.598\n",
      "Step: 486, Loss: 0.591\n",
      "Step: 487, Loss: 0.594\n",
      "Step: 488, Loss: 0.589\n",
      "Step: 489, Loss: 0.583\n",
      "Step: 490, Loss: 0.585\n",
      "Step: 491, Loss: 0.583\n",
      "Step: 492, Loss: 0.578\n",
      "Step: 493, Loss: 0.571\n",
      "Step: 494, Loss: 0.577\n",
      "Step: 495, Loss: 0.572\n",
      "Step: 496, Loss: 0.565\n",
      "Step: 497, Loss: 0.565\n",
      "Step: 498, Loss: 0.565\n",
      "Step: 499, Loss: 0.562\n",
      "Step: 500, Loss: 0.553\n",
      "Step: 501, Loss: 0.561\n",
      "Step: 502, Loss: 0.554\n",
      "Step: 503, Loss: 0.553\n",
      "Step: 504, Loss: 0.545\n",
      "Step: 505, Loss: 0.551\n",
      "Step: 506, Loss: 0.546\n",
      "Step: 507, Loss: 0.539\n",
      "Step: 508, Loss: 0.540\n",
      "Step: 509, Loss: 0.544\n",
      "Step: 510, Loss: 0.535\n",
      "Step: 511, Loss: 0.530\n",
      "Step: 512, Loss: 0.537\n",
      "Step: 513, Loss: 0.530\n",
      "Step: 514, Loss: 0.527\n",
      "Step: 515, Loss: 0.524\n",
      "Step: 516, Loss: 0.528\n",
      "Step: 517, Loss: 0.522\n",
      "Step: 518, Loss: 0.516\n",
      "Step: 519, Loss: 0.522\n",
      "Step: 520, Loss: 0.516\n",
      "Step: 521, Loss: 0.515\n",
      "Step: 522, Loss: 0.509\n",
      "Step: 523, Loss: 0.514\n",
      "Step: 524, Loss: 0.509\n",
      "Step: 525, Loss: 0.505\n",
      "Step: 526, Loss: 0.505\n",
      "Step: 527, Loss: 0.506\n",
      "Step: 528, Loss: 0.501\n",
      "Step: 529, Loss: 0.496\n",
      "Step: 530, Loss: 0.502\n",
      "Step: 531, Loss: 0.495\n",
      "Step: 532, Loss: 0.496\n",
      "Step: 533, Loss: 0.490\n",
      "Step: 534, Loss: 0.495\n",
      "Step: 535, Loss: 0.488\n",
      "Step: 536, Loss: 0.486\n",
      "Step: 537, Loss: 0.488\n",
      "Step: 538, Loss: 0.485\n",
      "Step: 539, Loss: 0.482\n",
      "Step: 540, Loss: 0.479\n",
      "Step: 541, Loss: 0.481\n",
      "Step: 542, Loss: 0.478\n",
      "Step: 543, Loss: 0.475\n",
      "Step: 544, Loss: 0.473\n",
      "Step: 545, Loss: 0.476\n",
      "Step: 546, Loss: 0.470\n",
      "Step: 547, Loss: 0.466\n",
      "Step: 548, Loss: 0.471\n",
      "Step: 549, Loss: 0.467\n",
      "Step: 550, Loss: 0.466\n",
      "Step: 551, Loss: 0.461\n",
      "Step: 552, Loss: 0.464\n",
      "Step: 553, Loss: 0.461\n",
      "Step: 554, Loss: 0.456\n",
      "Step: 555, Loss: 0.458\n",
      "Step: 556, Loss: 0.459\n",
      "Step: 557, Loss: 0.454\n",
      "Step: 558, Loss: 0.450\n",
      "Step: 559, Loss: 0.454\n",
      "Step: 560, Loss: 0.451\n",
      "Step: 561, Loss: 0.448\n",
      "Step: 562, Loss: 0.446\n",
      "Step: 563, Loss: 0.448\n",
      "Step: 564, Loss: 0.445\n",
      "Step: 565, Loss: 0.440\n",
      "Step: 566, Loss: 0.445\n",
      "Step: 567, Loss: 0.440\n",
      "Step: 568, Loss: 0.440\n",
      "Step: 569, Loss: 0.434\n",
      "Step: 570, Loss: 0.439\n",
      "Step: 571, Loss: 0.435\n",
      "Step: 572, Loss: 0.432\n",
      "Step: 573, Loss: 0.431\n",
      "Step: 574, Loss: 0.435\n",
      "Step: 575, Loss: 0.428\n",
      "Step: 576, Loss: 0.425\n",
      "Step: 577, Loss: 0.431\n",
      "Step: 578, Loss: 0.424\n",
      "Step: 579, Loss: 0.426\n",
      "Step: 580, Loss: 0.421\n",
      "Step: 581, Loss: 0.425\n",
      "Step: 582, Loss: 0.419\n",
      "Step: 583, Loss: 0.418\n",
      "Step: 584, Loss: 0.420\n",
      "Step: 585, Loss: 0.417\n",
      "Step: 586, Loss: 0.416\n",
      "Step: 587, Loss: 0.412\n",
      "Step: 588, Loss: 0.415\n",
      "Step: 589, Loss: 0.412\n",
      "Step: 590, Loss: 0.410\n",
      "Step: 591, Loss: 0.408\n",
      "Step: 592, Loss: 0.411\n",
      "Step: 593, Loss: 0.406\n",
      "Step: 594, Loss: 0.402\n",
      "Step: 595, Loss: 0.407\n",
      "Step: 596, Loss: 0.402\n",
      "Step: 597, Loss: 0.404\n",
      "Step: 598, Loss: 0.398\n",
      "Step: 599, Loss: 0.402\n",
      "Step: 600, Loss: 0.398\n",
      "Step: 601, Loss: 0.396\n",
      "Step: 602, Loss: 0.396\n",
      "Step: 603, Loss: 0.396\n",
      "Step: 604, Loss: 0.393\n",
      "Step: 605, Loss: 0.390\n",
      "Step: 606, Loss: 0.393\n",
      "Step: 607, Loss: 0.391\n",
      "Step: 608, Loss: 0.388\n",
      "Step: 609, Loss: 0.387\n",
      "Step: 610, Loss: 0.389\n",
      "Step: 611, Loss: 0.385\n",
      "Step: 612, Loss: 0.382\n",
      "Step: 613, Loss: 0.386\n",
      "Step: 614, Loss: 0.382\n",
      "Step: 615, Loss: 0.382\n",
      "Step: 616, Loss: 0.377\n",
      "Step: 617, Loss: 0.381\n",
      "Step: 618, Loss: 0.378\n",
      "Step: 619, Loss: 0.375\n",
      "Step: 620, Loss: 0.375\n",
      "Step: 621, Loss: 0.378\n",
      "Step: 622, Loss: 0.372\n",
      "Step: 623, Loss: 0.369\n",
      "Step: 624, Loss: 0.374\n",
      "Step: 625, Loss: 0.370\n",
      "Step: 626, Loss: 0.369\n",
      "Step: 627, Loss: 0.366\n",
      "Step: 628, Loss: 0.370\n",
      "Step: 629, Loss: 0.366\n",
      "Step: 630, Loss: 0.363\n",
      "Step: 631, Loss: 0.366\n",
      "Step: 632, Loss: 0.363\n",
      "Step: 633, Loss: 0.362\n",
      "Step: 634, Loss: 0.359\n",
      "Step: 635, Loss: 0.362\n",
      "Step: 636, Loss: 0.359\n",
      "Step: 637, Loss: 0.357\n",
      "Step: 638, Loss: 0.356\n",
      "Step: 639, Loss: 0.358\n",
      "Step: 640, Loss: 0.355\n",
      "Step: 641, Loss: 0.351\n",
      "Step: 642, Loss: 0.355\n",
      "Step: 643, Loss: 0.351\n",
      "Step: 644, Loss: 0.352\n",
      "Step: 645, Loss: 0.348\n",
      "Step: 646, Loss: 0.352\n",
      "Step: 647, Loss: 0.347\n",
      "Step: 648, Loss: 0.346\n",
      "Step: 649, Loss: 0.347\n",
      "Step: 650, Loss: 0.346\n",
      "Step: 651, Loss: 0.344\n",
      "Step: 652, Loss: 0.341\n",
      "Step: 653, Loss: 0.344\n",
      "Step: 654, Loss: 0.341\n",
      "Step: 655, Loss: 0.340\n",
      "Step: 656, Loss: 0.338\n",
      "Step: 657, Loss: 0.341\n",
      "Step: 658, Loss: 0.337\n",
      "Step: 659, Loss: 0.335\n",
      "Step: 660, Loss: 0.338\n",
      "Step: 661, Loss: 0.335\n",
      "Step: 662, Loss: 0.335\n",
      "Step: 663, Loss: 0.331\n",
      "Step: 664, Loss: 0.334\n",
      "Step: 665, Loss: 0.331\n",
      "Step: 666, Loss: 0.330\n",
      "Step: 667, Loss: 0.329\n",
      "Step: 668, Loss: 0.331\n",
      "Step: 669, Loss: 0.327\n",
      "Step: 670, Loss: 0.325\n",
      "Step: 671, Loss: 0.328\n",
      "Step: 672, Loss: 0.326\n",
      "Step: 673, Loss: 0.324\n",
      "Step: 674, Loss: 0.322\n",
      "Step: 675, Loss: 0.325\n",
      "Step: 676, Loss: 0.322\n",
      "Step: 677, Loss: 0.319\n",
      "Step: 678, Loss: 0.322\n",
      "Step: 679, Loss: 0.320\n",
      "Step: 680, Loss: 0.319\n",
      "Step: 681, Loss: 0.315\n",
      "Step: 682, Loss: 0.318\n",
      "Step: 683, Loss: 0.316\n",
      "Step: 684, Loss: 0.315\n",
      "Step: 685, Loss: 0.313\n",
      "Step: 686, Loss: 0.316\n",
      "Step: 687, Loss: 0.312\n",
      "Step: 688, Loss: 0.310\n",
      "Step: 689, Loss: 0.314\n",
      "Step: 690, Loss: 0.310\n",
      "Step: 691, Loss: 0.310\n",
      "Step: 692, Loss: 0.307\n",
      "Step: 693, Loss: 0.310\n",
      "Step: 694, Loss: 0.307\n",
      "Step: 695, Loss: 0.305\n",
      "Step: 696, Loss: 0.306\n",
      "Step: 697, Loss: 0.305\n",
      "Step: 698, Loss: 0.304\n",
      "Step: 699, Loss: 0.302\n",
      "Step: 700, Loss: 0.304\n",
      "Step: 701, Loss: 0.302\n",
      "Step: 702, Loss: 0.301\n",
      "Step: 703, Loss: 0.300\n",
      "Step: 704, Loss: 0.301\n",
      "Step: 705, Loss: 0.299\n",
      "Step: 706, Loss: 0.296\n",
      "Step: 707, Loss: 0.299\n",
      "Step: 708, Loss: 0.296\n",
      "Step: 709, Loss: 0.297\n",
      "Step: 710, Loss: 0.293\n",
      "Step: 711, Loss: 0.296\n",
      "Step: 712, Loss: 0.293\n",
      "Step: 713, Loss: 0.293\n",
      "Step: 714, Loss: 0.292\n",
      "Step: 715, Loss: 0.293\n",
      "Step: 716, Loss: 0.290\n",
      "Step: 717, Loss: 0.288\n",
      "Step: 718, Loss: 0.290\n",
      "Step: 719, Loss: 0.289\n",
      "Step: 720, Loss: 0.288\n",
      "Step: 721, Loss: 0.286\n",
      "Step: 722, Loss: 0.288\n",
      "Step: 723, Loss: 0.285\n",
      "Step: 724, Loss: 0.283\n",
      "Step: 725, Loss: 0.285\n",
      "Step: 726, Loss: 0.284\n",
      "Step: 727, Loss: 0.283\n",
      "Step: 728, Loss: 0.280\n",
      "Step: 729, Loss: 0.282\n",
      "Step: 730, Loss: 0.281\n",
      "Step: 731, Loss: 0.279\n",
      "Step: 732, Loss: 0.279\n",
      "Step: 733, Loss: 0.281\n",
      "Step: 734, Loss: 0.277\n",
      "Step: 735, Loss: 0.275\n",
      "Step: 736, Loss: 0.278\n",
      "Step: 737, Loss: 0.276\n",
      "Step: 738, Loss: 0.276\n",
      "Step: 739, Loss: 0.273\n",
      "Step: 740, Loss: 0.276\n",
      "Step: 741, Loss: 0.273\n",
      "Step: 742, Loss: 0.271\n",
      "Step: 743, Loss: 0.273\n",
      "Step: 744, Loss: 0.272\n",
      "Step: 745, Loss: 0.271\n",
      "Step: 746, Loss: 0.268\n",
      "Step: 747, Loss: 0.270\n",
      "Step: 748, Loss: 0.269\n",
      "Step: 749, Loss: 0.268\n",
      "Step: 750, Loss: 0.267\n",
      "Step: 751, Loss: 0.268\n",
      "Step: 752, Loss: 0.266\n",
      "Step: 753, Loss: 0.264\n",
      "Step: 754, Loss: 0.267\n",
      "Step: 755, Loss: 0.264\n",
      "Step: 756, Loss: 0.264\n",
      "Step: 757, Loss: 0.262\n",
      "Step: 758, Loss: 0.264\n",
      "Step: 759, Loss: 0.261\n",
      "Step: 760, Loss: 0.261\n",
      "Step: 761, Loss: 0.261\n",
      "Step: 762, Loss: 0.261\n",
      "Step: 763, Loss: 0.259\n",
      "Step: 764, Loss: 0.258\n",
      "Step: 765, Loss: 0.259\n",
      "Step: 766, Loss: 0.258\n",
      "Step: 767, Loss: 0.257\n",
      "Step: 768, Loss: 0.255\n",
      "Step: 769, Loss: 0.257\n",
      "Step: 770, Loss: 0.255\n",
      "Step: 771, Loss: 0.253\n",
      "Step: 772, Loss: 0.255\n",
      "Step: 773, Loss: 0.254\n",
      "Step: 774, Loss: 0.253\n",
      "Step: 775, Loss: 0.251\n",
      "Step: 776, Loss: 0.253\n",
      "Step: 777, Loss: 0.251\n",
      "Step: 778, Loss: 0.250\n",
      "Step: 779, Loss: 0.250\n",
      "Step: 780, Loss: 0.251\n",
      "Step: 781, Loss: 0.249\n",
      "Step: 782, Loss: 0.247\n",
      "Step: 783, Loss: 0.248\n",
      "Step: 784, Loss: 0.248\n",
      "Step: 785, Loss: 0.246\n",
      "Step: 786, Loss: 0.245\n",
      "Step: 787, Loss: 0.247\n",
      "Step: 788, Loss: 0.245\n",
      "Step: 789, Loss: 0.243\n",
      "Step: 790, Loss: 0.245\n",
      "Step: 791, Loss: 0.244\n",
      "Step: 792, Loss: 0.243\n",
      "Step: 793, Loss: 0.241\n",
      "Step: 794, Loss: 0.243\n",
      "Step: 795, Loss: 0.241\n",
      "Step: 796, Loss: 0.240\n",
      "Step: 797, Loss: 0.239\n",
      "Step: 798, Loss: 0.242\n",
      "Step: 799, Loss: 0.239\n",
      "Step: 800, Loss: 0.237\n",
      "Step: 801, Loss: 0.240\n",
      "Step: 802, Loss: 0.237\n",
      "Step: 803, Loss: 0.238\n",
      "Step: 804, Loss: 0.235\n",
      "Step: 805, Loss: 0.237\n",
      "Step: 806, Loss: 0.235\n",
      "Step: 807, Loss: 0.234\n",
      "Step: 808, Loss: 0.235\n",
      "Step: 809, Loss: 0.234\n",
      "Step: 810, Loss: 0.234\n",
      "Step: 811, Loss: 0.232\n",
      "Step: 812, Loss: 0.233\n",
      "Step: 813, Loss: 0.232\n",
      "Step: 814, Loss: 0.231\n",
      "Step: 815, Loss: 0.230\n",
      "Step: 816, Loss: 0.232\n",
      "Step: 817, Loss: 0.230\n",
      "Step: 818, Loss: 0.228\n",
      "Step: 819, Loss: 0.230\n",
      "Step: 820, Loss: 0.228\n",
      "Step: 821, Loss: 0.229\n",
      "Step: 822, Loss: 0.226\n",
      "Step: 823, Loss: 0.228\n",
      "Step: 824, Loss: 0.226\n",
      "Step: 825, Loss: 0.226\n",
      "Step: 826, Loss: 0.225\n",
      "Step: 827, Loss: 0.226\n",
      "Step: 828, Loss: 0.225\n",
      "Step: 829, Loss: 0.223\n",
      "Step: 830, Loss: 0.224\n",
      "Step: 831, Loss: 0.224\n",
      "Step: 832, Loss: 0.223\n",
      "Step: 833, Loss: 0.222\n",
      "Step: 834, Loss: 0.223\n",
      "Step: 835, Loss: 0.222\n",
      "Step: 836, Loss: 0.220\n",
      "Step: 837, Loss: 0.221\n",
      "Step: 838, Loss: 0.221\n",
      "Step: 839, Loss: 0.220\n",
      "Step: 840, Loss: 0.218\n",
      "Step: 841, Loss: 0.220\n",
      "Step: 842, Loss: 0.219\n",
      "Step: 843, Loss: 0.218\n",
      "Step: 844, Loss: 0.217\n",
      "Step: 845, Loss: 0.219\n",
      "Step: 846, Loss: 0.217\n",
      "Step: 847, Loss: 0.215\n",
      "Step: 848, Loss: 0.217\n",
      "Step: 849, Loss: 0.216\n",
      "Step: 850, Loss: 0.215\n",
      "Step: 851, Loss: 0.213\n",
      "Step: 852, Loss: 0.215\n",
      "Step: 853, Loss: 0.214\n",
      "Step: 854, Loss: 0.213\n",
      "Step: 855, Loss: 0.213\n",
      "Step: 856, Loss: 0.213\n",
      "Step: 857, Loss: 0.212\n",
      "Step: 858, Loss: 0.211\n",
      "Step: 859, Loss: 0.212\n",
      "Step: 860, Loss: 0.212\n",
      "Step: 861, Loss: 0.211\n",
      "Step: 862, Loss: 0.209\n",
      "Step: 863, Loss: 0.211\n",
      "Step: 864, Loss: 0.210\n",
      "Step: 865, Loss: 0.208\n",
      "Step: 866, Loss: 0.210\n",
      "Step: 867, Loss: 0.208\n",
      "Step: 868, Loss: 0.208\n",
      "Step: 869, Loss: 0.206\n",
      "Step: 870, Loss: 0.208\n",
      "Step: 871, Loss: 0.206\n",
      "Step: 872, Loss: 0.206\n",
      "Step: 873, Loss: 0.205\n",
      "Step: 874, Loss: 0.206\n",
      "Step: 875, Loss: 0.205\n",
      "Step: 876, Loss: 0.204\n",
      "Step: 877, Loss: 0.204\n",
      "Step: 878, Loss: 0.204\n",
      "Step: 879, Loss: 0.204\n",
      "Step: 880, Loss: 0.202\n",
      "Step: 881, Loss: 0.204\n",
      "Step: 882, Loss: 0.203\n",
      "Step: 883, Loss: 0.200\n",
      "Step: 884, Loss: 0.202\n",
      "Step: 885, Loss: 0.201\n",
      "Step: 886, Loss: 0.201\n",
      "Step: 887, Loss: 0.199\n",
      "Step: 888, Loss: 0.200\n",
      "Step: 889, Loss: 0.200\n",
      "Step: 890, Loss: 0.199\n",
      "Step: 891, Loss: 0.198\n",
      "Step: 892, Loss: 0.199\n",
      "Step: 893, Loss: 0.198\n",
      "Step: 894, Loss: 0.197\n",
      "Step: 895, Loss: 0.198\n",
      "Step: 896, Loss: 0.198\n",
      "Step: 897, Loss: 0.197\n",
      "Step: 898, Loss: 0.195\n",
      "Step: 899, Loss: 0.197\n",
      "Step: 900, Loss: 0.196\n",
      "Step: 901, Loss: 0.194\n",
      "Step: 902, Loss: 0.195\n",
      "Step: 903, Loss: 0.195\n",
      "Step: 904, Loss: 0.194\n",
      "Step: 905, Loss: 0.193\n",
      "Step: 906, Loss: 0.194\n",
      "Step: 907, Loss: 0.193\n",
      "Step: 908, Loss: 0.193\n",
      "Step: 909, Loss: 0.192\n",
      "Step: 910, Loss: 0.193\n",
      "Step: 911, Loss: 0.192\n",
      "Step: 912, Loss: 0.190\n",
      "Step: 913, Loss: 0.192\n",
      "Step: 914, Loss: 0.191\n",
      "Step: 915, Loss: 0.191\n",
      "Step: 916, Loss: 0.189\n",
      "Step: 917, Loss: 0.190\n",
      "Step: 918, Loss: 0.190\n",
      "Step: 919, Loss: 0.189\n",
      "Step: 920, Loss: 0.189\n",
      "Step: 921, Loss: 0.189\n",
      "Step: 922, Loss: 0.188\n",
      "Step: 923, Loss: 0.187\n",
      "Step: 924, Loss: 0.188\n",
      "Step: 925, Loss: 0.188\n",
      "Step: 926, Loss: 0.187\n",
      "Step: 927, Loss: 0.186\n",
      "Step: 928, Loss: 0.187\n",
      "Step: 929, Loss: 0.186\n",
      "Step: 930, Loss: 0.185\n",
      "Step: 931, Loss: 0.186\n",
      "Step: 932, Loss: 0.185\n",
      "Step: 933, Loss: 0.186\n",
      "Step: 934, Loss: 0.184\n",
      "Step: 935, Loss: 0.185\n",
      "Step: 936, Loss: 0.184\n",
      "Step: 937, Loss: 0.184\n",
      "Step: 938, Loss: 0.183\n",
      "Step: 939, Loss: 0.184\n",
      "Step: 940, Loss: 0.183\n",
      "Step: 941, Loss: 0.182\n",
      "Step: 942, Loss: 0.182\n",
      "Step: 943, Loss: 0.183\n",
      "Step: 944, Loss: 0.182\n",
      "Step: 945, Loss: 0.181\n",
      "Step: 946, Loss: 0.182\n",
      "Step: 947, Loss: 0.181\n",
      "Step: 948, Loss: 0.180\n",
      "Step: 949, Loss: 0.180\n",
      "Step: 950, Loss: 0.181\n",
      "Step: 951, Loss: 0.180\n",
      "Step: 952, Loss: 0.179\n",
      "Step: 953, Loss: 0.179\n",
      "Step: 954, Loss: 0.179\n",
      "Step: 955, Loss: 0.179\n",
      "Step: 956, Loss: 0.178\n",
      "Step: 957, Loss: 0.179\n",
      "Step: 958, Loss: 0.178\n",
      "Step: 959, Loss: 0.177\n",
      "Step: 960, Loss: 0.177\n",
      "Step: 961, Loss: 0.178\n",
      "Step: 962, Loss: 0.177\n",
      "Step: 963, Loss: 0.176\n",
      "Step: 964, Loss: 0.177\n",
      "Step: 965, Loss: 0.176\n",
      "Step: 966, Loss: 0.175\n",
      "Step: 967, Loss: 0.175\n",
      "Step: 968, Loss: 0.176\n",
      "Step: 969, Loss: 0.176\n",
      "Step: 970, Loss: 0.174\n",
      "Step: 971, Loss: 0.174\n",
      "Step: 972, Loss: 0.175\n",
      "Step: 973, Loss: 0.174\n",
      "Step: 974, Loss: 0.173\n",
      "Step: 975, Loss: 0.175\n",
      "Step: 976, Loss: 0.174\n",
      "Step: 977, Loss: 0.172\n",
      "Step: 978, Loss: 0.173\n",
      "Step: 979, Loss: 0.173\n",
      "Step: 980, Loss: 0.173\n",
      "Step: 981, Loss: 0.171\n",
      "Step: 982, Loss: 0.172\n",
      "Step: 983, Loss: 0.172\n",
      "Step: 984, Loss: 0.172\n",
      "Step: 985, Loss: 0.171\n",
      "Step: 986, Loss: 0.172\n",
      "Step: 987, Loss: 0.171\n",
      "Step: 988, Loss: 0.170\n",
      "Step: 989, Loss: 0.170\n",
      "Step: 990, Loss: 0.171\n",
      "Step: 991, Loss: 0.170\n",
      "Step: 992, Loss: 0.169\n",
      "Step: 993, Loss: 0.170\n",
      "Step: 994, Loss: 0.170\n",
      "Step: 995, Loss: 0.168\n",
      "Step: 996, Loss: 0.169\n",
      "Step: 997, Loss: 0.169\n",
      "Step: 998, Loss: 0.169\n",
      "Step: 999, Loss: 0.168\n",
      "Step: 1000, Loss: 0.168\n",
      "Step: 1001, Loss: 0.168\n",
      "Step: 1002, Loss: 0.168\n",
      "Step: 1003, Loss: 0.167\n",
      "Step: 1004, Loss: 0.168\n",
      "Step: 1005, Loss: 0.167\n",
      "Step: 1006, Loss: 0.166\n",
      "Step: 1007, Loss: 0.167\n",
      "Step: 1008, Loss: 0.167\n",
      "Step: 1009, Loss: 0.166\n",
      "Step: 1010, Loss: 0.165\n",
      "Step: 1011, Loss: 0.167\n",
      "Step: 1012, Loss: 0.166\n",
      "Step: 1013, Loss: 0.165\n",
      "Step: 1014, Loss: 0.165\n",
      "Step: 1015, Loss: 0.166\n",
      "Step: 1016, Loss: 0.165\n",
      "Step: 1017, Loss: 0.164\n",
      "Step: 1018, Loss: 0.164\n",
      "Step: 1019, Loss: 0.165\n",
      "Step: 1020, Loss: 0.164\n",
      "Step: 1021, Loss: 0.163\n",
      "Step: 1022, Loss: 0.165\n",
      "Step: 1023, Loss: 0.164\n",
      "Step: 1024, Loss: 0.162\n",
      "Step: 1025, Loss: 0.163\n",
      "Step: 1026, Loss: 0.164\n",
      "Step: 1027, Loss: 0.163\n",
      "Step: 1028, Loss: 0.162\n",
      "Step: 1029, Loss: 0.163\n",
      "Step: 1030, Loss: 0.163\n",
      "Step: 1031, Loss: 0.162\n",
      "Step: 1032, Loss: 0.162\n",
      "Step: 1033, Loss: 0.162\n",
      "Step: 1034, Loss: 0.162\n",
      "Step: 1035, Loss: 0.161\n",
      "Step: 1036, Loss: 0.161\n",
      "Step: 1037, Loss: 0.162\n",
      "Step: 1038, Loss: 0.161\n",
      "Step: 1039, Loss: 0.160\n",
      "Step: 1040, Loss: 0.161\n",
      "Step: 1041, Loss: 0.161\n",
      "Step: 1042, Loss: 0.159\n",
      "Step: 1043, Loss: 0.160\n",
      "Step: 1044, Loss: 0.160\n",
      "Step: 1045, Loss: 0.161\n",
      "Step: 1046, Loss: 0.159\n",
      "Step: 1047, Loss: 0.160\n",
      "Step: 1048, Loss: 0.159\n",
      "Step: 1049, Loss: 0.159\n",
      "Step: 1050, Loss: 0.158\n",
      "Step: 1051, Loss: 0.159\n",
      "Step: 1052, Loss: 0.159\n",
      "Step: 1053, Loss: 0.158\n",
      "Step: 1054, Loss: 0.158\n",
      "Step: 1055, Loss: 0.159\n",
      "Step: 1056, Loss: 0.158\n",
      "Step: 1057, Loss: 0.157\n",
      "Step: 1058, Loss: 0.158\n",
      "Step: 1059, Loss: 0.158\n",
      "Step: 1060, Loss: 0.157\n",
      "Step: 1061, Loss: 0.157\n",
      "Step: 1062, Loss: 0.158\n",
      "Step: 1063, Loss: 0.157\n",
      "Step: 1064, Loss: 0.156\n",
      "Step: 1065, Loss: 0.156\n",
      "Step: 1066, Loss: 0.157\n",
      "Step: 1067, Loss: 0.156\n",
      "Step: 1068, Loss: 0.156\n",
      "Step: 1069, Loss: 0.157\n",
      "Step: 1070, Loss: 0.156\n",
      "Step: 1071, Loss: 0.155\n",
      "Step: 1072, Loss: 0.155\n",
      "Step: 1073, Loss: 0.156\n",
      "Step: 1074, Loss: 0.156\n",
      "Step: 1075, Loss: 0.154\n",
      "Step: 1076, Loss: 0.155\n",
      "Step: 1077, Loss: 0.155\n",
      "Step: 1078, Loss: 0.154\n",
      "Step: 1079, Loss: 0.154\n",
      "Step: 1080, Loss: 0.155\n",
      "Step: 1081, Loss: 0.155\n",
      "Step: 1082, Loss: 0.154\n",
      "Step: 1083, Loss: 0.153\n",
      "Step: 1084, Loss: 0.155\n",
      "Step: 1085, Loss: 0.154\n",
      "Step: 1086, Loss: 0.153\n",
      "Step: 1087, Loss: 0.154\n",
      "Step: 1088, Loss: 0.154\n",
      "Step: 1089, Loss: 0.152\n",
      "Step: 1090, Loss: 0.153\n",
      "Step: 1091, Loss: 0.153\n",
      "Step: 1092, Loss: 0.153\n",
      "Step: 1093, Loss: 0.152\n",
      "Step: 1094, Loss: 0.152\n",
      "Step: 1095, Loss: 0.152\n",
      "Step: 1096, Loss: 0.152\n",
      "Step: 1097, Loss: 0.151\n",
      "Step: 1098, Loss: 0.153\n",
      "Step: 1099, Loss: 0.152\n",
      "Step: 1100, Loss: 0.151\n",
      "Step: 1101, Loss: 0.151\n",
      "Step: 1102, Loss: 0.152\n",
      "Step: 1103, Loss: 0.151\n",
      "Step: 1104, Loss: 0.150\n",
      "Step: 1105, Loss: 0.152\n",
      "Step: 1106, Loss: 0.151\n",
      "Step: 1107, Loss: 0.150\n",
      "Step: 1108, Loss: 0.150\n",
      "Step: 1109, Loss: 0.151\n",
      "Step: 1110, Loss: 0.151\n",
      "Step: 1111, Loss: 0.150\n",
      "Step: 1112, Loss: 0.150\n",
      "Step: 1113, Loss: 0.150\n",
      "Step: 1114, Loss: 0.150\n",
      "Step: 1115, Loss: 0.149\n",
      "Step: 1116, Loss: 0.150\n",
      "Step: 1117, Loss: 0.150\n",
      "Step: 1118, Loss: 0.149\n",
      "Step: 1119, Loss: 0.149\n",
      "Step: 1120, Loss: 0.150\n",
      "Step: 1121, Loss: 0.149\n",
      "Step: 1122, Loss: 0.148\n",
      "Step: 1123, Loss: 0.149\n",
      "Step: 1124, Loss: 0.149\n",
      "Step: 1125, Loss: 0.148\n",
      "Step: 1126, Loss: 0.148\n",
      "Step: 1127, Loss: 0.149\n",
      "Step: 1128, Loss: 0.149\n",
      "Step: 1129, Loss: 0.148\n",
      "Step: 1130, Loss: 0.148\n",
      "Step: 1131, Loss: 0.148\n",
      "Step: 1132, Loss: 0.148\n",
      "Step: 1133, Loss: 0.147\n",
      "Step: 1134, Loss: 0.148\n",
      "Step: 1135, Loss: 0.148\n",
      "Step: 1136, Loss: 0.147\n",
      "Step: 1137, Loss: 0.147\n",
      "Step: 1138, Loss: 0.148\n",
      "Step: 1139, Loss: 0.148\n",
      "Step: 1140, Loss: 0.146\n",
      "Step: 1141, Loss: 0.147\n",
      "Step: 1142, Loss: 0.147\n",
      "Step: 1143, Loss: 0.146\n",
      "Step: 1144, Loss: 0.146\n",
      "Step: 1145, Loss: 0.147\n",
      "Step: 1146, Loss: 0.147\n",
      "Step: 1147, Loss: 0.146\n",
      "Step: 1148, Loss: 0.146\n",
      "Step: 1149, Loss: 0.147\n",
      "Step: 1150, Loss: 0.146\n",
      "Step: 1151, Loss: 0.145\n",
      "Step: 1152, Loss: 0.146\n",
      "Step: 1153, Loss: 0.146\n",
      "Step: 1154, Loss: 0.144\n",
      "Step: 1155, Loss: 0.145\n",
      "Step: 1156, Loss: 0.146\n",
      "Step: 1157, Loss: 0.146\n",
      "Step: 1158, Loss: 0.144\n",
      "Step: 1159, Loss: 0.145\n",
      "Step: 1160, Loss: 0.145\n",
      "Step: 1161, Loss: 0.145\n",
      "Step: 1162, Loss: 0.144\n",
      "Step: 1163, Loss: 0.145\n",
      "Step: 1164, Loss: 0.145\n",
      "Step: 1165, Loss: 0.144\n",
      "Step: 1166, Loss: 0.144\n",
      "Step: 1167, Loss: 0.145\n",
      "Step: 1168, Loss: 0.144\n",
      "Step: 1169, Loss: 0.143\n",
      "Step: 1170, Loss: 0.144\n",
      "Step: 1171, Loss: 0.144\n",
      "Step: 1172, Loss: 0.143\n",
      "Step: 1173, Loss: 0.143\n",
      "Step: 1174, Loss: 0.144\n",
      "Step: 1175, Loss: 0.144\n",
      "Step: 1176, Loss: 0.143\n",
      "Step: 1177, Loss: 0.143\n",
      "Step: 1178, Loss: 0.143\n",
      "Step: 1179, Loss: 0.143\n",
      "Step: 1180, Loss: 0.142\n",
      "Step: 1181, Loss: 0.143\n",
      "Step: 1182, Loss: 0.143\n",
      "Step: 1183, Loss: 0.142\n",
      "Step: 1184, Loss: 0.142\n",
      "Step: 1185, Loss: 0.143\n",
      "Step: 1186, Loss: 0.142\n",
      "Step: 1187, Loss: 0.141\n",
      "Step: 1188, Loss: 0.142\n",
      "Step: 1189, Loss: 0.142\n",
      "Step: 1190, Loss: 0.141\n",
      "Step: 1191, Loss: 0.141\n",
      "Step: 1192, Loss: 0.142\n",
      "Step: 1193, Loss: 0.142\n",
      "Step: 1194, Loss: 0.141\n",
      "Step: 1195, Loss: 0.141\n",
      "Step: 1196, Loss: 0.142\n",
      "Step: 1197, Loss: 0.141\n",
      "Step: 1198, Loss: 0.140\n",
      "Step: 1199, Loss: 0.141\n",
      "Step: 1200, Loss: 0.141\n",
      "Step: 1201, Loss: 0.140\n",
      "Step: 1202, Loss: 0.140\n",
      "Step: 1203, Loss: 0.141\n",
      "Step: 1204, Loss: 0.141\n",
      "Step: 1205, Loss: 0.140\n",
      "Step: 1206, Loss: 0.140\n",
      "Step: 1207, Loss: 0.141\n",
      "Step: 1208, Loss: 0.140\n",
      "Step: 1209, Loss: 0.139\n",
      "Step: 1210, Loss: 0.141\n",
      "Step: 1211, Loss: 0.140\n",
      "Step: 1212, Loss: 0.140\n",
      "Step: 1213, Loss: 0.139\n",
      "Step: 1214, Loss: 0.140\n",
      "Step: 1215, Loss: 0.140\n",
      "Step: 1216, Loss: 0.139\n",
      "Step: 1217, Loss: 0.140\n",
      "Step: 1218, Loss: 0.140\n",
      "Step: 1219, Loss: 0.138\n",
      "Step: 1220, Loss: 0.139\n",
      "Step: 1221, Loss: 0.140\n",
      "Step: 1222, Loss: 0.140\n",
      "Step: 1223, Loss: 0.138\n",
      "Step: 1224, Loss: 0.139\n",
      "Step: 1225, Loss: 0.139\n",
      "Step: 1226, Loss: 0.139\n",
      "Step: 1227, Loss: 0.138\n",
      "Step: 1228, Loss: 0.139\n",
      "Step: 1229, Loss: 0.139\n",
      "Step: 1230, Loss: 0.138\n",
      "Step: 1231, Loss: 0.137\n",
      "Step: 1232, Loss: 0.139\n",
      "Step: 1233, Loss: 0.138\n",
      "Step: 1234, Loss: 0.137\n",
      "Step: 1235, Loss: 0.138\n",
      "Step: 1236, Loss: 0.138\n",
      "Step: 1237, Loss: 0.137\n",
      "Step: 1238, Loss: 0.137\n",
      "Step: 1239, Loss: 0.138\n",
      "Step: 1240, Loss: 0.138\n",
      "Step: 1241, Loss: 0.137\n",
      "Step: 1242, Loss: 0.137\n",
      "Step: 1243, Loss: 0.138\n",
      "Step: 1244, Loss: 0.137\n",
      "Step: 1245, Loss: 0.137\n",
      "Step: 1246, Loss: 0.137\n",
      "Step: 1247, Loss: 0.137\n",
      "Step: 1248, Loss: 0.136\n",
      "Step: 1249, Loss: 0.136\n",
      "Step: 1250, Loss: 0.137\n",
      "Step: 1251, Loss: 0.137\n",
      "Step: 1252, Loss: 0.136\n",
      "Step: 1253, Loss: 0.136\n",
      "Step: 1254, Loss: 0.137\n",
      "Step: 1255, Loss: 0.136\n",
      "Step: 1256, Loss: 0.136\n",
      "Step: 1257, Loss: 0.137\n",
      "Step: 1258, Loss: 0.136\n",
      "Step: 1259, Loss: 0.136\n",
      "Step: 1260, Loss: 0.135\n",
      "Step: 1261, Loss: 0.137\n",
      "Step: 1262, Loss: 0.136\n",
      "Step: 1263, Loss: 0.135\n",
      "Step: 1264, Loss: 0.136\n",
      "Step: 1265, Loss: 0.136\n",
      "Step: 1266, Loss: 0.135\n",
      "Step: 1267, Loss: 0.135\n",
      "Step: 1268, Loss: 0.136\n",
      "Step: 1269, Loss: 0.136\n",
      "Step: 1270, Loss: 0.135\n",
      "Step: 1271, Loss: 0.135\n",
      "Step: 1272, Loss: 0.135\n",
      "Step: 1273, Loss: 0.135\n",
      "Step: 1274, Loss: 0.134\n",
      "Step: 1275, Loss: 0.135\n",
      "Step: 1276, Loss: 0.135\n",
      "Step: 1277, Loss: 0.134\n",
      "Step: 1278, Loss: 0.134\n",
      "Step: 1279, Loss: 0.135\n",
      "Step: 1280, Loss: 0.135\n",
      "Step: 1281, Loss: 0.134\n",
      "Step: 1282, Loss: 0.135\n",
      "Step: 1283, Loss: 0.134\n",
      "Step: 1284, Loss: 0.134\n",
      "Step: 1285, Loss: 0.133\n",
      "Step: 1286, Loss: 0.135\n",
      "Step: 1287, Loss: 0.134\n",
      "Step: 1288, Loss: 0.134\n",
      "Step: 1289, Loss: 0.134\n",
      "Step: 1290, Loss: 0.134\n",
      "Step: 1291, Loss: 0.134\n",
      "Step: 1292, Loss: 0.133\n",
      "Step: 1293, Loss: 0.134\n",
      "Step: 1294, Loss: 0.134\n",
      "Step: 1295, Loss: 0.133\n",
      "Step: 1296, Loss: 0.133\n",
      "Step: 1297, Loss: 0.134\n",
      "Step: 1298, Loss: 0.133\n",
      "Step: 1299, Loss: 0.133\n",
      "Step: 1300, Loss: 0.133\n",
      "Step: 1301, Loss: 0.133\n",
      "Step: 1302, Loss: 0.133\n",
      "Step: 1303, Loss: 0.132\n",
      "Step: 1304, Loss: 0.134\n",
      "Step: 1305, Loss: 0.133\n",
      "Step: 1306, Loss: 0.132\n",
      "Step: 1307, Loss: 0.132\n",
      "Step: 1308, Loss: 0.133\n",
      "Step: 1309, Loss: 0.133\n",
      "Step: 1310, Loss: 0.132\n",
      "Step: 1311, Loss: 0.133\n",
      "Step: 1312, Loss: 0.133\n",
      "Step: 1313, Loss: 0.132\n",
      "Step: 1314, Loss: 0.132\n",
      "Step: 1315, Loss: 0.133\n",
      "Step: 1316, Loss: 0.133\n",
      "Step: 1317, Loss: 0.132\n",
      "Step: 1318, Loss: 0.132\n",
      "Step: 1319, Loss: 0.132\n",
      "Step: 1320, Loss: 0.132\n",
      "Step: 1321, Loss: 0.131\n",
      "Step: 1322, Loss: 0.132\n",
      "Step: 1323, Loss: 0.132\n",
      "Step: 1324, Loss: 0.131\n",
      "Step: 1325, Loss: 0.131\n",
      "Step: 1326, Loss: 0.132\n",
      "Step: 1327, Loss: 0.132\n",
      "Step: 1328, Loss: 0.131\n",
      "Step: 1329, Loss: 0.132\n",
      "Step: 1330, Loss: 0.132\n",
      "Step: 1331, Loss: 0.131\n",
      "Step: 1332, Loss: 0.131\n",
      "Step: 1333, Loss: 0.132\n",
      "Step: 1334, Loss: 0.132\n",
      "Step: 1335, Loss: 0.131\n",
      "Step: 1336, Loss: 0.131\n",
      "Step: 1337, Loss: 0.131\n",
      "Step: 1338, Loss: 0.131\n",
      "Step: 1339, Loss: 0.130\n",
      "Step: 1340, Loss: 0.131\n",
      "Step: 1341, Loss: 0.131\n",
      "Step: 1342, Loss: 0.130\n",
      "Step: 1343, Loss: 0.130\n",
      "Step: 1344, Loss: 0.131\n",
      "Step: 1345, Loss: 0.130\n",
      "Step: 1346, Loss: 0.130\n",
      "Step: 1347, Loss: 0.131\n",
      "Step: 1348, Loss: 0.130\n",
      "Step: 1349, Loss: 0.130\n",
      "Step: 1350, Loss: 0.129\n",
      "Step: 1351, Loss: 0.131\n",
      "Step: 1352, Loss: 0.130\n",
      "Step: 1353, Loss: 0.130\n",
      "Step: 1354, Loss: 0.130\n",
      "Step: 1355, Loss: 0.130\n",
      "Step: 1356, Loss: 0.130\n",
      "Step: 1357, Loss: 0.129\n",
      "Step: 1358, Loss: 0.130\n",
      "Step: 1359, Loss: 0.130\n",
      "Step: 1360, Loss: 0.129\n",
      "Step: 1361, Loss: 0.129\n",
      "Step: 1362, Loss: 0.130\n",
      "Step: 1363, Loss: 0.130\n",
      "Step: 1364, Loss: 0.129\n",
      "Step: 1365, Loss: 0.129\n",
      "Step: 1366, Loss: 0.130\n",
      "Step: 1367, Loss: 0.129\n",
      "Step: 1368, Loss: 0.128\n",
      "Step: 1369, Loss: 0.130\n",
      "Step: 1370, Loss: 0.129\n",
      "Step: 1371, Loss: 0.129\n",
      "Step: 1372, Loss: 0.128\n",
      "Step: 1373, Loss: 0.130\n",
      "Step: 1374, Loss: 0.129\n",
      "Step: 1375, Loss: 0.128\n",
      "Step: 1376, Loss: 0.129\n",
      "Step: 1377, Loss: 0.129\n",
      "Step: 1378, Loss: 0.128\n",
      "Step: 1379, Loss: 0.128\n",
      "Step: 1380, Loss: 0.129\n",
      "Step: 1381, Loss: 0.129\n",
      "Step: 1382, Loss: 0.128\n",
      "Step: 1383, Loss: 0.128\n",
      "Step: 1384, Loss: 0.129\n",
      "Step: 1385, Loss: 0.128\n",
      "Step: 1386, Loss: 0.128\n",
      "Step: 1387, Loss: 0.129\n",
      "Step: 1388, Loss: 0.128\n",
      "Step: 1389, Loss: 0.128\n",
      "Step: 1390, Loss: 0.127\n",
      "Step: 1391, Loss: 0.129\n",
      "Step: 1392, Loss: 0.128\n",
      "Step: 1393, Loss: 0.127\n",
      "Step: 1394, Loss: 0.128\n",
      "Step: 1395, Loss: 0.128\n",
      "Step: 1396, Loss: 0.127\n",
      "Step: 1397, Loss: 0.127\n",
      "Step: 1398, Loss: 0.128\n",
      "Step: 1399, Loss: 0.128\n",
      "Step: 1400, Loss: 0.127\n",
      "Step: 1401, Loss: 0.127\n",
      "Step: 1402, Loss: 0.128\n",
      "Step: 1403, Loss: 0.127\n",
      "Step: 1404, Loss: 0.127\n",
      "Step: 1405, Loss: 0.128\n",
      "Step: 1406, Loss: 0.127\n",
      "Step: 1407, Loss: 0.127\n",
      "Step: 1408, Loss: 0.126\n",
      "Step: 1409, Loss: 0.128\n",
      "Step: 1410, Loss: 0.127\n",
      "Step: 1411, Loss: 0.126\n",
      "Step: 1412, Loss: 0.127\n",
      "Step: 1413, Loss: 0.127\n",
      "Step: 1414, Loss: 0.126\n",
      "Step: 1415, Loss: 0.126\n",
      "Step: 1416, Loss: 0.127\n",
      "Step: 1417, Loss: 0.127\n",
      "Step: 1418, Loss: 0.126\n",
      "Step: 1419, Loss: 0.126\n",
      "Step: 1420, Loss: 0.127\n",
      "Step: 1421, Loss: 0.126\n",
      "Step: 1422, Loss: 0.126\n",
      "Step: 1423, Loss: 0.127\n",
      "Step: 1424, Loss: 0.126\n",
      "Step: 1425, Loss: 0.126\n",
      "Step: 1426, Loss: 0.126\n",
      "Step: 1427, Loss: 0.127\n",
      "Step: 1428, Loss: 0.127\n",
      "Step: 1429, Loss: 0.126\n",
      "Step: 1430, Loss: 0.126\n",
      "Step: 1431, Loss: 0.126\n",
      "Step: 1432, Loss: 0.126\n",
      "Step: 1433, Loss: 0.125\n",
      "Step: 1434, Loss: 0.126\n",
      "Step: 1435, Loss: 0.126\n",
      "Step: 1436, Loss: 0.125\n",
      "Step: 1437, Loss: 0.125\n",
      "Step: 1438, Loss: 0.126\n",
      "Step: 1439, Loss: 0.126\n",
      "Step: 1440, Loss: 0.125\n",
      "Step: 1441, Loss: 0.126\n",
      "Step: 1442, Loss: 0.126\n",
      "Step: 1443, Loss: 0.125\n",
      "Step: 1444, Loss: 0.125\n",
      "Step: 1445, Loss: 0.126\n",
      "Step: 1446, Loss: 0.126\n",
      "Step: 1447, Loss: 0.125\n",
      "Step: 1448, Loss: 0.125\n",
      "Step: 1449, Loss: 0.125\n",
      "Step: 1450, Loss: 0.125\n",
      "Step: 1451, Loss: 0.124\n",
      "Step: 1452, Loss: 0.125\n",
      "Step: 1453, Loss: 0.125\n",
      "Step: 1454, Loss: 0.124\n",
      "Step: 1455, Loss: 0.124\n",
      "Step: 1456, Loss: 0.125\n",
      "Step: 1457, Loss: 0.125\n",
      "Step: 1458, Loss: 0.124\n",
      "Step: 1459, Loss: 0.125\n",
      "Step: 1460, Loss: 0.125\n",
      "Step: 1461, Loss: 0.124\n",
      "Step: 1462, Loss: 0.124\n",
      "Step: 1463, Loss: 0.125\n",
      "Step: 1464, Loss: 0.125\n",
      "Step: 1465, Loss: 0.124\n",
      "Step: 1466, Loss: 0.124\n",
      "Step: 1467, Loss: 0.124\n",
      "Step: 1468, Loss: 0.124\n",
      "Step: 1469, Loss: 0.124\n",
      "Step: 1470, Loss: 0.124\n",
      "Step: 1471, Loss: 0.124\n",
      "Step: 1472, Loss: 0.124\n",
      "Step: 1473, Loss: 0.123\n",
      "Step: 1474, Loss: 0.125\n",
      "Step: 1475, Loss: 0.124\n",
      "Step: 1476, Loss: 0.123\n",
      "Step: 1477, Loss: 0.124\n",
      "Step: 1478, Loss: 0.124\n",
      "Step: 1479, Loss: 0.123\n",
      "Step: 1480, Loss: 0.123\n",
      "Step: 1481, Loss: 0.124\n",
      "Step: 1482, Loss: 0.124\n",
      "Step: 1483, Loss: 0.123\n",
      "Step: 1484, Loss: 0.123\n",
      "Step: 1485, Loss: 0.124\n",
      "Step: 1486, Loss: 0.123\n",
      "Step: 1487, Loss: 0.123\n",
      "Step: 1488, Loss: 0.123\n",
      "Step: 1489, Loss: 0.123\n",
      "Step: 1490, Loss: 0.123\n",
      "Step: 1491, Loss: 0.123\n",
      "Step: 1492, Loss: 0.123\n",
      "Step: 1493, Loss: 0.123\n",
      "Step: 1494, Loss: 0.123\n",
      "Step: 1495, Loss: 0.123\n",
      "Step: 1496, Loss: 0.123\n",
      "Step: 1497, Loss: 0.123\n",
      "Step: 1498, Loss: 0.122\n",
      "Step: 1499, Loss: 0.123\n",
      "Step: 1500, Loss: 0.123\n",
      "Step: 1501, Loss: 0.122\n",
      "Step: 1502, Loss: 0.122\n",
      "Step: 1503, Loss: 0.123\n",
      "Step: 1504, Loss: 0.123\n",
      "Step: 1505, Loss: 0.122\n",
      "Step: 1506, Loss: 0.123\n",
      "Step: 1507, Loss: 0.123\n",
      "Step: 1508, Loss: 0.122\n",
      "Step: 1509, Loss: 0.122\n",
      "Step: 1510, Loss: 0.123\n",
      "Step: 1511, Loss: 0.122\n",
      "Step: 1512, Loss: 0.122\n",
      "Step: 1513, Loss: 0.122\n",
      "Step: 1514, Loss: 0.122\n",
      "Step: 1515, Loss: 0.122\n",
      "Step: 1516, Loss: 0.122\n",
      "Step: 1517, Loss: 0.122\n",
      "Step: 1518, Loss: 0.122\n",
      "Step: 1519, Loss: 0.122\n",
      "Step: 1520, Loss: 0.121\n",
      "Step: 1521, Loss: 0.123\n",
      "Step: 1522, Loss: 0.122\n",
      "Step: 1523, Loss: 0.121\n",
      "Step: 1524, Loss: 0.122\n",
      "Step: 1525, Loss: 0.122\n",
      "Step: 1526, Loss: 0.121\n",
      "Step: 1527, Loss: 0.121\n",
      "Step: 1528, Loss: 0.122\n",
      "Step: 1529, Loss: 0.122\n",
      "Step: 1530, Loss: 0.121\n",
      "Step: 1531, Loss: 0.121\n",
      "Step: 1532, Loss: 0.122\n",
      "Step: 1533, Loss: 0.121\n",
      "Step: 1534, Loss: 0.121\n",
      "Step: 1535, Loss: 0.121\n",
      "Step: 1536, Loss: 0.121\n",
      "Step: 1537, Loss: 0.121\n",
      "Step: 1538, Loss: 0.120\n",
      "Step: 1539, Loss: 0.121\n",
      "Step: 1540, Loss: 0.121\n",
      "Step: 1541, Loss: 0.120\n",
      "Step: 1542, Loss: 0.121\n",
      "Step: 1543, Loss: 0.121\n",
      "Step: 1544, Loss: 0.120\n",
      "Step: 1545, Loss: 0.120\n",
      "Step: 1546, Loss: 0.121\n",
      "Step: 1547, Loss: 0.121\n",
      "Step: 1548, Loss: 0.120\n",
      "Step: 1549, Loss: 0.120\n",
      "Step: 1550, Loss: 0.121\n",
      "Step: 1551, Loss: 0.120\n",
      "Step: 1552, Loss: 0.120\n",
      "Step: 1553, Loss: 0.121\n",
      "Step: 1554, Loss: 0.121\n",
      "Step: 1555, Loss: 0.120\n",
      "Step: 1556, Loss: 0.120\n",
      "Step: 1557, Loss: 0.121\n",
      "Step: 1558, Loss: 0.121\n",
      "Step: 1559, Loss: 0.120\n",
      "Step: 1560, Loss: 0.120\n",
      "Step: 1561, Loss: 0.120\n",
      "Step: 1562, Loss: 0.120\n",
      "Step: 1563, Loss: 0.119\n",
      "Step: 1564, Loss: 0.120\n",
      "Step: 1565, Loss: 0.120\n",
      "Step: 1566, Loss: 0.120\n",
      "Step: 1567, Loss: 0.119\n",
      "Step: 1568, Loss: 0.120\n",
      "Step: 1569, Loss: 0.120\n",
      "Step: 1570, Loss: 0.119\n",
      "Step: 1571, Loss: 0.120\n",
      "Step: 1572, Loss: 0.120\n",
      "Step: 1573, Loss: 0.119\n",
      "Step: 1574, Loss: 0.119\n",
      "Step: 1575, Loss: 0.120\n",
      "Step: 1576, Loss: 0.120\n",
      "Step: 1577, Loss: 0.119\n",
      "Step: 1578, Loss: 0.119\n",
      "Step: 1579, Loss: 0.120\n",
      "Step: 1580, Loss: 0.119\n",
      "Step: 1581, Loss: 0.119\n",
      "Step: 1582, Loss: 0.120\n",
      "Step: 1583, Loss: 0.119\n",
      "Step: 1584, Loss: 0.119\n",
      "Step: 1585, Loss: 0.118\n",
      "Step: 1586, Loss: 0.120\n",
      "Step: 1587, Loss: 0.119\n",
      "Step: 1588, Loss: 0.118\n",
      "Step: 1589, Loss: 0.119\n",
      "Step: 1590, Loss: 0.119\n",
      "Step: 1591, Loss: 0.119\n",
      "Step: 1592, Loss: 0.118\n",
      "Step: 1593, Loss: 0.119\n",
      "Step: 1594, Loss: 0.119\n",
      "Step: 1595, Loss: 0.118\n",
      "Step: 1596, Loss: 0.118\n",
      "Step: 1597, Loss: 0.119\n",
      "Step: 1598, Loss: 0.119\n",
      "Step: 1599, Loss: 0.118\n",
      "Step: 1600, Loss: 0.119\n",
      "Step: 1601, Loss: 0.119\n",
      "Step: 1602, Loss: 0.118\n",
      "Step: 1603, Loss: 0.118\n",
      "Step: 1604, Loss: 0.119\n",
      "Step: 1605, Loss: 0.119\n",
      "Step: 1606, Loss: 0.118\n",
      "Step: 1607, Loss: 0.118\n",
      "Step: 1608, Loss: 0.118\n",
      "Step: 1609, Loss: 0.118\n",
      "Step: 1610, Loss: 0.117\n",
      "Step: 1611, Loss: 0.118\n",
      "Step: 1612, Loss: 0.118\n",
      "Step: 1613, Loss: 0.118\n",
      "Step: 1614, Loss: 0.117\n",
      "Step: 1615, Loss: 0.118\n",
      "Step: 1616, Loss: 0.118\n",
      "Step: 1617, Loss: 0.117\n",
      "Step: 1618, Loss: 0.118\n",
      "Step: 1619, Loss: 0.118\n",
      "Step: 1620, Loss: 0.117\n",
      "Step: 1621, Loss: 0.117\n",
      "Step: 1622, Loss: 0.118\n",
      "Step: 1623, Loss: 0.118\n",
      "Step: 1624, Loss: 0.117\n",
      "Step: 1625, Loss: 0.117\n",
      "Step: 1626, Loss: 0.118\n",
      "Step: 1627, Loss: 0.117\n",
      "Step: 1628, Loss: 0.117\n",
      "Step: 1629, Loss: 0.118\n",
      "Step: 1630, Loss: 0.117\n",
      "Step: 1631, Loss: 0.117\n",
      "Step: 1632, Loss: 0.116\n",
      "Step: 1633, Loss: 0.118\n",
      "Step: 1634, Loss: 0.117\n",
      "Step: 1635, Loss: 0.117\n",
      "Step: 1636, Loss: 0.117\n",
      "Step: 1637, Loss: 0.117\n",
      "Step: 1638, Loss: 0.117\n",
      "Step: 1639, Loss: 0.116\n",
      "Step: 1640, Loss: 0.118\n",
      "Step: 1641, Loss: 0.117\n",
      "Step: 1642, Loss: 0.117\n",
      "Step: 1643, Loss: 0.116\n",
      "Step: 1644, Loss: 0.117\n",
      "Step: 1645, Loss: 0.117\n",
      "Step: 1646, Loss: 0.116\n",
      "Step: 1647, Loss: 0.117\n",
      "Step: 1648, Loss: 0.117\n",
      "Step: 1649, Loss: 0.116\n",
      "Step: 1650, Loss: 0.116\n",
      "Step: 1651, Loss: 0.117\n",
      "Step: 1652, Loss: 0.117\n",
      "Step: 1653, Loss: 0.116\n",
      "Step: 1654, Loss: 0.116\n",
      "Step: 1655, Loss: 0.117\n",
      "Step: 1656, Loss: 0.116\n",
      "Step: 1657, Loss: 0.116\n",
      "Step: 1658, Loss: 0.117\n",
      "Step: 1659, Loss: 0.116\n",
      "Step: 1660, Loss: 0.116\n",
      "Step: 1661, Loss: 0.115\n",
      "Step: 1662, Loss: 0.117\n",
      "Step: 1663, Loss: 0.116\n",
      "Step: 1664, Loss: 0.116\n",
      "Step: 1665, Loss: 0.116\n",
      "Step: 1666, Loss: 0.116\n",
      "Step: 1667, Loss: 0.115\n",
      "Step: 1668, Loss: 0.115\n",
      "Step: 1669, Loss: 0.117\n",
      "Step: 1670, Loss: 0.116\n",
      "Step: 1671, Loss: 0.115\n",
      "Step: 1672, Loss: 0.116\n",
      "Step: 1673, Loss: 0.116\n",
      "Step: 1674, Loss: 0.116\n",
      "Step: 1675, Loss: 0.115\n",
      "Step: 1676, Loss: 0.116\n",
      "Step: 1677, Loss: 0.116\n",
      "Step: 1678, Loss: 0.115\n",
      "Step: 1679, Loss: 0.115\n",
      "Step: 1680, Loss: 0.116\n",
      "Step: 1681, Loss: 0.115\n",
      "Step: 1682, Loss: 0.115\n",
      "Step: 1683, Loss: 0.116\n",
      "Step: 1684, Loss: 0.115\n",
      "Step: 1685, Loss: 0.115\n",
      "Step: 1686, Loss: 0.115\n",
      "Step: 1687, Loss: 0.116\n",
      "Step: 1688, Loss: 0.116\n",
      "Step: 1689, Loss: 0.115\n",
      "Step: 1690, Loss: 0.115\n",
      "Step: 1691, Loss: 0.116\n",
      "Step: 1692, Loss: 0.115\n",
      "Step: 1693, Loss: 0.115\n",
      "Step: 1694, Loss: 0.115\n",
      "Step: 1695, Loss: 0.115\n",
      "Step: 1696, Loss: 0.115\n",
      "Step: 1697, Loss: 0.114\n",
      "Step: 1698, Loss: 0.116\n",
      "Step: 1699, Loss: 0.115\n",
      "Step: 1700, Loss: 0.114\n",
      "Step: 1701, Loss: 0.115\n",
      "Step: 1702, Loss: 0.115\n",
      "Step: 1703, Loss: 0.115\n",
      "Step: 1704, Loss: 0.114\n",
      "Step: 1705, Loss: 0.115\n",
      "Step: 1706, Loss: 0.115\n",
      "Step: 1707, Loss: 0.114\n",
      "Step: 1708, Loss: 0.114\n",
      "Step: 1709, Loss: 0.115\n",
      "Step: 1710, Loss: 0.114\n",
      "Step: 1711, Loss: 0.114\n",
      "Step: 1712, Loss: 0.115\n",
      "Step: 1713, Loss: 0.115\n",
      "Step: 1714, Loss: 0.114\n",
      "Step: 1715, Loss: 0.114\n",
      "Step: 1716, Loss: 0.115\n",
      "Step: 1717, Loss: 0.115\n",
      "Step: 1718, Loss: 0.114\n",
      "Step: 1719, Loss: 0.114\n",
      "Step: 1720, Loss: 0.114\n",
      "Step: 1721, Loss: 0.114\n",
      "Step: 1722, Loss: 0.113\n",
      "Step: 1723, Loss: 0.115\n",
      "Step: 1724, Loss: 0.114\n",
      "Step: 1725, Loss: 0.114\n",
      "Step: 1726, Loss: 0.113\n",
      "Step: 1727, Loss: 0.115\n",
      "Step: 1728, Loss: 0.114\n",
      "Step: 1729, Loss: 0.113\n",
      "Step: 1730, Loss: 0.114\n",
      "Step: 1731, Loss: 0.114\n",
      "Step: 1732, Loss: 0.113\n",
      "Step: 1733, Loss: 0.113\n",
      "Step: 1734, Loss: 0.114\n",
      "Step: 1735, Loss: 0.114\n",
      "Step: 1736, Loss: 0.113\n",
      "Step: 1737, Loss: 0.113\n",
      "Step: 1738, Loss: 0.114\n",
      "Step: 1739, Loss: 0.114\n",
      "Step: 1740, Loss: 0.113\n",
      "Step: 1741, Loss: 0.114\n",
      "Step: 1742, Loss: 0.114\n",
      "Step: 1743, Loss: 0.113\n",
      "Step: 1744, Loss: 0.113\n",
      "Step: 1745, Loss: 0.114\n",
      "Step: 1746, Loss: 0.113\n",
      "Step: 1747, Loss: 0.113\n",
      "Step: 1748, Loss: 0.114\n",
      "Step: 1749, Loss: 0.113\n",
      "Step: 1750, Loss: 0.113\n",
      "Step: 1751, Loss: 0.112\n",
      "Step: 1752, Loss: 0.114\n",
      "Step: 1753, Loss: 0.113\n",
      "Step: 1754, Loss: 0.113\n",
      "Step: 1755, Loss: 0.113\n",
      "Step: 1756, Loss: 0.113\n",
      "Step: 1757, Loss: 0.113\n",
      "Step: 1758, Loss: 0.113\n",
      "Step: 1759, Loss: 0.113\n",
      "Step: 1760, Loss: 0.113\n",
      "Step: 1761, Loss: 0.113\n",
      "Step: 1762, Loss: 0.112\n",
      "Step: 1763, Loss: 0.114\n",
      "Step: 1764, Loss: 0.113\n",
      "Step: 1765, Loss: 0.112\n",
      "Step: 1766, Loss: 0.113\n",
      "Step: 1767, Loss: 0.113\n",
      "Step: 1768, Loss: 0.112\n",
      "Step: 1769, Loss: 0.112\n",
      "Step: 1770, Loss: 0.113\n",
      "Step: 1771, Loss: 0.113\n",
      "Step: 1772, Loss: 0.112\n",
      "Step: 1773, Loss: 0.112\n",
      "Step: 1774, Loss: 0.113\n",
      "Step: 1775, Loss: 0.112\n",
      "Step: 1776, Loss: 0.112\n",
      "Step: 1777, Loss: 0.113\n",
      "Step: 1778, Loss: 0.113\n",
      "Step: 1779, Loss: 0.112\n",
      "Step: 1780, Loss: 0.112\n",
      "Step: 1781, Loss: 0.113\n",
      "Step: 1782, Loss: 0.113\n",
      "Step: 1783, Loss: 0.112\n",
      "Step: 1784, Loss: 0.112\n",
      "Step: 1785, Loss: 0.113\n",
      "Step: 1786, Loss: 0.112\n",
      "Step: 1787, Loss: 0.112\n",
      "Step: 1788, Loss: 0.112\n",
      "Step: 1789, Loss: 0.112\n",
      "Step: 1790, Loss: 0.112\n",
      "Step: 1791, Loss: 0.111\n",
      "Step: 1792, Loss: 0.113\n",
      "Step: 1793, Loss: 0.112\n",
      "Step: 1794, Loss: 0.111\n",
      "Step: 1795, Loss: 0.112\n",
      "Step: 1796, Loss: 0.112\n",
      "Step: 1797, Loss: 0.112\n",
      "Step: 1798, Loss: 0.111\n",
      "Step: 1799, Loss: 0.112\n",
      "Step: 1800, Loss: 0.112\n",
      "Step: 1801, Loss: 0.111\n",
      "Step: 1802, Loss: 0.111\n",
      "Step: 1803, Loss: 0.112\n",
      "Step: 1804, Loss: 0.112\n",
      "Step: 1805, Loss: 0.111\n",
      "Step: 1806, Loss: 0.112\n",
      "Step: 1807, Loss: 0.111\n",
      "Step: 1808, Loss: 0.111\n",
      "Step: 1809, Loss: 0.111\n",
      "Step: 1810, Loss: 0.112\n",
      "Step: 1811, Loss: 0.111\n",
      "Step: 1812, Loss: 0.111\n",
      "Step: 1813, Loss: 0.112\n",
      "Step: 1814, Loss: 0.111\n",
      "Step: 1815, Loss: 0.111\n",
      "Step: 1816, Loss: 0.110\n",
      "Step: 1817, Loss: 0.112\n",
      "Step: 1818, Loss: 0.111\n",
      "Step: 1819, Loss: 0.111\n",
      "Step: 1820, Loss: 0.111\n",
      "Step: 1821, Loss: 0.112\n",
      "Step: 1822, Loss: 0.111\n",
      "Step: 1823, Loss: 0.111\n",
      "Step: 1824, Loss: 0.111\n",
      "Step: 1825, Loss: 0.111\n",
      "Step: 1826, Loss: 0.111\n",
      "Step: 1827, Loss: 0.110\n",
      "Step: 1828, Loss: 0.111\n",
      "Step: 1829, Loss: 0.111\n",
      "Step: 1830, Loss: 0.110\n",
      "Step: 1831, Loss: 0.111\n",
      "Step: 1832, Loss: 0.111\n",
      "Step: 1833, Loss: 0.111\n",
      "Step: 1834, Loss: 0.110\n",
      "Step: 1835, Loss: 0.111\n",
      "Step: 1836, Loss: 0.111\n",
      "Step: 1837, Loss: 0.111\n",
      "Step: 1838, Loss: 0.110\n",
      "Step: 1839, Loss: 0.111\n",
      "Step: 1840, Loss: 0.110\n",
      "Step: 1841, Loss: 0.110\n",
      "Step: 1842, Loss: 0.111\n",
      "Step: 1843, Loss: 0.111\n",
      "Step: 1844, Loss: 0.110\n",
      "Step: 1845, Loss: 0.110\n",
      "Step: 1846, Loss: 0.111\n",
      "Step: 1847, Loss: 0.111\n",
      "Step: 1848, Loss: 0.110\n",
      "Step: 1849, Loss: 0.110\n",
      "Step: 1850, Loss: 0.111\n",
      "Step: 1851, Loss: 0.110\n",
      "Step: 1852, Loss: 0.110\n",
      "Step: 1853, Loss: 0.111\n",
      "Step: 1854, Loss: 0.110\n",
      "Step: 1855, Loss: 0.110\n",
      "Step: 1856, Loss: 0.109\n",
      "Step: 1857, Loss: 0.111\n",
      "Step: 1858, Loss: 0.110\n",
      "Step: 1859, Loss: 0.110\n",
      "Step: 1860, Loss: 0.110\n",
      "Step: 1861, Loss: 0.110\n",
      "Step: 1862, Loss: 0.110\n",
      "Step: 1863, Loss: 0.109\n",
      "Step: 1864, Loss: 0.111\n",
      "Step: 1865, Loss: 0.110\n",
      "Step: 1866, Loss: 0.110\n",
      "Step: 1867, Loss: 0.109\n",
      "Step: 1868, Loss: 0.110\n",
      "Step: 1869, Loss: 0.110\n",
      "Step: 1870, Loss: 0.109\n",
      "Step: 1871, Loss: 0.110\n",
      "Step: 1872, Loss: 0.110\n",
      "Step: 1873, Loss: 0.109\n",
      "Step: 1874, Loss: 0.109\n",
      "Step: 1875, Loss: 0.110\n",
      "Step: 1876, Loss: 0.110\n",
      "Step: 1877, Loss: 0.109\n",
      "Step: 1878, Loss: 0.109\n",
      "Step: 1879, Loss: 0.110\n",
      "Step: 1880, Loss: 0.109\n",
      "Step: 1881, Loss: 0.109\n",
      "Step: 1882, Loss: 0.110\n",
      "Step: 1883, Loss: 0.110\n",
      "Step: 1884, Loss: 0.109\n",
      "Step: 1885, Loss: 0.109\n",
      "Step: 1886, Loss: 0.110\n",
      "Step: 1887, Loss: 0.109\n",
      "Step: 1888, Loss: 0.109\n",
      "Step: 1889, Loss: 0.110\n",
      "Step: 1890, Loss: 0.109\n",
      "Step: 1891, Loss: 0.109\n",
      "Step: 1892, Loss: 0.108\n",
      "Step: 1893, Loss: 0.110\n",
      "Step: 1894, Loss: 0.109\n",
      "Step: 1895, Loss: 0.109\n",
      "Step: 1896, Loss: 0.109\n",
      "Step: 1897, Loss: 0.110\n",
      "Step: 1898, Loss: 0.109\n",
      "Step: 1899, Loss: 0.108\n",
      "Step: 1900, Loss: 0.109\n",
      "Step: 1901, Loss: 0.109\n",
      "Step: 1902, Loss: 0.109\n",
      "Step: 1903, Loss: 0.108\n",
      "Step: 1904, Loss: 0.109\n",
      "Step: 1905, Loss: 0.109\n",
      "Step: 1906, Loss: 0.108\n",
      "Step: 1907, Loss: 0.109\n",
      "Step: 1908, Loss: 0.109\n",
      "Step: 1909, Loss: 0.109\n",
      "Step: 1910, Loss: 0.108\n",
      "Step: 1911, Loss: 0.109\n",
      "Step: 1912, Loss: 0.109\n",
      "Step: 1913, Loss: 0.108\n",
      "Step: 1914, Loss: 0.108\n",
      "Step: 1915, Loss: 0.109\n",
      "Step: 1916, Loss: 0.108\n",
      "Step: 1917, Loss: 0.108\n",
      "Step: 1918, Loss: 0.109\n",
      "Step: 1919, Loss: 0.108\n",
      "Step: 1920, Loss: 0.108\n",
      "Step: 1921, Loss: 0.108\n",
      "Step: 1922, Loss: 0.109\n",
      "Step: 1923, Loss: 0.108\n",
      "Step: 1924, Loss: 0.108\n",
      "Step: 1925, Loss: 0.109\n",
      "Step: 1926, Loss: 0.108\n",
      "Step: 1927, Loss: 0.108\n",
      "Step: 1928, Loss: 0.107\n",
      "Step: 1929, Loss: 0.109\n",
      "Step: 1930, Loss: 0.108\n",
      "Step: 1931, Loss: 0.108\n",
      "Step: 1932, Loss: 0.108\n",
      "Step: 1933, Loss: 0.109\n",
      "Step: 1934, Loss: 0.108\n",
      "Step: 1935, Loss: 0.108\n",
      "Step: 1936, Loss: 0.108\n",
      "Step: 1937, Loss: 0.108\n",
      "Step: 1938, Loss: 0.108\n",
      "Step: 1939, Loss: 0.107\n",
      "Step: 1940, Loss: 0.109\n",
      "Step: 1941, Loss: 0.108\n",
      "Step: 1942, Loss: 0.107\n",
      "Step: 1943, Loss: 0.108\n",
      "Step: 1944, Loss: 0.108\n",
      "Step: 1945, Loss: 0.108\n",
      "Step: 1946, Loss: 0.107\n",
      "Step: 1947, Loss: 0.108\n",
      "Step: 1948, Loss: 0.108\n",
      "Step: 1949, Loss: 0.108\n",
      "Step: 1950, Loss: 0.107\n",
      "Step: 1951, Loss: 0.108\n",
      "Step: 1952, Loss: 0.108\n",
      "Step: 1953, Loss: 0.107\n",
      "Step: 1954, Loss: 0.108\n",
      "Step: 1955, Loss: 0.108\n",
      "Step: 1956, Loss: 0.107\n",
      "Step: 1957, Loss: 0.107\n",
      "Step: 1958, Loss: 0.108\n",
      "Step: 1959, Loss: 0.108\n",
      "Step: 1960, Loss: 0.107\n",
      "Step: 1961, Loss: 0.107\n",
      "Step: 1962, Loss: 0.108\n",
      "Step: 1963, Loss: 0.107\n",
      "Step: 1964, Loss: 0.107\n",
      "Step: 1965, Loss: 0.108\n",
      "Step: 1966, Loss: 0.107\n",
      "Step: 1967, Loss: 0.107\n",
      "Step: 1968, Loss: 0.107\n",
      "Step: 1969, Loss: 0.108\n",
      "Step: 1970, Loss: 0.107\n",
      "Step: 1971, Loss: 0.107\n",
      "Step: 1972, Loss: 0.107\n",
      "Step: 1973, Loss: 0.107\n",
      "Step: 1974, Loss: 0.107\n",
      "Step: 1975, Loss: 0.106\n",
      "Step: 1976, Loss: 0.108\n",
      "Step: 1977, Loss: 0.107\n",
      "Step: 1978, Loss: 0.107\n",
      "Step: 1979, Loss: 0.107\n",
      "Step: 1980, Loss: 0.107\n",
      "Step: 1981, Loss: 0.107\n",
      "Step: 1982, Loss: 0.107\n",
      "Step: 1983, Loss: 0.107\n",
      "Step: 1984, Loss: 0.107\n",
      "Step: 1985, Loss: 0.107\n",
      "Step: 1986, Loss: 0.106\n",
      "Step: 1987, Loss: 0.108\n",
      "Step: 1988, Loss: 0.107\n",
      "Step: 1989, Loss: 0.106\n",
      "Step: 1990, Loss: 0.107\n",
      "Step: 1991, Loss: 0.107\n",
      "Step: 1992, Loss: 0.107\n",
      "Step: 1993, Loss: 0.106\n",
      "Step: 1994, Loss: 0.107\n",
      "Step: 1995, Loss: 0.107\n",
      "Step: 1996, Loss: 0.106\n",
      "Step: 1997, Loss: 0.106\n",
      "Step: 1998, Loss: 0.107\n",
      "Step: 1999, Loss: 0.106\n",
      "Step: 2000, Loss: 0.106\n",
      "Step: 2001, Loss: 0.107\n",
      "Step: 2002, Loss: 0.107\n",
      "Step: 2003, Loss: 0.106\n",
      "Step: 2004, Loss: 0.106\n",
      "Step: 2005, Loss: 0.107\n",
      "Step: 2006, Loss: 0.107\n",
      "Step: 2007, Loss: 0.106\n",
      "Step: 2008, Loss: 0.106\n",
      "Step: 2009, Loss: 0.107\n",
      "Step: 2010, Loss: 0.106\n",
      "Step: 2011, Loss: 0.106\n",
      "Step: 2012, Loss: 0.107\n",
      "Step: 2013, Loss: 0.106\n",
      "Step: 2014, Loss: 0.106\n",
      "Step: 2015, Loss: 0.106\n",
      "Step: 2016, Loss: 0.107\n",
      "Step: 2017, Loss: 0.106\n",
      "Step: 2018, Loss: 0.106\n",
      "Step: 2019, Loss: 0.106\n",
      "Step: 2020, Loss: 0.106\n",
      "Step: 2021, Loss: 0.106\n",
      "Step: 2022, Loss: 0.105\n",
      "Step: 2023, Loss: 0.107\n",
      "Step: 2024, Loss: 0.106\n",
      "Step: 2025, Loss: 0.106\n",
      "Step: 2026, Loss: 0.106\n",
      "Step: 2027, Loss: 0.106\n",
      "Step: 2028, Loss: 0.106\n",
      "Step: 2029, Loss: 0.105\n",
      "Step: 2030, Loss: 0.106\n",
      "Step: 2031, Loss: 0.106\n",
      "Step: 2032, Loss: 0.106\n",
      "Step: 2033, Loss: 0.105\n",
      "Step: 2034, Loss: 0.107\n",
      "Step: 2035, Loss: 0.106\n",
      "Step: 2036, Loss: 0.105\n",
      "Step: 2037, Loss: 0.106\n",
      "Step: 2038, Loss: 0.106\n",
      "Step: 2039, Loss: 0.105\n",
      "Step: 2040, Loss: 0.105\n",
      "Step: 2041, Loss: 0.106\n",
      "Step: 2042, Loss: 0.106\n",
      "Step: 2043, Loss: 0.105\n",
      "Step: 2044, Loss: 0.105\n",
      "Step: 2045, Loss: 0.106\n",
      "Step: 2046, Loss: 0.105\n",
      "Step: 2047, Loss: 0.105\n",
      "Step: 2048, Loss: 0.106\n",
      "Step: 2049, Loss: 0.105\n",
      "Step: 2050, Loss: 0.105\n",
      "Step: 2051, Loss: 0.105\n",
      "Step: 2052, Loss: 0.106\n",
      "Step: 2053, Loss: 0.106\n",
      "Step: 2054, Loss: 0.105\n",
      "Step: 2055, Loss: 0.105\n",
      "Step: 2056, Loss: 0.106\n",
      "Step: 2057, Loss: 0.105\n",
      "Step: 2058, Loss: 0.104\n",
      "Step: 2059, Loss: 0.106\n",
      "Step: 2060, Loss: 0.105\n",
      "Step: 2061, Loss: 0.105\n",
      "Step: 2062, Loss: 0.104\n",
      "Step: 2063, Loss: 0.106\n",
      "Step: 2064, Loss: 0.105\n",
      "Step: 2065, Loss: 0.105\n",
      "Step: 2066, Loss: 0.105\n",
      "Step: 2067, Loss: 0.105\n",
      "Step: 2068, Loss: 0.105\n",
      "Step: 2069, Loss: 0.104\n",
      "Step: 2070, Loss: 0.106\n",
      "Step: 2071, Loss: 0.105\n",
      "Step: 2072, Loss: 0.104\n",
      "Step: 2073, Loss: 0.105\n",
      "Step: 2074, Loss: 0.105\n",
      "Step: 2075, Loss: 0.105\n",
      "Step: 2076, Loss: 0.104\n",
      "Step: 2077, Loss: 0.105\n",
      "Step: 2078, Loss: 0.105\n",
      "Step: 2079, Loss: 0.105\n",
      "Step: 2080, Loss: 0.104\n",
      "Step: 2081, Loss: 0.105\n",
      "Step: 2082, Loss: 0.105\n",
      "Step: 2083, Loss: 0.104\n",
      "Step: 2084, Loss: 0.105\n",
      "Step: 2085, Loss: 0.105\n",
      "Step: 2086, Loss: 0.104\n",
      "Step: 2087, Loss: 0.104\n",
      "Step: 2088, Loss: 0.105\n",
      "Step: 2089, Loss: 0.105\n",
      "Step: 2090, Loss: 0.104\n",
      "Step: 2091, Loss: 0.104\n",
      "Step: 2092, Loss: 0.105\n",
      "Step: 2093, Loss: 0.104\n",
      "Step: 2094, Loss: 0.104\n",
      "Step: 2095, Loss: 0.105\n",
      "Step: 2096, Loss: 0.104\n",
      "Step: 2097, Loss: 0.104\n",
      "Step: 2098, Loss: 0.104\n",
      "Step: 2099, Loss: 0.105\n",
      "Step: 2100, Loss: 0.104\n",
      "Step: 2101, Loss: 0.104\n",
      "Step: 2102, Loss: 0.104\n",
      "Step: 2103, Loss: 0.104\n",
      "Step: 2104, Loss: 0.104\n",
      "Step: 2105, Loss: 0.103\n",
      "Step: 2106, Loss: 0.105\n",
      "Step: 2107, Loss: 0.104\n",
      "Step: 2108, Loss: 0.104\n",
      "Step: 2109, Loss: 0.103\n",
      "Step: 2110, Loss: 0.105\n",
      "Step: 2111, Loss: 0.104\n",
      "Step: 2112, Loss: 0.104\n",
      "Step: 2113, Loss: 0.104\n",
      "Step: 2114, Loss: 0.104\n",
      "Step: 2115, Loss: 0.104\n",
      "Step: 2116, Loss: 0.103\n",
      "Step: 2117, Loss: 0.104\n",
      "Step: 2118, Loss: 0.104\n",
      "Step: 2119, Loss: 0.104\n",
      "Step: 2120, Loss: 0.103\n",
      "Step: 2121, Loss: 0.104\n",
      "Step: 2122, Loss: 0.104\n",
      "Step: 2123, Loss: 0.103\n",
      "Step: 2124, Loss: 0.104\n",
      "Step: 2125, Loss: 0.104\n",
      "Step: 2126, Loss: 0.104\n",
      "Step: 2127, Loss: 0.103\n",
      "Step: 2128, Loss: 0.104\n",
      "Step: 2129, Loss: 0.104\n",
      "Step: 2130, Loss: 0.103\n",
      "Step: 2131, Loss: 0.104\n",
      "Step: 2132, Loss: 0.104\n",
      "Step: 2133, Loss: 0.103\n",
      "Step: 2134, Loss: 0.103\n",
      "Step: 2135, Loss: 0.104\n",
      "Step: 2136, Loss: 0.104\n",
      "Step: 2137, Loss: 0.103\n",
      "Step: 2138, Loss: 0.103\n",
      "Step: 2139, Loss: 0.104\n",
      "Step: 2140, Loss: 0.103\n",
      "Step: 2141, Loss: 0.103\n",
      "Step: 2142, Loss: 0.104\n",
      "Step: 2143, Loss: 0.103\n",
      "Step: 2144, Loss: 0.103\n",
      "Step: 2145, Loss: 0.103\n",
      "Step: 2146, Loss: 0.104\n",
      "Step: 2147, Loss: 0.103\n",
      "Step: 2148, Loss: 0.103\n",
      "Step: 2149, Loss: 0.103\n",
      "Step: 2150, Loss: 0.103\n",
      "Step: 2151, Loss: 0.103\n",
      "Step: 2152, Loss: 0.102\n",
      "Step: 2153, Loss: 0.104\n",
      "Step: 2154, Loss: 0.103\n",
      "Step: 2155, Loss: 0.103\n",
      "Step: 2156, Loss: 0.103\n",
      "Step: 2157, Loss: 0.104\n",
      "Step: 2158, Loss: 0.103\n",
      "Step: 2159, Loss: 0.103\n",
      "Step: 2160, Loss: 0.103\n",
      "Step: 2161, Loss: 0.103\n",
      "Step: 2162, Loss: 0.103\n",
      "Step: 2163, Loss: 0.102\n",
      "Step: 2164, Loss: 0.103\n",
      "Step: 2165, Loss: 0.103\n",
      "Step: 2166, Loss: 0.102\n",
      "Step: 2167, Loss: 0.103\n",
      "Step: 2168, Loss: 0.103\n",
      "Step: 2169, Loss: 0.103\n",
      "Step: 2170, Loss: 0.102\n",
      "Step: 2171, Loss: 0.103\n",
      "Step: 2172, Loss: 0.103\n",
      "Step: 2173, Loss: 0.103\n",
      "Step: 2174, Loss: 0.102\n",
      "Step: 2175, Loss: 0.103\n",
      "Step: 2176, Loss: 0.103\n",
      "Step: 2177, Loss: 0.102\n",
      "Step: 2178, Loss: 0.103\n",
      "Step: 2179, Loss: 0.103\n",
      "Step: 2180, Loss: 0.102\n",
      "Step: 2181, Loss: 0.102\n",
      "Step: 2182, Loss: 0.103\n",
      "Step: 2183, Loss: 0.103\n",
      "Step: 2184, Loss: 0.102\n",
      "Step: 2185, Loss: 0.102\n",
      "Step: 2186, Loss: 0.103\n",
      "Step: 2187, Loss: 0.102\n",
      "Step: 2188, Loss: 0.102\n",
      "Step: 2189, Loss: 0.103\n",
      "Step: 2190, Loss: 0.103\n",
      "Step: 2191, Loss: 0.102\n",
      "Step: 2192, Loss: 0.102\n",
      "Step: 2193, Loss: 0.103\n",
      "Step: 2194, Loss: 0.102\n",
      "Step: 2195, Loss: 0.102\n",
      "Step: 2196, Loss: 0.102\n",
      "Step: 2197, Loss: 0.102\n",
      "Step: 2198, Loss: 0.102\n",
      "Step: 2199, Loss: 0.101\n",
      "Step: 2200, Loss: 0.103\n",
      "Step: 2201, Loss: 0.102\n",
      "Step: 2202, Loss: 0.102\n",
      "Step: 2203, Loss: 0.102\n",
      "Step: 2204, Loss: 0.102\n",
      "Step: 2205, Loss: 0.102\n",
      "Step: 2206, Loss: 0.102\n",
      "Step: 2207, Loss: 0.102\n",
      "Step: 2208, Loss: 0.102\n",
      "Step: 2209, Loss: 0.102\n",
      "Step: 2210, Loss: 0.101\n",
      "Step: 2211, Loss: 0.103\n",
      "Step: 2212, Loss: 0.102\n",
      "Step: 2213, Loss: 0.101\n",
      "Step: 2214, Loss: 0.102\n",
      "Step: 2215, Loss: 0.102\n",
      "Step: 2216, Loss: 0.102\n",
      "Step: 2217, Loss: 0.101\n",
      "Step: 2218, Loss: 0.102\n",
      "Step: 2219, Loss: 0.102\n",
      "Step: 2220, Loss: 0.102\n",
      "Step: 2221, Loss: 0.101\n",
      "Step: 2222, Loss: 0.102\n",
      "Step: 2223, Loss: 0.102\n",
      "Step: 2224, Loss: 0.101\n",
      "Step: 2225, Loss: 0.102\n",
      "Step: 2226, Loss: 0.102\n",
      "Step: 2227, Loss: 0.101\n",
      "Step: 2228, Loss: 0.101\n",
      "Step: 2229, Loss: 0.102\n",
      "Step: 2230, Loss: 0.102\n",
      "Step: 2231, Loss: 0.101\n",
      "Step: 2232, Loss: 0.101\n",
      "Step: 2233, Loss: 0.102\n",
      "Step: 2234, Loss: 0.101\n",
      "Step: 2235, Loss: 0.101\n",
      "Step: 2236, Loss: 0.102\n",
      "Step: 2237, Loss: 0.102\n",
      "Step: 2238, Loss: 0.101\n",
      "Step: 2239, Loss: 0.101\n",
      "Step: 2240, Loss: 0.102\n",
      "Step: 2241, Loss: 0.101\n",
      "Step: 2242, Loss: 0.101\n",
      "Step: 2243, Loss: 0.102\n",
      "Step: 2244, Loss: 0.101\n",
      "Step: 2245, Loss: 0.101\n",
      "Step: 2246, Loss: 0.100\n",
      "Step: 2247, Loss: 0.102\n",
      "Step: 2248, Loss: 0.101\n",
      "Step: 2249, Loss: 0.101\n",
      "Step: 2250, Loss: 0.101\n",
      "Step: 2251, Loss: 0.102\n",
      "Step: 2252, Loss: 0.101\n",
      "Step: 2253, Loss: 0.101\n",
      "Step: 2254, Loss: 0.101\n",
      "Step: 2255, Loss: 0.101\n",
      "Step: 2256, Loss: 0.101\n",
      "Step: 2257, Loss: 0.100\n",
      "Step: 2258, Loss: 0.102\n",
      "Step: 2259, Loss: 0.101\n",
      "Step: 2260, Loss: 0.101\n",
      "Step: 2261, Loss: 0.101\n",
      "Step: 2262, Loss: 0.101\n",
      "Step: 2263, Loss: 0.101\n",
      "Step: 2264, Loss: 0.100\n",
      "Step: 2265, Loss: 0.102\n",
      "Step: 2266, Loss: 0.101\n",
      "Step: 2267, Loss: 0.101\n",
      "Step: 2268, Loss: 0.100\n",
      "Step: 2269, Loss: 0.101\n",
      "Step: 2270, Loss: 0.101\n",
      "Step: 2271, Loss: 0.100\n",
      "Step: 2272, Loss: 0.101\n",
      "Step: 2273, Loss: 0.101\n",
      "Step: 2274, Loss: 0.100\n",
      "Step: 2275, Loss: 0.100\n",
      "Step: 2276, Loss: 0.101\n",
      "Step: 2277, Loss: 0.101\n",
      "Step: 2278, Loss: 0.100\n",
      "Step: 2279, Loss: 0.100\n",
      "Step: 2280, Loss: 0.101\n",
      "Step: 2281, Loss: 0.100\n",
      "Step: 2282, Loss: 0.100\n",
      "Step: 2283, Loss: 0.101\n",
      "Step: 2284, Loss: 0.101\n",
      "Step: 2285, Loss: 0.100\n",
      "Step: 2286, Loss: 0.100\n",
      "Step: 2287, Loss: 0.101\n",
      "Step: 2288, Loss: 0.100\n",
      "Step: 2289, Loss: 0.100\n",
      "Step: 2290, Loss: 0.101\n",
      "Step: 2291, Loss: 0.100\n",
      "Step: 2292, Loss: 0.100\n",
      "Step: 2293, Loss: 0.100\n",
      "Step: 2294, Loss: 0.101\n",
      "Step: 2295, Loss: 0.101\n",
      "Step: 2296, Loss: 0.100\n",
      "Step: 2297, Loss: 0.100\n",
      "Step: 2298, Loss: 0.101\n",
      "Step: 2299, Loss: 0.100\n",
      "Step: 2300, Loss: 0.100\n",
      "Step: 2301, Loss: 0.100\n",
      "Step: 2302, Loss: 0.100\n",
      "Step: 2303, Loss: 0.100\n",
      "Step: 2304, Loss: 0.099\n",
      "Step: 2305, Loss: 0.101\n",
      "Step: 2306, Loss: 0.100\n",
      "Step: 2307, Loss: 0.100\n",
      "Step: 2308, Loss: 0.100\n",
      "Step: 2309, Loss: 0.100\n",
      "Step: 2310, Loss: 0.100\n",
      "Step: 2311, Loss: 0.099\n",
      "Step: 2312, Loss: 0.101\n",
      "Step: 2313, Loss: 0.100\n",
      "Step: 2314, Loss: 0.100\n",
      "Step: 2315, Loss: 0.100\n",
      "Step: 2316, Loss: 0.100\n",
      "Step: 2317, Loss: 0.100\n",
      "Step: 2318, Loss: 0.100\n",
      "Step: 2319, Loss: 0.100\n",
      "Step: 2320, Loss: 0.100\n",
      "Step: 2321, Loss: 0.100\n",
      "Step: 2322, Loss: 0.099\n",
      "Step: 2323, Loss: 0.101\n",
      "Step: 2324, Loss: 0.100\n",
      "Step: 2325, Loss: 0.099\n",
      "Step: 2326, Loss: 0.100\n",
      "Step: 2327, Loss: 0.100\n",
      "Step: 2328, Loss: 0.100\n",
      "Step: 2329, Loss: 0.099\n",
      "Step: 2330, Loss: 0.100\n",
      "Step: 2331, Loss: 0.100\n",
      "Step: 2332, Loss: 0.099\n",
      "Step: 2333, Loss: 0.099\n",
      "Step: 2334, Loss: 0.100\n",
      "Step: 2335, Loss: 0.099\n",
      "Step: 2336, Loss: 0.099\n",
      "Step: 2337, Loss: 0.100\n",
      "Step: 2338, Loss: 0.100\n",
      "Step: 2339, Loss: 0.099\n",
      "Step: 2340, Loss: 0.099\n",
      "Step: 2341, Loss: 0.100\n",
      "Step: 2342, Loss: 0.100\n",
      "Step: 2343, Loss: 0.099\n",
      "Step: 2344, Loss: 0.099\n",
      "Step: 2345, Loss: 0.100\n",
      "Step: 2346, Loss: 0.099\n",
      "Step: 2347, Loss: 0.099\n",
      "Step: 2348, Loss: 0.100\n",
      "Step: 2349, Loss: 0.099\n",
      "Step: 2350, Loss: 0.099\n",
      "Step: 2351, Loss: 0.099\n",
      "Step: 2352, Loss: 0.100\n",
      "Step: 2353, Loss: 0.099\n",
      "Step: 2354, Loss: 0.099\n",
      "Step: 2355, Loss: 0.099\n",
      "Step: 2356, Loss: 0.099\n",
      "Step: 2357, Loss: 0.099\n",
      "Step: 2358, Loss: 0.098\n",
      "Step: 2359, Loss: 0.100\n",
      "Step: 2360, Loss: 0.099\n",
      "Step: 2361, Loss: 0.099\n",
      "Step: 2362, Loss: 0.099\n",
      "Step: 2363, Loss: 0.099\n",
      "Step: 2364, Loss: 0.099\n",
      "Step: 2365, Loss: 0.099\n",
      "Step: 2366, Loss: 0.099\n",
      "Step: 2367, Loss: 0.099\n",
      "Step: 2368, Loss: 0.099\n",
      "Step: 2369, Loss: 0.098\n",
      "Step: 2370, Loss: 0.100\n",
      "Step: 2371, Loss: 0.099\n",
      "Step: 2372, Loss: 0.098\n",
      "Step: 2373, Loss: 0.099\n",
      "Step: 2374, Loss: 0.099\n",
      "Step: 2375, Loss: 0.099\n",
      "Step: 2376, Loss: 0.098\n",
      "Step: 2377, Loss: 0.099\n",
      "Step: 2378, Loss: 0.099\n",
      "Step: 2379, Loss: 0.099\n",
      "Step: 2380, Loss: 0.098\n",
      "Step: 2381, Loss: 0.099\n",
      "Step: 2382, Loss: 0.099\n",
      "Step: 2383, Loss: 0.098\n",
      "Step: 2384, Loss: 0.099\n",
      "Step: 2385, Loss: 0.099\n",
      "Step: 2386, Loss: 0.098\n",
      "Step: 2387, Loss: 0.098\n",
      "Step: 2388, Loss: 0.099\n",
      "Step: 2389, Loss: 0.099\n",
      "Step: 2390, Loss: 0.098\n",
      "Step: 2391, Loss: 0.098\n",
      "Step: 2392, Loss: 0.099\n",
      "Step: 2393, Loss: 0.098\n",
      "Step: 2394, Loss: 0.098\n",
      "Step: 2395, Loss: 0.099\n",
      "Step: 2396, Loss: 0.099\n",
      "Step: 2397, Loss: 0.098\n",
      "Step: 2398, Loss: 0.098\n",
      "Step: 2399, Loss: 0.099\n",
      "Step: 2400, Loss: 0.098\n",
      "Step: 2401, Loss: 0.098\n",
      "Step: 2402, Loss: 0.099\n",
      "Step: 2403, Loss: 0.098\n",
      "Step: 2404, Loss: 0.098\n",
      "Step: 2405, Loss: 0.098\n",
      "Step: 2406, Loss: 0.099\n",
      "Step: 2407, Loss: 0.098\n",
      "Step: 2408, Loss: 0.098\n",
      "Step: 2409, Loss: 0.098\n",
      "Step: 2410, Loss: 0.099\n",
      "Step: 2411, Loss: 0.098\n",
      "Step: 2412, Loss: 0.098\n",
      "Step: 2413, Loss: 0.098\n",
      "Step: 2414, Loss: 0.098\n",
      "Step: 2415, Loss: 0.098\n",
      "Step: 2416, Loss: 0.097\n",
      "Step: 2417, Loss: 0.099\n",
      "Step: 2418, Loss: 0.098\n",
      "Step: 2419, Loss: 0.098\n",
      "Step: 2420, Loss: 0.098\n",
      "Step: 2421, Loss: 0.098\n",
      "Step: 2422, Loss: 0.098\n",
      "Step: 2423, Loss: 0.097\n",
      "Step: 2424, Loss: 0.098\n",
      "Step: 2425, Loss: 0.098\n",
      "Step: 2426, Loss: 0.098\n",
      "Step: 2427, Loss: 0.097\n",
      "Step: 2428, Loss: 0.098\n",
      "Step: 2429, Loss: 0.098\n",
      "Step: 2430, Loss: 0.097\n",
      "Step: 2431, Loss: 0.098\n",
      "Step: 2432, Loss: 0.098\n",
      "Step: 2433, Loss: 0.098\n",
      "Step: 2434, Loss: 0.097\n",
      "Step: 2435, Loss: 0.098\n",
      "Step: 2436, Loss: 0.098\n",
      "Step: 2437, Loss: 0.097\n",
      "Step: 2438, Loss: 0.098\n",
      "Step: 2439, Loss: 0.098\n",
      "Step: 2440, Loss: 0.098\n",
      "Step: 2441, Loss: 0.097\n",
      "Step: 2442, Loss: 0.098\n",
      "Step: 2443, Loss: 0.098\n",
      "Step: 2444, Loss: 0.097\n",
      "Step: 2445, Loss: 0.097\n",
      "Step: 2446, Loss: 0.098\n",
      "Step: 2447, Loss: 0.097\n",
      "Step: 2448, Loss: 0.097\n",
      "Step: 2449, Loss: 0.098\n",
      "Step: 2450, Loss: 0.098\n",
      "Step: 2451, Loss: 0.097\n",
      "Step: 2452, Loss: 0.097\n",
      "Step: 2453, Loss: 0.098\n",
      "Step: 2454, Loss: 0.098\n",
      "Step: 2455, Loss: 0.097\n",
      "Step: 2456, Loss: 0.097\n",
      "Step: 2457, Loss: 0.098\n",
      "Step: 2458, Loss: 0.097\n",
      "Step: 2459, Loss: 0.097\n",
      "Step: 2460, Loss: 0.098\n",
      "Step: 2461, Loss: 0.097\n",
      "Step: 2462, Loss: 0.097\n",
      "Step: 2463, Loss: 0.097\n",
      "Step: 2464, Loss: 0.098\n",
      "Step: 2465, Loss: 0.097\n",
      "Step: 2466, Loss: 0.097\n",
      "Step: 2467, Loss: 0.097\n",
      "Step: 2468, Loss: 0.097\n",
      "Step: 2469, Loss: 0.097\n",
      "Step: 2470, Loss: 0.096\n",
      "Step: 2471, Loss: 0.098\n",
      "Step: 2472, Loss: 0.097\n",
      "Step: 2473, Loss: 0.097\n",
      "Step: 2474, Loss: 0.097\n",
      "Step: 2475, Loss: 0.097\n",
      "Step: 2476, Loss: 0.097\n",
      "Step: 2477, Loss: 0.097\n",
      "Step: 2478, Loss: 0.097\n",
      "Step: 2479, Loss: 0.097\n",
      "Step: 2480, Loss: 0.097\n",
      "Step: 2481, Loss: 0.096\n",
      "Step: 2482, Loss: 0.098\n",
      "Step: 2483, Loss: 0.097\n",
      "Step: 2484, Loss: 0.096\n",
      "Step: 2485, Loss: 0.097\n",
      "Step: 2486, Loss: 0.097\n",
      "Step: 2487, Loss: 0.097\n",
      "Step: 2488, Loss: 0.096\n",
      "Step: 2489, Loss: 0.098\n",
      "Step: 2490, Loss: 0.097\n",
      "Step: 2491, Loss: 0.097\n",
      "Step: 2492, Loss: 0.096\n",
      "Step: 2493, Loss: 0.097\n",
      "Step: 2494, Loss: 0.097\n",
      "Step: 2495, Loss: 0.096\n",
      "Step: 2496, Loss: 0.097\n",
      "Step: 2497, Loss: 0.097\n",
      "Step: 2498, Loss: 0.097\n",
      "Step: 2499, Loss: 0.096\n",
      "Step: 2500, Loss: 0.097\n",
      "Step: 2501, Loss: 0.097\n",
      "Step: 2502, Loss: 0.096\n",
      "Step: 2503, Loss: 0.096\n",
      "Step: 2504, Loss: 0.097\n",
      "Step: 2505, Loss: 0.096\n",
      "Step: 2506, Loss: 0.096\n",
      "Step: 2507, Loss: 0.097\n",
      "Step: 2508, Loss: 0.097\n",
      "Step: 2509, Loss: 0.096\n",
      "Step: 2510, Loss: 0.096\n",
      "Step: 2511, Loss: 0.097\n",
      "Step: 2512, Loss: 0.096\n",
      "Step: 2513, Loss: 0.096\n",
      "Step: 2514, Loss: 0.097\n",
      "Step: 2515, Loss: 0.096\n",
      "Step: 2516, Loss: 0.096\n",
      "Step: 2517, Loss: 0.096\n",
      "Step: 2518, Loss: 0.097\n",
      "Step: 2519, Loss: 0.096\n",
      "Step: 2520, Loss: 0.096\n",
      "Step: 2521, Loss: 0.096\n",
      "Step: 2522, Loss: 0.097\n",
      "Step: 2523, Loss: 0.096\n",
      "Step: 2524, Loss: 0.096\n",
      "Step: 2525, Loss: 0.096\n",
      "Step: 2526, Loss: 0.096\n",
      "Step: 2527, Loss: 0.096\n",
      "Step: 2528, Loss: 0.095\n",
      "Step: 2529, Loss: 0.097\n",
      "Step: 2530, Loss: 0.096\n",
      "Step: 2531, Loss: 0.096\n",
      "Step: 2532, Loss: 0.096\n",
      "Step: 2533, Loss: 0.096\n",
      "Step: 2534, Loss: 0.096\n",
      "Step: 2535, Loss: 0.095\n",
      "Step: 2536, Loss: 0.096\n",
      "Step: 2537, Loss: 0.096\n",
      "Step: 2538, Loss: 0.096\n",
      "Step: 2539, Loss: 0.096\n",
      "Step: 2540, Loss: 0.096\n",
      "Step: 2541, Loss: 0.096\n",
      "Step: 2542, Loss: 0.096\n",
      "Step: 2543, Loss: 0.096\n",
      "Step: 2544, Loss: 0.096\n",
      "Step: 2545, Loss: 0.096\n",
      "Step: 2546, Loss: 0.095\n",
      "Step: 2547, Loss: 0.096\n",
      "Step: 2548, Loss: 0.096\n",
      "Step: 2549, Loss: 0.095\n",
      "Step: 2550, Loss: 0.096\n",
      "Step: 2551, Loss: 0.096\n",
      "Step: 2552, Loss: 0.096\n",
      "Step: 2553, Loss: 0.095\n",
      "Step: 2554, Loss: 0.096\n",
      "Step: 2555, Loss: 0.096\n",
      "Step: 2556, Loss: 0.095\n",
      "Step: 2557, Loss: 0.095\n",
      "Step: 2558, Loss: 0.096\n",
      "Step: 2559, Loss: 0.095\n",
      "Step: 2560, Loss: 0.095\n",
      "Step: 2561, Loss: 0.096\n",
      "Step: 2562, Loss: 0.096\n",
      "Step: 2563, Loss: 0.095\n",
      "Step: 2564, Loss: 0.095\n",
      "Step: 2565, Loss: 0.096\n",
      "Step: 2566, Loss: 0.096\n",
      "Step: 2567, Loss: 0.095\n",
      "Step: 2568, Loss: 0.095\n",
      "Step: 2569, Loss: 0.096\n",
      "Step: 2570, Loss: 0.095\n",
      "Step: 2571, Loss: 0.095\n",
      "Step: 2572, Loss: 0.096\n",
      "Step: 2573, Loss: 0.096\n",
      "Step: 2574, Loss: 0.095\n",
      "Step: 2575, Loss: 0.095\n",
      "Step: 2576, Loss: 0.096\n",
      "Step: 2577, Loss: 0.095\n",
      "Step: 2578, Loss: 0.095\n",
      "Step: 2579, Loss: 0.095\n",
      "Step: 2580, Loss: 0.095\n",
      "Step: 2581, Loss: 0.095\n",
      "Step: 2582, Loss: 0.094\n",
      "Step: 2583, Loss: 0.096\n",
      "Step: 2584, Loss: 0.095\n",
      "Step: 2585, Loss: 0.095\n",
      "Step: 2586, Loss: 0.095\n",
      "Step: 2587, Loss: 0.095\n",
      "Step: 2588, Loss: 0.095\n",
      "Step: 2589, Loss: 0.095\n",
      "Step: 2590, Loss: 0.095\n",
      "Step: 2591, Loss: 0.095\n",
      "Step: 2592, Loss: 0.095\n",
      "Step: 2593, Loss: 0.094\n",
      "Step: 2594, Loss: 0.096\n",
      "Step: 2595, Loss: 0.095\n",
      "Step: 2596, Loss: 0.095\n",
      "Step: 2597, Loss: 0.095\n",
      "Step: 2598, Loss: 0.095\n",
      "Step: 2599, Loss: 0.095\n",
      "Step: 2600, Loss: 0.094\n",
      "Step: 2601, Loss: 0.095\n",
      "Step: 2602, Loss: 0.095\n",
      "Step: 2603, Loss: 0.095\n",
      "Step: 2604, Loss: 0.094\n",
      "Step: 2605, Loss: 0.095\n",
      "Step: 2606, Loss: 0.095\n",
      "Step: 2607, Loss: 0.094\n",
      "Step: 2608, Loss: 0.095\n",
      "Step: 2609, Loss: 0.095\n",
      "Step: 2610, Loss: 0.095\n",
      "Step: 2611, Loss: 0.094\n",
      "Step: 2612, Loss: 0.095\n",
      "Step: 2613, Loss: 0.095\n",
      "Step: 2614, Loss: 0.094\n",
      "Step: 2615, Loss: 0.095\n",
      "Step: 2616, Loss: 0.095\n",
      "Step: 2617, Loss: 0.094\n",
      "Step: 2618, Loss: 0.094\n",
      "Step: 2619, Loss: 0.095\n",
      "Step: 2620, Loss: 0.095\n",
      "Step: 2621, Loss: 0.094\n",
      "Step: 2622, Loss: 0.094\n",
      "Step: 2623, Loss: 0.095\n",
      "Step: 2624, Loss: 0.094\n",
      "Step: 2625, Loss: 0.094\n",
      "Step: 2626, Loss: 0.095\n",
      "Step: 2627, Loss: 0.094\n",
      "Step: 2628, Loss: 0.094\n",
      "Step: 2629, Loss: 0.094\n",
      "Step: 2630, Loss: 0.095\n",
      "Step: 2631, Loss: 0.094\n",
      "Step: 2632, Loss: 0.094\n",
      "Step: 2633, Loss: 0.094\n",
      "Step: 2634, Loss: 0.095\n",
      "Step: 2635, Loss: 0.094\n",
      "Step: 2636, Loss: 0.094\n",
      "Step: 2637, Loss: 0.095\n",
      "Step: 2638, Loss: 0.094\n",
      "Step: 2639, Loss: 0.094\n",
      "Step: 2640, Loss: 0.094\n",
      "Step: 2641, Loss: 0.095\n",
      "Step: 2642, Loss: 0.094\n",
      "Step: 2643, Loss: 0.094\n",
      "Step: 2644, Loss: 0.094\n",
      "Step: 2645, Loss: 0.094\n",
      "Step: 2646, Loss: 0.094\n",
      "Step: 2647, Loss: 0.093\n",
      "Step: 2648, Loss: 0.095\n",
      "Step: 2649, Loss: 0.094\n",
      "Step: 2650, Loss: 0.094\n",
      "Step: 2651, Loss: 0.094\n",
      "Step: 2652, Loss: 0.094\n",
      "Step: 2653, Loss: 0.094\n",
      "Step: 2654, Loss: 0.094\n",
      "Step: 2655, Loss: 0.094\n",
      "Step: 2656, Loss: 0.094\n",
      "Step: 2657, Loss: 0.094\n",
      "Step: 2658, Loss: 0.093\n",
      "Step: 2659, Loss: 0.095\n",
      "Step: 2660, Loss: 0.094\n",
      "Step: 2661, Loss: 0.093\n",
      "Step: 2662, Loss: 0.094\n",
      "Step: 2663, Loss: 0.094\n",
      "Step: 2664, Loss: 0.094\n",
      "Step: 2665, Loss: 0.093\n",
      "Step: 2666, Loss: 0.094\n",
      "Step: 2667, Loss: 0.094\n",
      "Step: 2668, Loss: 0.094\n",
      "Step: 2669, Loss: 0.093\n",
      "Step: 2670, Loss: 0.094\n",
      "Step: 2671, Loss: 0.094\n",
      "Step: 2672, Loss: 0.093\n",
      "Step: 2673, Loss: 0.094\n",
      "Step: 2674, Loss: 0.094\n",
      "Step: 2675, Loss: 0.094\n",
      "Step: 2676, Loss: 0.093\n",
      "Step: 2677, Loss: 0.094\n",
      "Step: 2678, Loss: 0.094\n",
      "Step: 2679, Loss: 0.093\n",
      "Step: 2680, Loss: 0.093\n",
      "Step: 2681, Loss: 0.094\n",
      "Step: 2682, Loss: 0.093\n",
      "Step: 2683, Loss: 0.093\n",
      "Step: 2684, Loss: 0.094\n",
      "Step: 2685, Loss: 0.094\n",
      "Step: 2686, Loss: 0.093\n",
      "Step: 2687, Loss: 0.093\n",
      "Step: 2688, Loss: 0.094\n",
      "Step: 2689, Loss: 0.093\n",
      "Step: 2690, Loss: 0.093\n",
      "Step: 2691, Loss: 0.093\n",
      "Step: 2692, Loss: 0.094\n",
      "Step: 2693, Loss: 0.093\n",
      "Step: 2694, Loss: 0.093\n",
      "Step: 2695, Loss: 0.094\n",
      "Step: 2696, Loss: 0.094\n",
      "Step: 2697, Loss: 0.093\n",
      "Step: 2698, Loss: 0.093\n",
      "Step: 2699, Loss: 0.094\n",
      "Step: 2700, Loss: 0.093\n",
      "Step: 2701, Loss: 0.093\n",
      "Step: 2702, Loss: 0.093\n",
      "Step: 2703, Loss: 0.093\n",
      "Step: 2704, Loss: 0.093\n",
      "Step: 2705, Loss: 0.092\n",
      "Step: 2706, Loss: 0.094\n",
      "Step: 2707, Loss: 0.093\n",
      "Step: 2708, Loss: 0.093\n",
      "Step: 2709, Loss: 0.093\n",
      "Step: 2710, Loss: 0.093\n",
      "Step: 2711, Loss: 0.093\n",
      "Step: 2712, Loss: 0.092\n",
      "Step: 2713, Loss: 0.094\n",
      "Step: 2714, Loss: 0.093\n",
      "Step: 2715, Loss: 0.093\n",
      "Step: 2716, Loss: 0.093\n",
      "Step: 2717, Loss: 0.093\n",
      "Step: 2718, Loss: 0.093\n",
      "Step: 2719, Loss: 0.093\n",
      "Step: 2720, Loss: 0.093\n",
      "Step: 2721, Loss: 0.093\n",
      "Step: 2722, Loss: 0.093\n",
      "Step: 2723, Loss: 0.092\n",
      "Step: 2724, Loss: 0.094\n",
      "Step: 2725, Loss: 0.093\n",
      "Step: 2726, Loss: 0.092\n",
      "Step: 2727, Loss: 0.093\n",
      "Step: 2728, Loss: 0.093\n",
      "Step: 2729, Loss: 0.093\n",
      "Step: 2730, Loss: 0.092\n",
      "Step: 2731, Loss: 0.093\n",
      "Step: 2732, Loss: 0.093\n",
      "Step: 2733, Loss: 0.093\n",
      "Step: 2734, Loss: 0.092\n",
      "Step: 2735, Loss: 0.093\n",
      "Step: 2736, Loss: 0.093\n",
      "Step: 2737, Loss: 0.092\n",
      "Step: 2738, Loss: 0.093\n",
      "Step: 2739, Loss: 0.093\n",
      "Step: 2740, Loss: 0.093\n",
      "Step: 2741, Loss: 0.092\n",
      "Step: 2742, Loss: 0.093\n",
      "Step: 2743, Loss: 0.093\n",
      "Step: 2744, Loss: 0.092\n",
      "Step: 2745, Loss: 0.092\n",
      "Step: 2746, Loss: 0.093\n",
      "Step: 2747, Loss: 0.092\n",
      "Step: 2748, Loss: 0.092\n",
      "Step: 2749, Loss: 0.093\n",
      "Step: 2750, Loss: 0.093\n",
      "Step: 2751, Loss: 0.092\n",
      "Step: 2752, Loss: 0.092\n",
      "Step: 2753, Loss: 0.093\n",
      "Step: 2754, Loss: 0.092\n",
      "Step: 2755, Loss: 0.092\n",
      "Step: 2756, Loss: 0.093\n",
      "Step: 2757, Loss: 0.093\n",
      "Step: 2758, Loss: 0.092\n",
      "Step: 2759, Loss: 0.092\n",
      "Step: 2760, Loss: 0.093\n",
      "Step: 2761, Loss: 0.092\n",
      "Step: 2762, Loss: 0.092\n",
      "Step: 2763, Loss: 0.092\n",
      "Step: 2764, Loss: 0.093\n",
      "Step: 2765, Loss: 0.092\n",
      "Step: 2766, Loss: 0.092\n",
      "Step: 2767, Loss: 0.092\n",
      "Step: 2768, Loss: 0.092\n",
      "Step: 2769, Loss: 0.092\n",
      "Step: 2770, Loss: 0.091\n",
      "Step: 2771, Loss: 0.093\n",
      "Step: 2772, Loss: 0.092\n",
      "Step: 2773, Loss: 0.092\n",
      "Step: 2774, Loss: 0.092\n",
      "Step: 2775, Loss: 0.092\n",
      "Step: 2776, Loss: 0.092\n",
      "Step: 2777, Loss: 0.091\n",
      "Step: 2778, Loss: 0.093\n",
      "Step: 2779, Loss: 0.092\n",
      "Step: 2780, Loss: 0.092\n",
      "Step: 2781, Loss: 0.092\n",
      "Step: 2782, Loss: 0.092\n",
      "Step: 2783, Loss: 0.092\n",
      "Step: 2784, Loss: 0.092\n",
      "Step: 2785, Loss: 0.092\n",
      "Step: 2786, Loss: 0.092\n",
      "Step: 2787, Loss: 0.092\n",
      "Step: 2788, Loss: 0.091\n",
      "Step: 2789, Loss: 0.092\n",
      "Step: 2790, Loss: 0.092\n",
      "Step: 2791, Loss: 0.091\n",
      "Step: 2792, Loss: 0.092\n",
      "Step: 2793, Loss: 0.092\n",
      "Step: 2794, Loss: 0.092\n",
      "Step: 2795, Loss: 0.091\n",
      "Step: 2796, Loss: 0.092\n",
      "Step: 2797, Loss: 0.092\n",
      "Step: 2798, Loss: 0.092\n",
      "Step: 2799, Loss: 0.091\n",
      "Step: 2800, Loss: 0.092\n",
      "Step: 2801, Loss: 0.092\n",
      "Step: 2802, Loss: 0.091\n",
      "Step: 2803, Loss: 0.092\n",
      "Step: 2804, Loss: 0.092\n",
      "Step: 2805, Loss: 0.091\n",
      "Step: 2806, Loss: 0.091\n",
      "Step: 2807, Loss: 0.092\n",
      "Step: 2808, Loss: 0.092\n",
      "Step: 2809, Loss: 0.091\n",
      "Step: 2810, Loss: 0.091\n",
      "Step: 2811, Loss: 0.092\n",
      "Step: 2812, Loss: 0.091\n",
      "Step: 2813, Loss: 0.091\n",
      "Step: 2814, Loss: 0.092\n",
      "Step: 2815, Loss: 0.092\n",
      "Step: 2816, Loss: 0.091\n",
      "Step: 2817, Loss: 0.091\n",
      "Step: 2818, Loss: 0.092\n",
      "Step: 2819, Loss: 0.091\n",
      "Step: 2820, Loss: 0.091\n",
      "Step: 2821, Loss: 0.092\n",
      "Step: 2822, Loss: 0.091\n",
      "Step: 2823, Loss: 0.091\n",
      "Step: 2824, Loss: 0.091\n",
      "Step: 2825, Loss: 0.092\n",
      "Step: 2826, Loss: 0.091\n",
      "Step: 2827, Loss: 0.091\n",
      "Step: 2828, Loss: 0.091\n",
      "Step: 2829, Loss: 0.092\n",
      "Step: 2830, Loss: 0.091\n",
      "Step: 2831, Loss: 0.091\n",
      "Step: 2832, Loss: 0.092\n",
      "Step: 2833, Loss: 0.091\n",
      "Step: 2834, Loss: 0.091\n",
      "Step: 2835, Loss: 0.090\n",
      "Step: 2836, Loss: 0.092\n",
      "Step: 2837, Loss: 0.091\n",
      "Step: 2838, Loss: 0.091\n",
      "Step: 2839, Loss: 0.091\n",
      "Step: 2840, Loss: 0.091\n",
      "Step: 2841, Loss: 0.091\n",
      "Step: 2842, Loss: 0.090\n",
      "Step: 2843, Loss: 0.091\n",
      "Step: 2844, Loss: 0.091\n",
      "Step: 2845, Loss: 0.091\n",
      "Step: 2846, Loss: 0.090\n",
      "Step: 2847, Loss: 0.091\n",
      "Step: 2848, Loss: 0.091\n",
      "Step: 2849, Loss: 0.091\n",
      "Step: 2850, Loss: 0.091\n",
      "Step: 2851, Loss: 0.091\n",
      "Step: 2852, Loss: 0.091\n",
      "Step: 2853, Loss: 0.090\n",
      "Step: 2854, Loss: 0.091\n",
      "Step: 2855, Loss: 0.091\n",
      "Step: 2856, Loss: 0.091\n",
      "Step: 2857, Loss: 0.090\n",
      "Step: 2858, Loss: 0.091\n",
      "Step: 2859, Loss: 0.091\n",
      "Step: 2860, Loss: 0.090\n",
      "Step: 2861, Loss: 0.091\n",
      "Step: 2862, Loss: 0.091\n",
      "Step: 2863, Loss: 0.091\n",
      "Step: 2864, Loss: 0.090\n",
      "Step: 2865, Loss: 0.091\n",
      "Step: 2866, Loss: 0.091\n",
      "Step: 2867, Loss: 0.090\n",
      "Step: 2868, Loss: 0.091\n",
      "Step: 2869, Loss: 0.091\n",
      "Step: 2870, Loss: 0.090\n",
      "Step: 2871, Loss: 0.090\n",
      "Step: 2872, Loss: 0.091\n",
      "Step: 2873, Loss: 0.091\n",
      "Step: 2874, Loss: 0.090\n",
      "Step: 2875, Loss: 0.090\n",
      "Step: 2876, Loss: 0.091\n",
      "Step: 2877, Loss: 0.090\n",
      "Step: 2878, Loss: 0.090\n",
      "Step: 2879, Loss: 0.091\n",
      "Step: 2880, Loss: 0.090\n",
      "Step: 2881, Loss: 0.090\n",
      "Step: 2882, Loss: 0.090\n",
      "Step: 2883, Loss: 0.091\n",
      "Step: 2884, Loss: 0.090\n",
      "Step: 2885, Loss: 0.090\n",
      "Step: 2886, Loss: 0.090\n",
      "Step: 2887, Loss: 0.090\n",
      "Step: 2888, Loss: 0.090\n",
      "Step: 2889, Loss: 0.089\n",
      "Step: 2890, Loss: 0.091\n",
      "Step: 2891, Loss: 0.090\n",
      "Step: 2892, Loss: 0.090\n",
      "Step: 2893, Loss: 0.090\n",
      "Step: 2894, Loss: 0.091\n",
      "Step: 2895, Loss: 0.090\n",
      "Step: 2896, Loss: 0.090\n",
      "Step: 2897, Loss: 0.090\n",
      "Step: 2898, Loss: 0.090\n",
      "Step: 2899, Loss: 0.090\n",
      "Step: 2900, Loss: 0.089\n",
      "Step: 2901, Loss: 0.091\n",
      "Step: 2902, Loss: 0.090\n",
      "Step: 2903, Loss: 0.090\n",
      "Step: 2904, Loss: 0.090\n",
      "Step: 2905, Loss: 0.090\n",
      "Step: 2906, Loss: 0.090\n",
      "Step: 2907, Loss: 0.089\n",
      "Step: 2908, Loss: 0.090\n",
      "Step: 2909, Loss: 0.090\n",
      "Step: 2910, Loss: 0.090\n",
      "Step: 2911, Loss: 0.089\n",
      "Step: 2912, Loss: 0.091\n",
      "Step: 2913, Loss: 0.090\n",
      "Step: 2914, Loss: 0.090\n",
      "Step: 2915, Loss: 0.090\n",
      "Step: 2916, Loss: 0.090\n",
      "Step: 2917, Loss: 0.090\n",
      "Step: 2918, Loss: 0.089\n",
      "Step: 2919, Loss: 0.090\n",
      "Step: 2920, Loss: 0.090\n",
      "Step: 2921, Loss: 0.089\n",
      "Step: 2922, Loss: 0.090\n",
      "Step: 2923, Loss: 0.090\n",
      "Step: 2924, Loss: 0.090\n",
      "Step: 2925, Loss: 0.089\n",
      "Step: 2926, Loss: 0.090\n",
      "Step: 2927, Loss: 0.090\n",
      "Step: 2928, Loss: 0.090\n",
      "Step: 2929, Loss: 0.089\n",
      "Step: 2930, Loss: 0.090\n",
      "Step: 2931, Loss: 0.090\n",
      "Step: 2932, Loss: 0.089\n",
      "Step: 2933, Loss: 0.090\n",
      "Step: 2934, Loss: 0.090\n",
      "Step: 2935, Loss: 0.090\n",
      "Step: 2936, Loss: 0.089\n",
      "Step: 2937, Loss: 0.090\n",
      "Step: 2938, Loss: 0.090\n",
      "Step: 2939, Loss: 0.089\n",
      "Step: 2940, Loss: 0.089\n",
      "Step: 2941, Loss: 0.090\n",
      "Step: 2942, Loss: 0.089\n",
      "Step: 2943, Loss: 0.089\n",
      "Step: 2944, Loss: 0.090\n",
      "Step: 2945, Loss: 0.089\n",
      "Step: 2946, Loss: 0.089\n",
      "Step: 2947, Loss: 0.089\n",
      "Step: 2948, Loss: 0.090\n",
      "Step: 2949, Loss: 0.089\n",
      "Step: 2950, Loss: 0.089\n",
      "Step: 2951, Loss: 0.089\n",
      "Step: 2952, Loss: 0.089\n",
      "Step: 2953, Loss: 0.089\n",
      "Step: 2954, Loss: 0.089\n",
      "Step: 2955, Loss: 0.090\n",
      "Step: 2956, Loss: 0.089\n",
      "Step: 2957, Loss: 0.089\n",
      "Step: 2958, Loss: 0.089\n",
      "Step: 2959, Loss: 0.090\n",
      "Step: 2960, Loss: 0.089\n",
      "Step: 2961, Loss: 0.089\n",
      "Step: 2962, Loss: 0.089\n",
      "Step: 2963, Loss: 0.089\n",
      "Step: 2964, Loss: 0.089\n",
      "Step: 2965, Loss: 0.089\n",
      "Step: 2966, Loss: 0.090\n",
      "Step: 2967, Loss: 0.089\n",
      "Step: 2968, Loss: 0.089\n",
      "Step: 2969, Loss: 0.089\n",
      "Step: 2970, Loss: 0.089\n",
      "Step: 2971, Loss: 0.089\n",
      "Step: 2972, Loss: 0.088\n",
      "Step: 2973, Loss: 0.089\n",
      "Step: 2974, Loss: 0.089\n",
      "Step: 2975, Loss: 0.089\n",
      "Step: 2976, Loss: 0.088\n",
      "Step: 2977, Loss: 0.090\n",
      "Step: 2978, Loss: 0.089\n",
      "Step: 2979, Loss: 0.089\n",
      "Step: 2980, Loss: 0.089\n",
      "Step: 2981, Loss: 0.089\n",
      "Step: 2982, Loss: 0.089\n",
      "Step: 2983, Loss: 0.088\n",
      "Step: 2984, Loss: 0.089\n",
      "Step: 2985, Loss: 0.089\n",
      "Step: 2986, Loss: 0.089\n",
      "Step: 2987, Loss: 0.088\n",
      "Step: 2988, Loss: 0.089\n",
      "Step: 2989, Loss: 0.089\n",
      "Step: 2990, Loss: 0.088\n",
      "Step: 2991, Loss: 0.089\n",
      "Step: 2992, Loss: 0.089\n",
      "Step: 2993, Loss: 0.089\n",
      "Step: 2994, Loss: 0.088\n",
      "Step: 2995, Loss: 0.089\n",
      "Step: 2996, Loss: 0.089\n",
      "Step: 2997, Loss: 0.088\n",
      "Step: 2998, Loss: 0.089\n",
      "Step: 2999, Loss: 0.089\n",
      "Step: 3000, Loss: 0.089\n",
      "Step: 3001, Loss: 0.088\n",
      "Step: 3002, Loss: 0.089\n",
      "Step: 3003, Loss: 0.089\n",
      "Step: 3004, Loss: 0.088\n",
      "Step: 3005, Loss: 0.088\n",
      "Step: 3006, Loss: 0.089\n",
      "Step: 3007, Loss: 0.089\n",
      "Step: 3008, Loss: 0.088\n",
      "Step: 3009, Loss: 0.089\n",
      "Step: 3010, Loss: 0.088\n",
      "Step: 3011, Loss: 0.088\n",
      "Step: 3012, Loss: 0.088\n",
      "Step: 3013, Loss: 0.089\n",
      "Step: 3014, Loss: 0.089\n",
      "Step: 3015, Loss: 0.088\n",
      "Step: 3016, Loss: 0.088\n",
      "Step: 3017, Loss: 0.089\n",
      "Step: 3018, Loss: 0.088\n",
      "Step: 3019, Loss: 0.088\n",
      "Step: 3020, Loss: 0.089\n",
      "Step: 3021, Loss: 0.089\n",
      "Step: 3022, Loss: 0.088\n",
      "Step: 3023, Loss: 0.088\n",
      "Step: 3024, Loss: 0.089\n",
      "Step: 3025, Loss: 0.088\n",
      "Step: 3026, Loss: 0.088\n",
      "Step: 3027, Loss: 0.088\n",
      "Step: 3028, Loss: 0.088\n",
      "Step: 3029, Loss: 0.088\n",
      "Step: 3030, Loss: 0.088\n",
      "Step: 3031, Loss: 0.089\n",
      "Step: 3032, Loss: 0.088\n",
      "Step: 3033, Loss: 0.088\n",
      "Step: 3034, Loss: 0.088\n",
      "Step: 3035, Loss: 0.089\n",
      "Step: 3036, Loss: 0.088\n",
      "Step: 3037, Loss: 0.088\n",
      "Step: 3038, Loss: 0.088\n",
      "Step: 3039, Loss: 0.088\n",
      "Step: 3040, Loss: 0.088\n",
      "Step: 3041, Loss: 0.087\n",
      "Step: 3042, Loss: 0.089\n",
      "Step: 3043, Loss: 0.088\n",
      "Step: 3044, Loss: 0.088\n",
      "Step: 3045, Loss: 0.088\n",
      "Step: 3046, Loss: 0.088\n",
      "Step: 3047, Loss: 0.088\n",
      "Step: 3048, Loss: 0.087\n",
      "Step: 3049, Loss: 0.088\n",
      "Step: 3050, Loss: 0.088\n",
      "Step: 3051, Loss: 0.088\n",
      "Step: 3052, Loss: 0.088\n",
      "Step: 3053, Loss: 0.088\n",
      "Step: 3054, Loss: 0.088\n",
      "Step: 3055, Loss: 0.088\n",
      "Step: 3056, Loss: 0.088\n",
      "Step: 3057, Loss: 0.088\n",
      "Step: 3058, Loss: 0.088\n",
      "Step: 3059, Loss: 0.087\n",
      "Step: 3060, Loss: 0.088\n",
      "Step: 3061, Loss: 0.088\n",
      "Step: 3062, Loss: 0.087\n",
      "Step: 3063, Loss: 0.088\n",
      "Step: 3064, Loss: 0.088\n",
      "Step: 3065, Loss: 0.088\n",
      "Step: 3066, Loss: 0.087\n",
      "Step: 3067, Loss: 0.088\n",
      "Step: 3068, Loss: 0.088\n",
      "Step: 3069, Loss: 0.088\n",
      "Step: 3070, Loss: 0.087\n",
      "Step: 3071, Loss: 0.088\n",
      "Step: 3072, Loss: 0.087\n",
      "Step: 3073, Loss: 0.087\n",
      "Step: 3074, Loss: 0.088\n",
      "Step: 3075, Loss: 0.088\n",
      "Step: 3076, Loss: 0.087\n",
      "Step: 3077, Loss: 0.087\n",
      "Step: 3078, Loss: 0.088\n",
      "Step: 3079, Loss: 0.088\n",
      "Step: 3080, Loss: 0.087\n",
      "Step: 3081, Loss: 0.087\n",
      "Step: 3082, Loss: 0.088\n",
      "Step: 3083, Loss: 0.087\n",
      "Step: 3084, Loss: 0.087\n",
      "Step: 3085, Loss: 0.088\n",
      "Step: 3086, Loss: 0.088\n",
      "Step: 3087, Loss: 0.088\n",
      "Step: 3088, Loss: 0.087\n",
      "Step: 3089, Loss: 0.088\n",
      "Step: 3090, Loss: 0.087\n",
      "Step: 3091, Loss: 0.087\n",
      "Step: 3092, Loss: 0.087\n",
      "Step: 3093, Loss: 0.087\n",
      "Step: 3094, Loss: 0.087\n",
      "Step: 3095, Loss: 0.087\n",
      "Step: 3096, Loss: 0.088\n",
      "Step: 3097, Loss: 0.087\n",
      "Step: 3098, Loss: 0.087\n",
      "Step: 3099, Loss: 0.087\n",
      "Step: 3100, Loss: 0.087\n",
      "Step: 3101, Loss: 0.087\n",
      "Step: 3102, Loss: 0.087\n",
      "Step: 3103, Loss: 0.087\n",
      "Step: 3104, Loss: 0.087\n",
      "Step: 3105, Loss: 0.087\n",
      "Step: 3106, Loss: 0.086\n",
      "Step: 3107, Loss: 0.088\n",
      "Step: 3108, Loss: 0.087\n",
      "Step: 3109, Loss: 0.087\n",
      "Step: 3110, Loss: 0.087\n",
      "Step: 3111, Loss: 0.087\n",
      "Step: 3112, Loss: 0.087\n",
      "Step: 3113, Loss: 0.086\n",
      "Step: 3114, Loss: 0.088\n",
      "Step: 3115, Loss: 0.087\n",
      "Step: 3116, Loss: 0.087\n",
      "Step: 3117, Loss: 0.087\n",
      "Step: 3118, Loss: 0.087\n",
      "Step: 3119, Loss: 0.087\n",
      "Step: 3120, Loss: 0.087\n",
      "Step: 3121, Loss: 0.087\n",
      "Step: 3122, Loss: 0.087\n",
      "Step: 3123, Loss: 0.087\n",
      "Step: 3124, Loss: 0.086\n",
      "Step: 3125, Loss: 0.087\n",
      "Step: 3126, Loss: 0.087\n",
      "Step: 3127, Loss: 0.087\n",
      "Step: 3128, Loss: 0.087\n",
      "Step: 3129, Loss: 0.087\n",
      "Step: 3130, Loss: 0.087\n",
      "Step: 3131, Loss: 0.086\n",
      "Step: 3132, Loss: 0.087\n",
      "Step: 3133, Loss: 0.087\n",
      "Step: 3134, Loss: 0.087\n",
      "Step: 3135, Loss: 0.086\n",
      "Step: 3136, Loss: 0.087\n",
      "Step: 3137, Loss: 0.087\n",
      "Step: 3138, Loss: 0.086\n",
      "Step: 3139, Loss: 0.087\n",
      "Step: 3140, Loss: 0.087\n",
      "Step: 3141, Loss: 0.087\n",
      "Step: 3142, Loss: 0.086\n",
      "Step: 3143, Loss: 0.087\n",
      "Step: 3144, Loss: 0.087\n",
      "Step: 3145, Loss: 0.086\n",
      "Step: 3146, Loss: 0.086\n",
      "Step: 3147, Loss: 0.087\n",
      "Step: 3148, Loss: 0.086\n",
      "Step: 3149, Loss: 0.086\n",
      "Step: 3150, Loss: 0.087\n",
      "Step: 3151, Loss: 0.087\n",
      "Step: 3152, Loss: 0.087\n",
      "Step: 3153, Loss: 0.086\n",
      "Step: 3154, Loss: 0.087\n",
      "Step: 3155, Loss: 0.086\n",
      "Step: 3156, Loss: 0.086\n",
      "Step: 3157, Loss: 0.087\n",
      "Step: 3158, Loss: 0.086\n",
      "Step: 3159, Loss: 0.086\n",
      "Step: 3160, Loss: 0.086\n",
      "Step: 3161, Loss: 0.087\n",
      "Step: 3162, Loss: 0.086\n",
      "Step: 3163, Loss: 0.086\n",
      "Step: 3164, Loss: 0.086\n",
      "Step: 3165, Loss: 0.087\n",
      "Step: 3166, Loss: 0.086\n",
      "Step: 3167, Loss: 0.086\n",
      "Step: 3168, Loss: 0.087\n",
      "Step: 3169, Loss: 0.086\n",
      "Step: 3170, Loss: 0.086\n",
      "Step: 3171, Loss: 0.086\n",
      "Step: 3172, Loss: 0.087\n",
      "Step: 3173, Loss: 0.086\n",
      "Step: 3174, Loss: 0.086\n",
      "Step: 3175, Loss: 0.086\n",
      "Step: 3176, Loss: 0.086\n",
      "Step: 3177, Loss: 0.086\n",
      "Step: 3178, Loss: 0.085\n",
      "Step: 3179, Loss: 0.087\n",
      "Step: 3180, Loss: 0.086\n",
      "Step: 3181, Loss: 0.086\n",
      "Step: 3182, Loss: 0.086\n",
      "Step: 3183, Loss: 0.086\n",
      "Step: 3184, Loss: 0.086\n",
      "Step: 3185, Loss: 0.086\n",
      "Step: 3186, Loss: 0.086\n",
      "Step: 3187, Loss: 0.086\n",
      "Step: 3188, Loss: 0.086\n",
      "Step: 3189, Loss: 0.085\n",
      "Step: 3190, Loss: 0.086\n",
      "Step: 3191, Loss: 0.086\n",
      "Step: 3192, Loss: 0.086\n",
      "Step: 3193, Loss: 0.086\n",
      "Step: 3194, Loss: 0.086\n",
      "Step: 3195, Loss: 0.086\n",
      "Step: 3196, Loss: 0.085\n",
      "Step: 3197, Loss: 0.086\n",
      "Step: 3198, Loss: 0.086\n",
      "Step: 3199, Loss: 0.086\n",
      "Step: 3200, Loss: 0.085\n",
      "Step: 3201, Loss: 0.086\n",
      "Step: 3202, Loss: 0.086\n",
      "Step: 3203, Loss: 0.086\n",
      "Step: 3204, Loss: 0.086\n",
      "Step: 3205, Loss: 0.086\n",
      "Step: 3206, Loss: 0.086\n",
      "Step: 3207, Loss: 0.085\n",
      "Step: 3208, Loss: 0.086\n",
      "Step: 3209, Loss: 0.086\n",
      "Step: 3210, Loss: 0.085\n",
      "Step: 3211, Loss: 0.085\n",
      "Step: 3212, Loss: 0.086\n",
      "Step: 3213, Loss: 0.086\n",
      "Step: 3214, Loss: 0.085\n",
      "Step: 3215, Loss: 0.086\n",
      "Step: 3216, Loss: 0.086\n",
      "Step: 3217, Loss: 0.086\n",
      "Step: 3218, Loss: 0.085\n",
      "Step: 3219, Loss: 0.086\n",
      "Step: 3220, Loss: 0.085\n",
      "Step: 3221, Loss: 0.085\n",
      "Step: 3222, Loss: 0.086\n",
      "Step: 3223, Loss: 0.085\n",
      "Step: 3224, Loss: 0.086\n",
      "Step: 3225, Loss: 0.085\n",
      "Step: 3226, Loss: 0.086\n",
      "Step: 3227, Loss: 0.086\n",
      "Step: 3228, Loss: 0.085\n",
      "Step: 3229, Loss: 0.085\n",
      "Step: 3230, Loss: 0.086\n",
      "Step: 3231, Loss: 0.085\n",
      "Step: 3232, Loss: 0.085\n",
      "Step: 3233, Loss: 0.086\n",
      "Step: 3234, Loss: 0.085\n",
      "Step: 3235, Loss: 0.085\n",
      "Step: 3236, Loss: 0.085\n",
      "Step: 3237, Loss: 0.086\n",
      "Step: 3238, Loss: 0.085\n",
      "Step: 3239, Loss: 0.085\n",
      "Step: 3240, Loss: 0.085\n",
      "Step: 3241, Loss: 0.085\n",
      "Step: 3242, Loss: 0.085\n",
      "Step: 3243, Loss: 0.085\n",
      "Step: 3244, Loss: 0.086\n",
      "Step: 3245, Loss: 0.085\n",
      "Step: 3246, Loss: 0.085\n",
      "Step: 3247, Loss: 0.085\n",
      "Step: 3248, Loss: 0.086\n",
      "Step: 3249, Loss: 0.085\n",
      "Step: 3250, Loss: 0.085\n",
      "Step: 3251, Loss: 0.085\n",
      "Step: 3252, Loss: 0.085\n",
      "Step: 3253, Loss: 0.085\n",
      "Step: 3254, Loss: 0.085\n",
      "Step: 3255, Loss: 0.086\n",
      "Step: 3256, Loss: 0.085\n",
      "Step: 3257, Loss: 0.085\n",
      "Step: 3258, Loss: 0.085\n",
      "Step: 3259, Loss: 0.085\n",
      "Step: 3260, Loss: 0.085\n",
      "Step: 3261, Loss: 0.084\n",
      "Step: 3262, Loss: 0.085\n",
      "Step: 3263, Loss: 0.085\n",
      "Step: 3264, Loss: 0.085\n",
      "Step: 3265, Loss: 0.084\n",
      "Step: 3266, Loss: 0.085\n",
      "Step: 3267, Loss: 0.085\n",
      "Step: 3268, Loss: 0.085\n",
      "Step: 3269, Loss: 0.085\n",
      "Step: 3270, Loss: 0.085\n",
      "Step: 3271, Loss: 0.085\n",
      "Step: 3272, Loss: 0.084\n",
      "Step: 3273, Loss: 0.085\n",
      "Step: 3274, Loss: 0.085\n",
      "Step: 3275, Loss: 0.085\n",
      "Step: 3276, Loss: 0.084\n",
      "Step: 3277, Loss: 0.085\n",
      "Step: 3278, Loss: 0.085\n",
      "Step: 3279, Loss: 0.085\n",
      "Step: 3280, Loss: 0.085\n",
      "Step: 3281, Loss: 0.085\n",
      "Step: 3282, Loss: 0.085\n",
      "Step: 3283, Loss: 0.084\n",
      "Step: 3284, Loss: 0.085\n",
      "Step: 3285, Loss: 0.085\n",
      "Step: 3286, Loss: 0.084\n",
      "Step: 3287, Loss: 0.085\n",
      "Step: 3288, Loss: 0.085\n",
      "Step: 3289, Loss: 0.085\n",
      "Step: 3290, Loss: 0.084\n",
      "Step: 3291, Loss: 0.085\n",
      "Step: 3292, Loss: 0.085\n",
      "Step: 3293, Loss: 0.085\n",
      "Step: 3294, Loss: 0.084\n",
      "Step: 3295, Loss: 0.085\n",
      "Step: 3296, Loss: 0.085\n",
      "Step: 3297, Loss: 0.084\n",
      "Step: 3298, Loss: 0.085\n",
      "Step: 3299, Loss: 0.085\n",
      "Step: 3300, Loss: 0.085\n",
      "Step: 3301, Loss: 0.084\n",
      "Step: 3302, Loss: 0.085\n",
      "Step: 3303, Loss: 0.085\n",
      "Step: 3304, Loss: 0.084\n",
      "Step: 3305, Loss: 0.084\n",
      "Step: 3306, Loss: 0.085\n",
      "Step: 3307, Loss: 0.084\n",
      "Step: 3308, Loss: 0.084\n",
      "Step: 3309, Loss: 0.085\n",
      "Step: 3310, Loss: 0.085\n",
      "Step: 3311, Loss: 0.084\n",
      "Step: 3312, Loss: 0.084\n",
      "Step: 3313, Loss: 0.085\n",
      "Step: 3314, Loss: 0.084\n",
      "Step: 3315, Loss: 0.084\n",
      "Step: 3316, Loss: 0.084\n",
      "Step: 3317, Loss: 0.084\n",
      "Step: 3318, Loss: 0.084\n",
      "Step: 3319, Loss: 0.084\n",
      "Step: 3320, Loss: 0.085\n",
      "Step: 3321, Loss: 0.084\n",
      "Step: 3322, Loss: 0.084\n",
      "Step: 3323, Loss: 0.084\n",
      "Step: 3324, Loss: 0.084\n",
      "Step: 3325, Loss: 0.084\n",
      "Step: 3326, Loss: 0.084\n",
      "Step: 3327, Loss: 0.084\n",
      "Step: 3328, Loss: 0.084\n",
      "Step: 3329, Loss: 0.084\n",
      "Step: 3330, Loss: 0.084\n",
      "Step: 3331, Loss: 0.085\n",
      "Step: 3332, Loss: 0.084\n",
      "Step: 3333, Loss: 0.084\n",
      "Step: 3334, Loss: 0.084\n",
      "Step: 3335, Loss: 0.084\n",
      "Step: 3336, Loss: 0.084\n",
      "Step: 3337, Loss: 0.083\n",
      "Step: 3338, Loss: 0.085\n",
      "Step: 3339, Loss: 0.084\n",
      "Step: 3340, Loss: 0.084\n",
      "Step: 3341, Loss: 0.084\n",
      "Step: 3342, Loss: 0.084\n",
      "Step: 3343, Loss: 0.084\n",
      "Step: 3344, Loss: 0.084\n",
      "Step: 3345, Loss: 0.084\n",
      "Step: 3346, Loss: 0.084\n",
      "Step: 3347, Loss: 0.084\n",
      "Step: 3348, Loss: 0.083\n",
      "Step: 3349, Loss: 0.084\n",
      "Step: 3350, Loss: 0.084\n",
      "Step: 3351, Loss: 0.084\n",
      "Step: 3352, Loss: 0.084\n",
      "Step: 3353, Loss: 0.084\n",
      "Step: 3354, Loss: 0.084\n",
      "Step: 3355, Loss: 0.083\n",
      "Step: 3356, Loss: 0.084\n",
      "Step: 3357, Loss: 0.084\n",
      "Step: 3358, Loss: 0.084\n",
      "Step: 3359, Loss: 0.083\n",
      "Step: 3360, Loss: 0.084\n",
      "Step: 3361, Loss: 0.084\n",
      "Step: 3362, Loss: 0.084\n",
      "Step: 3363, Loss: 0.084\n",
      "Step: 3364, Loss: 0.084\n",
      "Step: 3365, Loss: 0.084\n",
      "Step: 3366, Loss: 0.083\n",
      "Step: 3367, Loss: 0.084\n",
      "Step: 3368, Loss: 0.084\n",
      "Step: 3369, Loss: 0.084\n",
      "Step: 3370, Loss: 0.083\n",
      "Step: 3371, Loss: 0.084\n",
      "Step: 3372, Loss: 0.084\n",
      "Step: 3373, Loss: 0.083\n",
      "Step: 3374, Loss: 0.084\n",
      "Step: 3375, Loss: 0.084\n",
      "Step: 3376, Loss: 0.084\n",
      "Step: 3377, Loss: 0.083\n",
      "Step: 3378, Loss: 0.084\n",
      "Step: 3379, Loss: 0.084\n",
      "Step: 3380, Loss: 0.083\n",
      "Step: 3381, Loss: 0.084\n",
      "Step: 3382, Loss: 0.084\n",
      "Step: 3383, Loss: 0.083\n",
      "Step: 3384, Loss: 0.083\n",
      "Step: 3385, Loss: 0.084\n",
      "Step: 3386, Loss: 0.084\n",
      "Step: 3387, Loss: 0.083\n",
      "Step: 3388, Loss: 0.083\n",
      "Step: 3389, Loss: 0.084\n",
      "Step: 3390, Loss: 0.083\n",
      "Step: 3391, Loss: 0.083\n",
      "Step: 3392, Loss: 0.084\n",
      "Step: 3393, Loss: 0.083\n",
      "Step: 3394, Loss: 0.084\n",
      "Step: 3395, Loss: 0.083\n",
      "Step: 3396, Loss: 0.084\n",
      "Step: 3397, Loss: 0.083\n",
      "Step: 3398, Loss: 0.083\n",
      "Step: 3399, Loss: 0.083\n",
      "Step: 3400, Loss: 0.083\n",
      "Step: 3401, Loss: 0.083\n",
      "Step: 3402, Loss: 0.083\n",
      "Step: 3403, Loss: 0.084\n",
      "Step: 3404, Loss: 0.083\n",
      "Step: 3405, Loss: 0.083\n",
      "Step: 3406, Loss: 0.083\n",
      "Step: 3407, Loss: 0.084\n",
      "Step: 3408, Loss: 0.083\n",
      "Step: 3409, Loss: 0.083\n",
      "Step: 3410, Loss: 0.083\n",
      "Step: 3411, Loss: 0.083\n",
      "Step: 3412, Loss: 0.083\n",
      "Step: 3413, Loss: 0.083\n",
      "Step: 3414, Loss: 0.084\n",
      "Step: 3415, Loss: 0.083\n",
      "Step: 3416, Loss: 0.083\n",
      "Step: 3417, Loss: 0.083\n",
      "Step: 3418, Loss: 0.083\n",
      "Step: 3419, Loss: 0.083\n",
      "Step: 3420, Loss: 0.082\n",
      "Step: 3421, Loss: 0.083\n",
      "Step: 3422, Loss: 0.083\n",
      "Step: 3423, Loss: 0.083\n",
      "Step: 3424, Loss: 0.082\n",
      "Step: 3425, Loss: 0.083\n",
      "Step: 3426, Loss: 0.083\n",
      "Step: 3427, Loss: 0.083\n",
      "Step: 3428, Loss: 0.083\n",
      "Step: 3429, Loss: 0.083\n",
      "Step: 3430, Loss: 0.083\n",
      "Step: 3431, Loss: 0.082\n",
      "Step: 3432, Loss: 0.083\n",
      "Step: 3433, Loss: 0.083\n",
      "Step: 3434, Loss: 0.083\n",
      "Step: 3435, Loss: 0.083\n",
      "Step: 3436, Loss: 0.083\n",
      "Step: 3437, Loss: 0.083\n",
      "Step: 3438, Loss: 0.082\n",
      "Step: 3439, Loss: 0.083\n",
      "Step: 3440, Loss: 0.083\n",
      "Step: 3441, Loss: 0.083\n",
      "Step: 3442, Loss: 0.082\n",
      "Step: 3443, Loss: 0.083\n",
      "Step: 3444, Loss: 0.083\n",
      "Step: 3445, Loss: 0.083\n",
      "Step: 3446, Loss: 0.083\n",
      "Step: 3447, Loss: 0.083\n",
      "Step: 3448, Loss: 0.083\n",
      "Step: 3449, Loss: 0.082\n",
      "Step: 3450, Loss: 0.083\n",
      "Step: 3451, Loss: 0.083\n",
      "Step: 3452, Loss: 0.083\n",
      "Step: 3453, Loss: 0.082\n",
      "Step: 3454, Loss: 0.083\n",
      "Step: 3455, Loss: 0.083\n",
      "Step: 3456, Loss: 0.082\n",
      "Step: 3457, Loss: 0.083\n",
      "Step: 3458, Loss: 0.083\n",
      "Step: 3459, Loss: 0.083\n",
      "Step: 3460, Loss: 0.082\n",
      "Step: 3461, Loss: 0.083\n",
      "Step: 3462, Loss: 0.082\n",
      "Step: 3463, Loss: 0.082\n",
      "Step: 3464, Loss: 0.082\n",
      "Step: 3465, Loss: 0.083\n",
      "Step: 3466, Loss: 0.082\n",
      "Step: 3467, Loss: 0.082\n",
      "Step: 3468, Loss: 0.083\n",
      "Step: 3469, Loss: 0.083\n",
      "Step: 3470, Loss: 0.082\n",
      "Step: 3471, Loss: 0.082\n",
      "Step: 3472, Loss: 0.083\n",
      "Step: 3473, Loss: 0.082\n",
      "Step: 3474, Loss: 0.082\n",
      "Step: 3475, Loss: 0.083\n",
      "Step: 3476, Loss: 0.082\n",
      "Step: 3477, Loss: 0.082\n",
      "Step: 3478, Loss: 0.082\n",
      "Step: 3479, Loss: 0.083\n",
      "Step: 3480, Loss: 0.082\n",
      "Step: 3481, Loss: 0.082\n",
      "Step: 3482, Loss: 0.082\n",
      "Step: 3483, Loss: 0.083\n",
      "Step: 3484, Loss: 0.082\n",
      "Step: 3485, Loss: 0.082\n",
      "Step: 3486, Loss: 0.083\n",
      "Step: 3487, Loss: 0.082\n",
      "Step: 3488, Loss: 0.082\n",
      "Step: 3489, Loss: 0.082\n",
      "Step: 3490, Loss: 0.083\n",
      "Step: 3491, Loss: 0.082\n",
      "Step: 3492, Loss: 0.082\n",
      "Step: 3493, Loss: 0.082\n",
      "Step: 3494, Loss: 0.082\n",
      "Step: 3495, Loss: 0.082\n",
      "Step: 3496, Loss: 0.081\n",
      "Step: 3497, Loss: 0.083\n",
      "Step: 3498, Loss: 0.082\n",
      "Step: 3499, Loss: 0.082\n",
      "Step: 3500, Loss: 0.082\n",
      "Step: 3501, Loss: 0.082\n",
      "Step: 3502, Loss: 0.082\n",
      "Step: 3503, Loss: 0.082\n",
      "Step: 3504, Loss: 0.082\n",
      "Step: 3505, Loss: 0.082\n",
      "Step: 3506, Loss: 0.082\n",
      "Step: 3507, Loss: 0.081\n",
      "Step: 3508, Loss: 0.083\n",
      "Step: 3509, Loss: 0.082\n",
      "Step: 3510, Loss: 0.082\n",
      "Step: 3511, Loss: 0.082\n",
      "Step: 3512, Loss: 0.082\n",
      "Step: 3513, Loss: 0.082\n",
      "Step: 3514, Loss: 0.081\n",
      "Step: 3515, Loss: 0.082\n",
      "Step: 3516, Loss: 0.082\n",
      "Step: 3517, Loss: 0.082\n",
      "Step: 3518, Loss: 0.081\n",
      "Step: 3519, Loss: 0.082\n",
      "Step: 3520, Loss: 0.082\n",
      "Step: 3521, Loss: 0.082\n",
      "Step: 3522, Loss: 0.082\n",
      "Step: 3523, Loss: 0.082\n",
      "Step: 3524, Loss: 0.082\n",
      "Step: 3525, Loss: 0.081\n",
      "Step: 3526, Loss: 0.082\n",
      "Step: 3527, Loss: 0.082\n",
      "Step: 3528, Loss: 0.081\n",
      "Step: 3529, Loss: 0.082\n",
      "Step: 3530, Loss: 0.082\n",
      "Step: 3531, Loss: 0.082\n",
      "Step: 3532, Loss: 0.081\n",
      "Step: 3533, Loss: 0.082\n",
      "Step: 3534, Loss: 0.082\n",
      "Step: 3535, Loss: 0.082\n",
      "Step: 3536, Loss: 0.081\n",
      "Step: 3537, Loss: 0.082\n",
      "Step: 3538, Loss: 0.081\n",
      "Step: 3539, Loss: 0.081\n",
      "Step: 3540, Loss: 0.082\n",
      "Step: 3541, Loss: 0.082\n",
      "Step: 3542, Loss: 0.082\n",
      "Step: 3543, Loss: 0.081\n",
      "Step: 3544, Loss: 0.082\n",
      "Step: 3545, Loss: 0.082\n",
      "Step: 3546, Loss: 0.081\n",
      "Step: 3547, Loss: 0.081\n",
      "Step: 3548, Loss: 0.082\n",
      "Step: 3549, Loss: 0.081\n",
      "Step: 3550, Loss: 0.081\n",
      "Step: 3551, Loss: 0.082\n",
      "Step: 3552, Loss: 0.082\n",
      "Step: 3553, Loss: 0.082\n",
      "Step: 3554, Loss: 0.081\n",
      "Step: 3555, Loss: 0.082\n",
      "Step: 3556, Loss: 0.081\n",
      "Step: 3557, Loss: 0.081\n",
      "Step: 3558, Loss: 0.082\n",
      "Step: 3559, Loss: 0.081\n",
      "Step: 3560, Loss: 0.081\n",
      "Step: 3561, Loss: 0.081\n",
      "Step: 3562, Loss: 0.082\n",
      "Step: 3563, Loss: 0.081\n",
      "Step: 3564, Loss: 0.081\n",
      "Step: 3565, Loss: 0.081\n",
      "Step: 3566, Loss: 0.082\n",
      "Step: 3567, Loss: 0.081\n",
      "Step: 3568, Loss: 0.081\n",
      "Step: 3569, Loss: 0.082\n",
      "Step: 3570, Loss: 0.081\n",
      "Step: 3571, Loss: 0.081\n",
      "Step: 3572, Loss: 0.081\n",
      "Step: 3573, Loss: 0.082\n",
      "Step: 3574, Loss: 0.081\n",
      "Step: 3575, Loss: 0.081\n",
      "Step: 3576, Loss: 0.081\n",
      "Step: 3577, Loss: 0.081\n",
      "Step: 3578, Loss: 0.081\n",
      "Step: 3579, Loss: 0.080\n",
      "Step: 3580, Loss: 0.082\n",
      "Step: 3581, Loss: 0.081\n",
      "Step: 3582, Loss: 0.081\n",
      "Step: 3583, Loss: 0.080\n",
      "Step: 3584, Loss: 0.081\n",
      "Step: 3585, Loss: 0.081\n",
      "Step: 3586, Loss: 0.081\n",
      "Step: 3587, Loss: 0.081\n",
      "Step: 3588, Loss: 0.081\n",
      "Step: 3589, Loss: 0.081\n",
      "Step: 3590, Loss: 0.080\n",
      "Step: 3591, Loss: 0.081\n",
      "Step: 3592, Loss: 0.081\n",
      "Step: 3593, Loss: 0.081\n",
      "Step: 3594, Loss: 0.081\n",
      "Step: 3595, Loss: 0.081\n",
      "Step: 3596, Loss: 0.081\n",
      "Step: 3597, Loss: 0.080\n",
      "Step: 3598, Loss: 0.081\n",
      "Step: 3599, Loss: 0.081\n",
      "Step: 3600, Loss: 0.081\n",
      "Step: 3601, Loss: 0.080\n",
      "Step: 3602, Loss: 0.081\n",
      "Step: 3603, Loss: 0.081\n",
      "Step: 3604, Loss: 0.081\n",
      "Step: 3605, Loss: 0.081\n",
      "Step: 3606, Loss: 0.081\n",
      "Step: 3607, Loss: 0.081\n",
      "Step: 3608, Loss: 0.080\n",
      "Step: 3609, Loss: 0.081\n",
      "Step: 3610, Loss: 0.081\n",
      "Step: 3611, Loss: 0.081\n",
      "Step: 3612, Loss: 0.080\n",
      "Step: 3613, Loss: 0.081\n",
      "Step: 3614, Loss: 0.081\n",
      "Step: 3615, Loss: 0.080\n",
      "Step: 3616, Loss: 0.081\n",
      "Step: 3617, Loss: 0.081\n",
      "Step: 3618, Loss: 0.081\n",
      "Step: 3619, Loss: 0.080\n",
      "Step: 3620, Loss: 0.081\n",
      "Step: 3621, Loss: 0.081\n",
      "Step: 3622, Loss: 0.080\n",
      "Step: 3623, Loss: 0.081\n",
      "Step: 3624, Loss: 0.080\n",
      "Step: 3625, Loss: 0.081\n",
      "Step: 3626, Loss: 0.080\n",
      "Step: 3627, Loss: 0.081\n",
      "Step: 3628, Loss: 0.081\n",
      "Step: 3629, Loss: 0.080\n",
      "Step: 3630, Loss: 0.080\n",
      "Step: 3631, Loss: 0.081\n",
      "Step: 3632, Loss: 0.080\n",
      "Step: 3633, Loss: 0.080\n",
      "Step: 3634, Loss: 0.081\n",
      "Step: 3635, Loss: 0.080\n",
      "Step: 3636, Loss: 0.081\n",
      "Step: 3637, Loss: 0.080\n",
      "Step: 3638, Loss: 0.081\n",
      "Step: 3639, Loss: 0.081\n",
      "Step: 3640, Loss: 0.080\n",
      "Step: 3641, Loss: 0.080\n",
      "Step: 3642, Loss: 0.081\n",
      "Step: 3643, Loss: 0.080\n",
      "Step: 3644, Loss: 0.080\n",
      "Step: 3645, Loss: 0.081\n",
      "Step: 3646, Loss: 0.080\n",
      "Step: 3647, Loss: 0.080\n",
      "Step: 3648, Loss: 0.080\n",
      "Step: 3649, Loss: 0.081\n",
      "Step: 3650, Loss: 0.080\n",
      "Step: 3651, Loss: 0.080\n",
      "Step: 3652, Loss: 0.080\n",
      "Step: 3653, Loss: 0.080\n",
      "Step: 3654, Loss: 0.080\n",
      "Step: 3655, Loss: 0.080\n",
      "Step: 3656, Loss: 0.081\n",
      "Step: 3657, Loss: 0.080\n",
      "Step: 3658, Loss: 0.080\n",
      "Step: 3659, Loss: 0.080\n",
      "Step: 3660, Loss: 0.081\n",
      "Step: 3661, Loss: 0.080\n",
      "Step: 3662, Loss: 0.080\n",
      "Step: 3663, Loss: 0.080\n",
      "Step: 3664, Loss: 0.080\n",
      "Step: 3665, Loss: 0.080\n",
      "Step: 3666, Loss: 0.079\n",
      "Step: 3667, Loss: 0.080\n",
      "Step: 3668, Loss: 0.080\n",
      "Step: 3669, Loss: 0.080\n",
      "Step: 3670, Loss: 0.080\n",
      "Step: 3671, Loss: 0.080\n",
      "Step: 3672, Loss: 0.080\n",
      "Step: 3673, Loss: 0.079\n",
      "Step: 3674, Loss: 0.080\n",
      "Step: 3675, Loss: 0.080\n",
      "Step: 3676, Loss: 0.080\n",
      "Step: 3677, Loss: 0.080\n",
      "Step: 3678, Loss: 0.080\n",
      "Step: 3679, Loss: 0.080\n",
      "Step: 3680, Loss: 0.080\n",
      "Step: 3681, Loss: 0.080\n",
      "Step: 3682, Loss: 0.080\n",
      "Step: 3683, Loss: 0.080\n",
      "Step: 3684, Loss: 0.079\n",
      "Step: 3685, Loss: 0.080\n",
      "Step: 3686, Loss: 0.080\n",
      "Step: 3687, Loss: 0.080\n",
      "Step: 3688, Loss: 0.080\n",
      "Step: 3689, Loss: 0.080\n",
      "Step: 3690, Loss: 0.080\n",
      "Step: 3691, Loss: 0.079\n",
      "Step: 3692, Loss: 0.080\n",
      "Step: 3693, Loss: 0.080\n",
      "Step: 3694, Loss: 0.080\n",
      "Step: 3695, Loss: 0.079\n",
      "Step: 3696, Loss: 0.080\n",
      "Step: 3697, Loss: 0.080\n",
      "Step: 3698, Loss: 0.080\n",
      "Step: 3699, Loss: 0.080\n",
      "Step: 3700, Loss: 0.080\n",
      "Step: 3701, Loss: 0.080\n",
      "Step: 3702, Loss: 0.079\n",
      "Step: 3703, Loss: 0.080\n",
      "Step: 3704, Loss: 0.080\n",
      "Step: 3705, Loss: 0.079\n",
      "Step: 3706, Loss: 0.079\n",
      "Step: 3707, Loss: 0.080\n",
      "Step: 3708, Loss: 0.080\n",
      "Step: 3709, Loss: 0.079\n",
      "Step: 3710, Loss: 0.080\n",
      "Step: 3711, Loss: 0.080\n",
      "Step: 3712, Loss: 0.080\n",
      "Step: 3713, Loss: 0.079\n",
      "Step: 3714, Loss: 0.080\n",
      "Step: 3715, Loss: 0.079\n",
      "Step: 3716, Loss: 0.079\n",
      "Step: 3717, Loss: 0.080\n",
      "Step: 3718, Loss: 0.079\n",
      "Step: 3719, Loss: 0.079\n",
      "Step: 3720, Loss: 0.079\n",
      "Step: 3721, Loss: 0.080\n",
      "Step: 3722, Loss: 0.080\n",
      "Step: 3723, Loss: 0.079\n",
      "Step: 3724, Loss: 0.079\n",
      "Step: 3725, Loss: 0.080\n",
      "Step: 3726, Loss: 0.079\n",
      "Step: 3727, Loss: 0.079\n",
      "Step: 3728, Loss: 0.080\n",
      "Step: 3729, Loss: 0.079\n",
      "Step: 3730, Loss: 0.079\n",
      "Step: 3731, Loss: 0.079\n",
      "Step: 3732, Loss: 0.080\n",
      "Step: 3733, Loss: 0.079\n",
      "Step: 3734, Loss: 0.079\n",
      "Step: 3735, Loss: 0.079\n",
      "Step: 3736, Loss: 0.079\n",
      "Step: 3737, Loss: 0.079\n",
      "Step: 3738, Loss: 0.079\n",
      "Step: 3739, Loss: 0.080\n",
      "Step: 3740, Loss: 0.079\n",
      "Step: 3741, Loss: 0.079\n",
      "Step: 3742, Loss: 0.079\n",
      "Step: 3743, Loss: 0.079\n",
      "Step: 3744, Loss: 0.079\n",
      "Step: 3745, Loss: 0.079\n",
      "Step: 3746, Loss: 0.079\n",
      "Step: 3747, Loss: 0.079\n",
      "Step: 3748, Loss: 0.079\n",
      "Step: 3749, Loss: 0.078\n",
      "Step: 3750, Loss: 0.079\n",
      "Step: 3751, Loss: 0.079\n",
      "Step: 3752, Loss: 0.079\n",
      "Step: 3753, Loss: 0.079\n",
      "Step: 3754, Loss: 0.079\n",
      "Step: 3755, Loss: 0.079\n",
      "Step: 3756, Loss: 0.079\n",
      "Step: 3757, Loss: 0.079\n",
      "Step: 3758, Loss: 0.079\n",
      "Step: 3759, Loss: 0.079\n",
      "Step: 3760, Loss: 0.078\n",
      "Step: 3761, Loss: 0.079\n",
      "Step: 3762, Loss: 0.079\n",
      "Step: 3763, Loss: 0.079\n",
      "Step: 3764, Loss: 0.079\n",
      "Step: 3765, Loss: 0.079\n",
      "Step: 3766, Loss: 0.079\n",
      "Step: 3767, Loss: 0.078\n",
      "Step: 3768, Loss: 0.079\n",
      "Step: 3769, Loss: 0.079\n",
      "Step: 3770, Loss: 0.079\n",
      "Step: 3771, Loss: 0.079\n",
      "Step: 3772, Loss: 0.079\n",
      "Step: 3773, Loss: 0.079\n",
      "Step: 3774, Loss: 0.078\n",
      "Step: 3775, Loss: 0.079\n",
      "Step: 3776, Loss: 0.079\n",
      "Step: 3777, Loss: 0.079\n",
      "Step: 3778, Loss: 0.078\n",
      "Step: 3779, Loss: 0.079\n",
      "Step: 3780, Loss: 0.079\n",
      "Step: 3781, Loss: 0.079\n",
      "Step: 3782, Loss: 0.079\n",
      "Step: 3783, Loss: 0.079\n",
      "Step: 3784, Loss: 0.079\n",
      "Step: 3785, Loss: 0.078\n",
      "Step: 3786, Loss: 0.079\n",
      "Step: 3787, Loss: 0.079\n",
      "Step: 3788, Loss: 0.079\n",
      "Step: 3789, Loss: 0.078\n",
      "Step: 3790, Loss: 0.079\n",
      "Step: 3791, Loss: 0.079\n",
      "Step: 3792, Loss: 0.078\n",
      "Step: 3793, Loss: 0.079\n",
      "Step: 3794, Loss: 0.079\n",
      "Step: 3795, Loss: 0.079\n",
      "Step: 3796, Loss: 0.078\n",
      "Step: 3797, Loss: 0.079\n",
      "Step: 3798, Loss: 0.079\n",
      "Step: 3799, Loss: 0.078\n",
      "Step: 3800, Loss: 0.079\n",
      "Step: 3801, Loss: 0.078\n",
      "Step: 3802, Loss: 0.079\n",
      "Step: 3803, Loss: 0.078\n",
      "Step: 3804, Loss: 0.079\n",
      "Step: 3805, Loss: 0.078\n",
      "Step: 3806, Loss: 0.078\n",
      "Step: 3807, Loss: 0.078\n",
      "Step: 3808, Loss: 0.079\n",
      "Step: 3809, Loss: 0.078\n",
      "Step: 3810, Loss: 0.078\n",
      "Step: 3811, Loss: 0.078\n",
      "Step: 3812, Loss: 0.078\n",
      "Step: 3813, Loss: 0.079\n",
      "Step: 3814, Loss: 0.078\n",
      "Step: 3815, Loss: 0.079\n",
      "Step: 3816, Loss: 0.079\n",
      "Step: 3817, Loss: 0.078\n",
      "Step: 3818, Loss: 0.078\n",
      "Step: 3819, Loss: 0.079\n",
      "Step: 3820, Loss: 0.078\n",
      "Step: 3821, Loss: 0.078\n",
      "Step: 3822, Loss: 0.079\n",
      "Step: 3823, Loss: 0.078\n",
      "Step: 3824, Loss: 0.078\n",
      "Step: 3825, Loss: 0.078\n",
      "Step: 3826, Loss: 0.079\n",
      "Step: 3827, Loss: 0.078\n",
      "Step: 3828, Loss: 0.078\n",
      "Step: 3829, Loss: 0.078\n",
      "Step: 3830, Loss: 0.078\n",
      "Step: 3831, Loss: 0.078\n",
      "Step: 3832, Loss: 0.078\n",
      "Step: 3833, Loss: 0.079\n",
      "Step: 3834, Loss: 0.078\n",
      "Step: 3835, Loss: 0.078\n",
      "Step: 3836, Loss: 0.078\n",
      "Step: 3837, Loss: 0.078\n",
      "Step: 3838, Loss: 0.078\n",
      "Step: 3839, Loss: 0.078\n",
      "Step: 3840, Loss: 0.078\n",
      "Step: 3841, Loss: 0.078\n",
      "Step: 3842, Loss: 0.078\n",
      "Step: 3843, Loss: 0.077\n",
      "Step: 3844, Loss: 0.078\n",
      "Step: 3845, Loss: 0.078\n",
      "Step: 3846, Loss: 0.078\n",
      "Step: 3847, Loss: 0.078\n",
      "Step: 3848, Loss: 0.078\n",
      "Step: 3849, Loss: 0.078\n",
      "Step: 3850, Loss: 0.077\n",
      "Step: 3851, Loss: 0.078\n",
      "Step: 3852, Loss: 0.078\n",
      "Step: 3853, Loss: 0.078\n",
      "Step: 3854, Loss: 0.078\n",
      "Step: 3855, Loss: 0.078\n",
      "Step: 3856, Loss: 0.078\n",
      "Step: 3857, Loss: 0.078\n",
      "Step: 3858, Loss: 0.078\n",
      "Step: 3859, Loss: 0.078\n",
      "Step: 3860, Loss: 0.078\n",
      "Step: 3861, Loss: 0.077\n",
      "Step: 3862, Loss: 0.078\n",
      "Step: 3863, Loss: 0.078\n",
      "Step: 3864, Loss: 0.078\n",
      "Step: 3865, Loss: 0.078\n",
      "Step: 3866, Loss: 0.078\n",
      "Step: 3867, Loss: 0.078\n",
      "Step: 3868, Loss: 0.077\n",
      "Step: 3869, Loss: 0.078\n",
      "Step: 3870, Loss: 0.078\n",
      "Step: 3871, Loss: 0.078\n",
      "Step: 3872, Loss: 0.077\n",
      "Step: 3873, Loss: 0.078\n",
      "Step: 3874, Loss: 0.078\n",
      "Step: 3875, Loss: 0.078\n",
      "Step: 3876, Loss: 0.078\n",
      "Step: 3877, Loss: 0.078\n",
      "Step: 3878, Loss: 0.078\n",
      "Step: 3879, Loss: 0.077\n",
      "Step: 3880, Loss: 0.078\n",
      "Step: 3881, Loss: 0.078\n",
      "Step: 3882, Loss: 0.077\n",
      "Step: 3883, Loss: 0.077\n",
      "Step: 3884, Loss: 0.078\n",
      "Step: 3885, Loss: 0.078\n",
      "Step: 3886, Loss: 0.077\n",
      "Step: 3887, Loss: 0.078\n",
      "Step: 3888, Loss: 0.078\n",
      "Step: 3889, Loss: 0.078\n",
      "Step: 3890, Loss: 0.077\n",
      "Step: 3891, Loss: 0.078\n",
      "Step: 3892, Loss: 0.077\n",
      "Step: 3893, Loss: 0.077\n",
      "Step: 3894, Loss: 0.078\n",
      "Step: 3895, Loss: 0.077\n",
      "Step: 3896, Loss: 0.078\n",
      "Step: 3897, Loss: 0.077\n",
      "Step: 3898, Loss: 0.078\n",
      "Step: 3899, Loss: 0.077\n",
      "Step: 3900, Loss: 0.077\n",
      "Step: 3901, Loss: 0.077\n",
      "Step: 3902, Loss: 0.078\n",
      "Step: 3903, Loss: 0.077\n",
      "Step: 3904, Loss: 0.077\n",
      "Step: 3905, Loss: 0.078\n",
      "Step: 3906, Loss: 0.077\n",
      "Step: 3907, Loss: 0.078\n",
      "Step: 3908, Loss: 0.077\n",
      "Step: 3909, Loss: 0.078\n",
      "Step: 3910, Loss: 0.077\n",
      "Step: 3911, Loss: 0.077\n",
      "Step: 3912, Loss: 0.077\n",
      "Step: 3913, Loss: 0.077\n",
      "Step: 3914, Loss: 0.077\n",
      "Step: 3915, Loss: 0.077\n",
      "Step: 3916, Loss: 0.078\n",
      "Step: 3917, Loss: 0.077\n",
      "Step: 3918, Loss: 0.077\n",
      "Step: 3919, Loss: 0.077\n",
      "Step: 3920, Loss: 0.077\n",
      "Step: 3921, Loss: 0.077\n",
      "Step: 3922, Loss: 0.077\n",
      "Step: 3923, Loss: 0.077\n",
      "Step: 3924, Loss: 0.077\n",
      "Step: 3925, Loss: 0.077\n",
      "Step: 3926, Loss: 0.077\n",
      "Step: 3927, Loss: 0.077\n",
      "Step: 3928, Loss: 0.077\n",
      "Step: 3929, Loss: 0.077\n",
      "Step: 3930, Loss: 0.077\n",
      "Step: 3931, Loss: 0.077\n",
      "Step: 3932, Loss: 0.077\n",
      "Step: 3933, Loss: 0.076\n",
      "Step: 3934, Loss: 0.077\n",
      "Step: 3935, Loss: 0.077\n",
      "Step: 3936, Loss: 0.077\n",
      "Step: 3937, Loss: 0.076\n",
      "Step: 3938, Loss: 0.077\n",
      "Step: 3939, Loss: 0.077\n",
      "Step: 3940, Loss: 0.077\n",
      "Step: 3941, Loss: 0.077\n",
      "Step: 3942, Loss: 0.077\n",
      "Step: 3943, Loss: 0.077\n",
      "Step: 3944, Loss: 0.076\n",
      "Step: 3945, Loss: 0.077\n",
      "Step: 3946, Loss: 0.077\n",
      "Step: 3947, Loss: 0.077\n",
      "Step: 3948, Loss: 0.077\n",
      "Step: 3949, Loss: 0.077\n",
      "Step: 3950, Loss: 0.077\n",
      "Step: 3951, Loss: 0.076\n",
      "Step: 3952, Loss: 0.077\n",
      "Step: 3953, Loss: 0.077\n",
      "Step: 3954, Loss: 0.077\n",
      "Step: 3955, Loss: 0.076\n",
      "Step: 3956, Loss: 0.077\n",
      "Step: 3957, Loss: 0.077\n",
      "Step: 3958, Loss: 0.077\n",
      "Step: 3959, Loss: 0.077\n",
      "Step: 3960, Loss: 0.077\n",
      "Step: 3961, Loss: 0.077\n",
      "Step: 3962, Loss: 0.076\n",
      "Step: 3963, Loss: 0.077\n",
      "Step: 3964, Loss: 0.077\n",
      "Step: 3965, Loss: 0.077\n",
      "Step: 3966, Loss: 0.076\n",
      "Step: 3967, Loss: 0.077\n",
      "Step: 3968, Loss: 0.077\n",
      "Step: 3969, Loss: 0.076\n",
      "Step: 3970, Loss: 0.077\n",
      "Step: 3971, Loss: 0.077\n",
      "Step: 3972, Loss: 0.077\n",
      "Step: 3973, Loss: 0.076\n",
      "Step: 3974, Loss: 0.077\n",
      "Step: 3975, Loss: 0.077\n",
      "Step: 3976, Loss: 0.076\n",
      "Step: 3977, Loss: 0.077\n",
      "Step: 3978, Loss: 0.077\n",
      "Step: 3979, Loss: 0.077\n",
      "Step: 3980, Loss: 0.076\n",
      "Step: 3981, Loss: 0.077\n",
      "Step: 3982, Loss: 0.077\n",
      "Step: 3983, Loss: 0.077\n",
      "Step: 3984, Loss: 0.076\n",
      "Step: 3985, Loss: 0.077\n",
      "Step: 3986, Loss: 0.076\n",
      "Step: 3987, Loss: 0.076\n",
      "Step: 3988, Loss: 0.077\n",
      "Step: 3989, Loss: 0.076\n",
      "Step: 3990, Loss: 0.077\n",
      "Step: 3991, Loss: 0.076\n",
      "Step: 3992, Loss: 0.077\n",
      "Step: 3993, Loss: 0.077\n",
      "Step: 3994, Loss: 0.076\n",
      "Step: 3995, Loss: 0.076\n",
      "Step: 3996, Loss: 0.077\n",
      "Step: 3997, Loss: 0.076\n",
      "Step: 3998, Loss: 0.076\n",
      "Step: 3999, Loss: 0.077\n",
      "Step: 4000, Loss: 0.077\n"
     ]
    }
   ],
   "source": [
    "set_lr(mm_optimizer, 1e-5)\n",
    "METADRIVE_MODEL_TRAIN_EPOCHS = 4000\n",
    "METADRIVE_MODEL_TRAIN_BATCH_SIZE = 2048\n",
    "\n",
    "while mm_step < METADRIVE_MODEL_TRAIN_EPOCHS:\n",
    "    # take up to n from the data buffer\n",
    "    data_batch = [next(mm_train_iter) for _ in range(METADRIVE_MODEL_TRAIN_BATCH_SIZE)]\n",
    "    # unpack the batch\n",
    "    s0_batch = [s0 for s0, _, _ in data_batch]\n",
    "    a_batch = [a for _, a, _ in data_batch]\n",
    "    s1_batch = [s1 for _, _, s1 in data_batch]\n",
    "    loss = metadrive_model_train_batch(mm, mm_optimizer, s0_batch, a_batch, s1_batch)\n",
    "    mm_losses.append(loss)\n",
    "    mm_step += 1\n",
    "    print(f\"Step: {mm_step}, Loss: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the losses over training\n",
    "plt.plot(list(range(len(mm_losses))), mm_losses, label='MM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 0.09680616110563278\n"
     ]
    }
   ],
   "source": [
    "s0_batch = [s0 for s0, _, _ in mm_validation_data]\n",
    "s1_batch = [s1 for _, _, s1 in mm_validation_data]\n",
    "a_batch = [a for _, a, _ in mm_validation_data]\n",
    "\n",
    "s0_tensor = state_batch_to_tensor(s0_batch, device)\n",
    "s1_tensor = state_batch_to_tensor(s1_batch, device)\n",
    "a_tensor = action_batch_to_tensor(a_batch, device)\n",
    "\n",
    "s1_pred = mm(s0_tensor, a_tensor)\n",
    "\n",
    "loss = (s1_pred - s1_tensor)**2\n",
    "loss_x = loss[:, 0]\n",
    "loss_y = loss[:, 1]\n",
    "loss_theta = loss[:, 2]\n",
    "\n",
    "print(\"validation loss\", loss.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.07631634920835495\n"
     ]
    }
   ],
   "source": [
    "s0_batch = [s0 for s0, _, _ in mm_train_data]\n",
    "s1_batch = [s1 for _, _, s1 in mm_train_data]\n",
    "a_batch = [a for _, a, _ in mm_train_data]\n",
    "\n",
    "s0_tensor = state_batch_to_tensor(s0_batch, device)\n",
    "s1_tensor = state_batch_to_tensor(s1_batch, device)\n",
    "a_tensor = action_batch_to_tensor(a_batch, device)\n",
    "\n",
    "s1_pred = mm(s0_tensor, a_tensor)\n",
    "\n",
    "loss = (s1_pred - s1_tensor)**2\n",
    "loss_x = loss[:, 0]\n",
    "loss_y = loss[:, 1]\n",
    "loss_theta = loss[:, 2]\n",
    "\n",
    "print(\"training loss\", loss.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(loss_x.cpu().detach().numpy(), bins=100)\n",
    "plt.title('Loss in x')\n",
    "plt.show()\n",
    "plt.hist(loss_y.cpu().detach().numpy(), bins=100)\n",
    "plt.title('Loss in y')\n",
    "plt.show()\n",
    "plt.hist(loss_theta.cpu().detach().numpy(), bins=100)\n",
    "plt.title('Loss in theta')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a neural network model of Metadrive through which we can backprop, training the IDM is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 00:16:17.130599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 00:16:18.331230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]2023-08-11 00:16:19.189922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.209057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.209432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.211627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.211867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.212042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.553873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.554077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.554245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-11 00:16:19.554375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21078 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1d:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from protos import scenario_pb2\n",
    "from tensorflow.data import TFRecordDataset\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def getFiles(path: str) -> list[str]:\n",
    "    path = os.path.expanduser(path)\n",
    "    files = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    return [f for f in files if os.path.isfile(f)]\n",
    "\n",
    "\n",
    "files = getFiles('~/data/waymo/')\n",
    "\n",
    "def parse_scenario(scenario: scenario_pb2.Scenario) -> list[State]:\n",
    "    states = []\n",
    "    for s in scenario.tracks[scenario.sdc_track_index].states:\n",
    "        if s.valid:\n",
    "            states.append(State(s.heading, np.array([s.velocity_x, s.velocity_y], dtype=np.float32)))\n",
    "    return states\n",
    "\n",
    "\n",
    "h: list[list[State]] = []\n",
    "\n",
    "for file_path in tqdm.tqdm(files):\n",
    "    for data in TFRecordDataset(file_path, compression_type=\"\").as_numpy_iterator():\n",
    "        scenario = scenario_pb2.Scenario()\n",
    "        scenario.ParseFromString(data)\n",
    "        h.append(parse_scenario(scenario))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 42268\n"
     ]
    }
   ],
   "source": [
    "idm_train_data: list[Observation] = []\n",
    "for states in h:\n",
    "    for i in range(len(states)-1):\n",
    "        idm_train_data.append((states[i], states[i+1]))\n",
    "\n",
    "print(\"train data:\", len(idm_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(idm_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a model that attempts to predict the action given the current state and the next state\n",
    "class InverseDynamicsModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input shape: (batch_size, 3, 2)\n",
    "        # output shape: (batch_size, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(3, 512, 2) # Bx3x2 -> Bx512x1\n",
    "        self.fc1 = nn.Linear(512, 256) # Bx512 -> Bx256\n",
    "        self.fc2 = nn.Linear(256, 256) # Bx256 -> Bx256\n",
    "        self.fc3 = nn.Linear(256, 2) # Bx256 -> Bx2\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.relu(self.conv1(x)) # Bx3x2 -> Bx512x1\n",
    "        x = torch.flatten(x, 1) # Bx512x1 -> Bx512\n",
    "        x = F.relu(self.fc1(x)) # Bx512 -> Bx256\n",
    "        x = F.relu(self.fc2(x)) # Bx256 -> Bx256\n",
    "        x = self.fc3(x) # Bx256 -> Bx2\n",
    "        return x\n",
    "\n",
    "def idm_train_batch(\n",
    "        mm: MetadriveModel,\n",
    "        idm: InverseDynamicsModel,\n",
    "        idm_optimizer: torch.optim.Optimizer,\n",
    "        obs_batch: list[Observation],\n",
    ") -> float:\n",
    "    device = deviceof(mm)\n",
    "\n",
    "    assert deviceof(idm) == device\n",
    "\n",
    "    obs_tensor = obs_batch_to_tensor(obs_batch, device)\n",
    "    s0_batch = state_batch_to_tensor([s0 for s0, _ in obs_batch], device)\n",
    "    s1_batch = state_batch_to_tensor([s1 for _, s1 in obs_batch], device)\n",
    "\n",
    "    idm_optimizer.zero_grad()\n",
    "\n",
    "    pred_action = idm(obs_tensor)\n",
    "    pred_s1 = mm(s0_batch, pred_action)\n",
    "\n",
    "    loss = F.mse_loss(pred_s1, s1_batch)\n",
    "    loss.backward()\n",
    "\n",
    "    idm_optimizer.step()\n",
    "\n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "idm_train_iter = itertools.cycle(idm_train_data)\n",
    "\n",
    "idm = InverseDynamicsModel().to(device)\n",
    "\n",
    "idm_optimizer = torch.optim.Adam(idm.parameters())\n",
    "\n",
    "idm_step = 0\n",
    "idm_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 56, Loss: 0.059\n",
      "Step: 57, Loss: 0.059\n",
      "Step: 58, Loss: 0.059\n",
      "Step: 59, Loss: 0.058\n",
      "Step: 60, Loss: 0.058\n",
      "Step: 61, Loss: 0.058\n",
      "Step: 62, Loss: 0.057\n",
      "Step: 63, Loss: 0.057\n",
      "Step: 64, Loss: 0.057\n",
      "Step: 65, Loss: 0.057\n",
      "Step: 66, Loss: 0.056\n",
      "Step: 67, Loss: 0.056\n",
      "Step: 68, Loss: 0.056\n",
      "Step: 69, Loss: 0.056\n",
      "Step: 70, Loss: 0.055\n",
      "Step: 71, Loss: 0.055\n",
      "Step: 72, Loss: 0.055\n",
      "Step: 73, Loss: 0.055\n",
      "Step: 74, Loss: 0.055\n",
      "Step: 75, Loss: 0.054\n",
      "Step: 76, Loss: 0.054\n",
      "Step: 77, Loss: 0.054\n",
      "Step: 78, Loss: 0.054\n",
      "Step: 79, Loss: 0.053\n",
      "Step: 80, Loss: 0.053\n",
      "Step: 81, Loss: 0.053\n",
      "Step: 82, Loss: 0.053\n",
      "Step: 83, Loss: 0.053\n",
      "Step: 84, Loss: 0.052\n",
      "Step: 85, Loss: 0.052\n",
      "Step: 86, Loss: 0.052\n",
      "Step: 87, Loss: 0.052\n",
      "Step: 88, Loss: 0.052\n",
      "Step: 89, Loss: 0.052\n",
      "Step: 90, Loss: 0.051\n",
      "Step: 91, Loss: 0.051\n",
      "Step: 92, Loss: 0.051\n",
      "Step: 93, Loss: 0.051\n",
      "Step: 94, Loss: 0.051\n",
      "Step: 95, Loss: 0.051\n",
      "Step: 96, Loss: 0.050\n",
      "Step: 97, Loss: 0.050\n",
      "Step: 98, Loss: 0.050\n",
      "Step: 99, Loss: 0.050\n",
      "Step: 100, Loss: 0.050\n",
      "Step: 101, Loss: 0.050\n",
      "Step: 102, Loss: 0.049\n",
      "Step: 103, Loss: 0.049\n",
      "Step: 104, Loss: 0.049\n",
      "Step: 105, Loss: 0.049\n",
      "Step: 106, Loss: 0.049\n",
      "Step: 107, Loss: 0.049\n",
      "Step: 108, Loss: 0.049\n",
      "Step: 109, Loss: 0.049\n",
      "Step: 110, Loss: 0.048\n",
      "Step: 111, Loss: 0.048\n",
      "Step: 112, Loss: 0.048\n",
      "Step: 113, Loss: 0.048\n",
      "Step: 114, Loss: 0.048\n",
      "Step: 115, Loss: 0.048\n",
      "Step: 116, Loss: 0.048\n",
      "Step: 117, Loss: 0.048\n",
      "Step: 118, Loss: 0.047\n",
      "Step: 119, Loss: 0.047\n",
      "Step: 120, Loss: 0.047\n",
      "Step: 121, Loss: 0.047\n",
      "Step: 122, Loss: 0.047\n",
      "Step: 123, Loss: 0.047\n",
      "Step: 124, Loss: 0.047\n",
      "Step: 125, Loss: 0.047\n",
      "Step: 126, Loss: 0.047\n",
      "Step: 127, Loss: 0.047\n",
      "Step: 128, Loss: 0.046\n",
      "Step: 129, Loss: 0.046\n",
      "Step: 130, Loss: 0.046\n",
      "Step: 131, Loss: 0.046\n",
      "Step: 132, Loss: 0.046\n",
      "Step: 133, Loss: 0.046\n",
      "Step: 134, Loss: 0.046\n",
      "Step: 135, Loss: 0.046\n",
      "Step: 136, Loss: 0.046\n",
      "Step: 137, Loss: 0.046\n",
      "Step: 138, Loss: 0.046\n",
      "Step: 139, Loss: 0.045\n",
      "Step: 140, Loss: 0.045\n",
      "Step: 141, Loss: 0.045\n",
      "Step: 142, Loss: 0.045\n",
      "Step: 143, Loss: 0.045\n",
      "Step: 144, Loss: 0.045\n",
      "Step: 145, Loss: 0.045\n",
      "Step: 146, Loss: 0.045\n",
      "Step: 147, Loss: 0.045\n",
      "Step: 148, Loss: 0.045\n",
      "Step: 149, Loss: 0.045\n",
      "Step: 150, Loss: 0.045\n",
      "Step: 151, Loss: 0.045\n",
      "Step: 152, Loss: 0.045\n",
      "Step: 153, Loss: 0.045\n",
      "Step: 154, Loss: 0.044\n",
      "Step: 155, Loss: 0.044\n",
      "Step: 156, Loss: 0.044\n",
      "Step: 157, Loss: 0.044\n",
      "Step: 158, Loss: 0.044\n",
      "Step: 159, Loss: 0.044\n",
      "Step: 160, Loss: 0.044\n",
      "Step: 161, Loss: 0.044\n",
      "Step: 162, Loss: 0.044\n",
      "Step: 163, Loss: 0.044\n",
      "Step: 164, Loss: 0.044\n",
      "Step: 165, Loss: 0.044\n",
      "Step: 166, Loss: 0.044\n",
      "Step: 167, Loss: 0.044\n",
      "Step: 168, Loss: 0.044\n",
      "Step: 169, Loss: 0.044\n",
      "Step: 170, Loss: 0.044\n",
      "Step: 171, Loss: 0.044\n",
      "Step: 172, Loss: 0.044\n",
      "Step: 173, Loss: 0.043\n",
      "Step: 174, Loss: 0.043\n",
      "Step: 175, Loss: 0.043\n",
      "Step: 176, Loss: 0.043\n",
      "Step: 177, Loss: 0.043\n",
      "Step: 178, Loss: 0.043\n",
      "Step: 179, Loss: 0.043\n",
      "Step: 180, Loss: 0.043\n",
      "Step: 181, Loss: 0.043\n",
      "Step: 182, Loss: 0.043\n",
      "Step: 183, Loss: 0.043\n",
      "Step: 184, Loss: 0.043\n",
      "Step: 185, Loss: 0.043\n",
      "Step: 186, Loss: 0.043\n",
      "Step: 187, Loss: 0.043\n",
      "Step: 188, Loss: 0.043\n",
      "Step: 189, Loss: 0.043\n",
      "Step: 190, Loss: 0.043\n",
      "Step: 191, Loss: 0.043\n",
      "Step: 192, Loss: 0.043\n",
      "Step: 193, Loss: 0.043\n",
      "Step: 194, Loss: 0.043\n",
      "Step: 195, Loss: 0.043\n",
      "Step: 196, Loss: 0.043\n",
      "Step: 197, Loss: 0.043\n",
      "Step: 198, Loss: 0.043\n",
      "Step: 199, Loss: 0.042\n",
      "Step: 200, Loss: 0.042\n",
      "Step: 201, Loss: 0.042\n",
      "Step: 202, Loss: 0.042\n",
      "Step: 203, Loss: 0.042\n",
      "Step: 204, Loss: 0.042\n",
      "Step: 205, Loss: 0.042\n",
      "Step: 206, Loss: 0.042\n",
      "Step: 207, Loss: 0.042\n",
      "Step: 208, Loss: 0.042\n",
      "Step: 209, Loss: 0.042\n",
      "Step: 210, Loss: 0.042\n",
      "Step: 211, Loss: 0.042\n",
      "Step: 212, Loss: 0.042\n",
      "Step: 213, Loss: 0.042\n",
      "Step: 214, Loss: 0.042\n",
      "Step: 215, Loss: 0.042\n",
      "Step: 216, Loss: 0.042\n",
      "Step: 217, Loss: 0.042\n",
      "Step: 218, Loss: 0.042\n",
      "Step: 219, Loss: 0.042\n",
      "Step: 220, Loss: 0.042\n",
      "Step: 221, Loss: 0.042\n",
      "Step: 222, Loss: 0.042\n",
      "Step: 223, Loss: 0.042\n",
      "Step: 224, Loss: 0.042\n",
      "Step: 225, Loss: 0.042\n",
      "Step: 226, Loss: 0.042\n",
      "Step: 227, Loss: 0.042\n",
      "Step: 228, Loss: 0.042\n",
      "Step: 229, Loss: 0.041\n",
      "Step: 230, Loss: 0.041\n",
      "Step: 231, Loss: 0.041\n",
      "Step: 232, Loss: 0.041\n",
      "Step: 233, Loss: 0.041\n",
      "Step: 234, Loss: 0.041\n",
      "Step: 235, Loss: 0.041\n",
      "Step: 236, Loss: 0.041\n",
      "Step: 237, Loss: 0.041\n",
      "Step: 238, Loss: 0.041\n",
      "Step: 239, Loss: 0.041\n",
      "Step: 240, Loss: 0.041\n",
      "Step: 241, Loss: 0.041\n",
      "Step: 242, Loss: 0.041\n",
      "Step: 243, Loss: 0.041\n",
      "Step: 244, Loss: 0.041\n",
      "Step: 245, Loss: 0.041\n",
      "Step: 246, Loss: 0.041\n",
      "Step: 247, Loss: 0.041\n",
      "Step: 248, Loss: 0.041\n",
      "Step: 249, Loss: 0.041\n",
      "Step: 250, Loss: 0.041\n",
      "Step: 251, Loss: 0.041\n",
      "Step: 252, Loss: 0.041\n",
      "Step: 253, Loss: 0.041\n",
      "Step: 254, Loss: 0.041\n",
      "Step: 255, Loss: 0.041\n",
      "Step: 256, Loss: 0.041\n",
      "Step: 257, Loss: 0.041\n",
      "Step: 258, Loss: 0.041\n",
      "Step: 259, Loss: 0.041\n",
      "Step: 260, Loss: 0.041\n",
      "Step: 261, Loss: 0.041\n",
      "Step: 262, Loss: 0.040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m idm_step \u001b[39m<\u001b[39m INVERSE_DYNAMICS_MODEL_TRAIN_EPOCHS:\n\u001b[1;32m      6\u001b[0m     obs_batch \u001b[39m=\u001b[39m [\u001b[39mnext\u001b[39m(idm_train_iter) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(INVERSE_DYNAMICS_MODEL_TRAIN_BATCH_SIZE)]\n\u001b[0;32m----> 7\u001b[0m     loss \u001b[39m=\u001b[39m idm_train_batch(\n\u001b[1;32m      8\u001b[0m         mm,\n\u001b[1;32m      9\u001b[0m         idm,\n\u001b[1;32m     10\u001b[0m         idm_optimizer,\n\u001b[1;32m     11\u001b[0m         obs_batch,\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     idm_losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     14\u001b[0m     idm_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[205], line 31\u001b[0m, in \u001b[0;36midm_train_batch\u001b[0;34m(mm, idm, idm_optimizer, obs_batch)\u001b[0m\n\u001b[1;32m     27\u001b[0m device \u001b[39m=\u001b[39m deviceof(mm)\n\u001b[1;32m     29\u001b[0m \u001b[39massert\u001b[39;00m deviceof(idm) \u001b[39m==\u001b[39m device\n\u001b[0;32m---> 31\u001b[0m obs_tensor \u001b[39m=\u001b[39m obs_batch_to_tensor(obs_batch, device)\n\u001b[1;32m     32\u001b[0m s0_batch \u001b[39m=\u001b[39m state_batch_to_tensor([s0 \u001b[39mfor\u001b[39;00m s0, _ \u001b[39min\u001b[39;00m obs_batch], device)\n\u001b[1;32m     33\u001b[0m s1_batch \u001b[39m=\u001b[39m state_batch_to_tensor([s1 \u001b[39mfor\u001b[39;00m _, s1 \u001b[39min\u001b[39;00m obs_batch], device)\n",
      "Cell \u001b[0;32mIn[78], line 94\u001b[0m, in \u001b[0;36mobs_batch_to_tensor\u001b[0;34m(obs, device)\u001b[0m\n\u001b[1;32m     91\u001b[0m observations \u001b[39m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m st0, st1 \u001b[39min\u001b[39;00m obs:\n\u001b[0;32m---> 94\u001b[0m     observations\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39marray([\n\u001b[1;32m     95\u001b[0m         [st0\u001b[39m.\u001b[39mvelocity[\u001b[39m0\u001b[39m], st1\u001b[39m.\u001b[39mvelocity[\u001b[39m0\u001b[39m]], \n\u001b[1;32m     96\u001b[0m         [st0\u001b[39m.\u001b[39mvelocity[\u001b[39m1\u001b[39m], st1\u001b[39m.\u001b[39mvelocity[\u001b[39m1\u001b[39m]],\n\u001b[1;32m     97\u001b[0m         [st0\u001b[39m.\u001b[39mheading, st1\u001b[39m.\u001b[39mheading]\n\u001b[1;32m     98\u001b[0m     ]))\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mstack(observations), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_lr(idm_optimizer, 1e-5)\n",
    "INVERSE_DYNAMICS_MODEL_TRAIN_EPOCHS = 1000\n",
    "INVERSE_DYNAMICS_MODEL_TRAIN_BATCH_SIZE = len(idm_train_data)\n",
    "\n",
    "while idm_step < INVERSE_DYNAMICS_MODEL_TRAIN_EPOCHS:\n",
    "    obs_batch = [next(idm_train_iter) for _ in range(INVERSE_DYNAMICS_MODEL_TRAIN_BATCH_SIZE)]\n",
    "    loss = idm_train_batch(\n",
    "        mm,\n",
    "        idm,\n",
    "        idm_optimizer,\n",
    "        obs_batch,\n",
    "    )\n",
    "    idm_losses.append(loss)\n",
    "    idm_step += 1\n",
    "    print(f\"Step: {idm_step}, Loss: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the losses over training\n",
    "plt.plot(list(range(len(idm_losses))), idm_losses, label='IDM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try visualizing our IDM by having it follow a track from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDMPolicy:\n",
    "    def __init__(self, net: InverseDynamicsModel):\n",
    "        self.net = net\n",
    "\n",
    "    def __call__(self, obs:Observation) -> Action:\n",
    "        # sample an action from the policy network\n",
    "        obs_tensor = obs_batch_to_tensor([obs], deviceof(self.net))\n",
    "        # sample an action from the policy network\n",
    "        with torch.no_grad():\n",
    "            steering, throttle = self.net(obs_tensor)[0]\n",
    "        return steering.item(), throttle.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MetaDrive-validation-v0\", config={\"on_continuous_line_done\": False, \"use_render\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.23398125171661377, 2.235740900039673)\n",
      "(0.22450466454029083, 2.2197630405426025)\n",
      "(0.21545925736427307, 2.19934344291687)\n",
      "(0.20586682856082916, 2.181835174560547)\n",
      "(0.1960819810628891, 2.163665533065796)\n",
      "(0.18639440834522247, 2.1444876194000244)\n",
      "(0.17687784135341644, 2.12400221824646)\n",
      "(0.16806603968143463, 2.1021616458892822)\n",
      "(0.15922504663467407, 2.0799710750579834)\n",
      "(0.15046285092830658, 2.0585060119628906)\n",
      "(0.14027027785778046, 2.0367465019226074)\n",
      "(0.13081146776676178, 2.01294207572937)\n",
      "(0.12264731526374817, 1.9907890558242798)\n",
      "(0.11516231298446655, 1.9738672971725464)\n",
      "(0.10618942975997925, 1.9583805799484253)\n",
      "(0.0952807366847992, 1.9372316598892212)\n",
      "(0.08570270240306854, 1.910286545753479)\n",
      "(0.07883188128471375, 1.8855081796646118)\n",
      "(0.07203981280326843, 1.8680500984191895)\n",
      "(0.06312429904937744, 1.853269100189209)\n",
      "(0.051841944456100464, 1.8336271047592163)\n",
      "(0.04071490466594696, 1.8072737455368042)\n",
      "(0.03375151753425598, 1.7818516492843628)\n",
      "(0.027531251311302185, 1.7625783681869507)\n",
      "(0.01902085542678833, 1.7461107969284058)\n",
      "(0.010251834988594055, 1.7276360988616943)\n",
      "(0.0024563074111938477, 1.704516887664795)\n",
      "(-0.004535600543022156, 1.6781814098358154)\n",
      "(-0.010804221034049988, 1.6536768674850464)\n",
      "(-0.016737431287765503, 1.6325947046279907)\n",
      "(-0.022941410541534424, 1.6132956743240356)\n",
      "(-0.028637751936912537, 1.5931340456008911)\n",
      "(-0.03277432918548584, 1.573100209236145)\n",
      "(-0.03752844035625458, 1.5544465780258179)\n",
      "(-0.04320511966943741, 1.534788966178894)\n",
      "(-0.04829804599285126, 1.5112197399139404)\n",
      "(-0.0512760728597641, 1.485801100730896)\n",
      "(-0.05340346693992615, 1.4638642072677612)\n",
      "(-0.057413019239902496, 1.4447981119155884)\n",
      "(-0.0607953816652298, 1.4233773946762085)\n",
      "(-0.06273561716079712, 1.401492714881897)\n",
      "(-0.06659659743309021, 1.3800801038742065)\n",
      "(-0.06964541971683502, 1.357002854347229)\n",
      "(-0.07069432735443115, 1.3344820737838745)\n",
      "(-0.07276448607444763, 1.3138819932937622)\n",
      "(-0.0752057284116745, 1.2940268516540527)\n",
      "(-0.0771687775850296, 1.2725588083267212)\n",
      "(-0.07795622944831848, 1.2513073682785034)\n",
      "(-0.08013150095939636, 1.2310420274734497)\n",
      "(-0.08213865011930466, 1.2086267471313477)\n",
      "(-0.08234640955924988, 1.1852351427078247)\n",
      "(-0.0830020159482956, 1.162681221961975)\n",
      "(-0.08382007479667664, 1.1403526067733765)\n",
      "(-0.08429918438196182, 1.1167105436325073)\n",
      "(-0.08428778499364853, 1.0914523601531982)\n",
      "(-0.08392821252346039, 1.0664888620376587)\n",
      "(-0.08330778777599335, 1.0424995422363281)\n",
      "(-0.08286882936954498, 1.018817663192749)\n",
      "(-0.08245468884706497, 0.9949331283569336)\n",
      "(-0.08174622058868408, 0.9718343615531921)\n",
      "(-0.08156874030828476, 0.9496022462844849)\n",
      "(-0.08128626644611359, 0.9269204139709473)\n",
      "(-0.08023597300052643, 0.9040477275848389)\n",
      "(-0.07880687713623047, 0.8820183873176575)\n",
      "(-0.07782141864299774, 0.8608118891716003)\n",
      "(-0.07693473249673843, 0.8392370939254761)\n",
      "(-0.0752701610326767, 0.8180424571037292)\n",
      "(-0.07334752380847931, 0.7979245781898499)\n",
      "(-0.0719631165266037, 0.7779995799064636)\n",
      "(-0.07054323703050613, 0.7572121620178223)\n",
      "(-0.06879198551177979, 0.7359519004821777)\n",
      "(-0.06722813099622726, 0.7140827178955078)\n",
      "(-0.0648537129163742, 0.6931415796279907)\n",
      "(-0.06315022706985474, 0.6737000942230225)\n",
      "(-0.06250276416540146, 0.6524025201797485)\n",
      "(-0.060508862137794495, 0.6286792159080505)\n",
      "(-0.05798821151256561, 0.6057257056236267)\n",
      "(-0.055654846131801605, 0.5851936340332031)\n",
      "(-0.05370258539915085, 0.5652927756309509)\n",
      "(-0.051786020398139954, 0.545319139957428)\n",
      "(-0.05016322433948517, 0.5251855850219727)\n",
      "(-0.047996729612350464, 0.5058136582374573)\n",
      "(-0.046168580651283264, 0.48711156845092773)\n",
      "(-0.04477319121360779, 0.46781378984451294)\n",
      "(-0.04279609024524689, 0.44972825050354004)\n",
      "(-0.0412016436457634, 0.4330018162727356)\n",
      "(-0.039891161024570465, 0.4162982106208801)\n",
      "(-0.03777480125427246, 0.39876776933670044)\n",
      "(-0.03556913882493973, 0.38101327419281006)\n",
      "(-0.03359169512987137, 0.3638155460357666)\n",
      "(-0.03172564134001732, 0.3475511968135834)\n",
      "(-0.029971513897180557, 0.3321031332015991)\n",
      "(-0.02836303785443306, 0.3160150349140167)\n",
      "(-0.02612810954451561, 0.29996055364608765)\n",
      "(-0.024513177573680878, 0.2855672240257263)\n",
      "(-0.023196879774332047, 0.27049723267555237)\n",
      "(-0.021211907267570496, 0.2546801269054413)\n",
      "(-0.01926754042506218, 0.23965950310230255)\n",
      "(-0.01757129281759262, 0.22523413598537445)\n",
      "(-0.015741996467113495, 0.21179793775081635)\n",
      "(-0.01419573649764061, 0.19939090311527252)\n",
      "(-0.012967973947525024, 0.18731920421123505)\n",
      "(-0.011985331773757935, 0.17636403441429138)\n",
      "(-0.011578571051359177, 0.16554343700408936)\n",
      "(-0.0107954740524292, 0.1539171189069748)\n",
      "(-0.010138418525457382, 0.142587348818779)\n",
      "(-0.009605787694454193, 0.13356329500675201)\n",
      "(-0.00920560210943222, 0.1256147176027298)\n",
      "(-0.00869268923997879, 0.1169259250164032)\n",
      "(-0.007839441299438477, 0.10952980816364288)\n",
      "(-0.007130883634090424, 0.10305628925561905)\n",
      "(-0.006502900272607803, 0.0962449312210083)\n",
      "(-0.006352655589580536, 0.09054511040449142)\n",
      "(-0.006290186196565628, 0.08531221747398376)\n",
      "(-0.006155837327241898, 0.08050873875617981)\n",
      "(-0.005934551358222961, 0.07661721110343933)\n",
      "(-0.006089530885219574, 0.07261405885219574)\n",
      "(-0.006160184741020203, 0.06752689927816391)\n",
      "(-0.006006699055433273, 0.0642065554857254)\n",
      "(-0.0060820505023002625, 0.06262803822755814)\n",
      "(-0.006405837833881378, 0.06128494068980217)\n",
      "(-0.006449401378631592, 0.05998401343822479)\n",
      "(-0.0065987929701805115, 0.05839325487613678)\n",
      "(-0.006437145173549652, 0.05635180324316025)\n",
      "(-0.0054693445563316345, 0.05505790561437607)\n",
      "(-0.004880599677562714, 0.05446501821279526)\n",
      "(-0.004579860717058182, 0.05422869324684143)\n",
      "(-0.004627503454685211, 0.05381049960851669)\n",
      "(-0.004296395927667618, 0.053212977945804596)\n",
      "(-0.003422847017645836, 0.053066685795784)\n",
      "(-0.0037248115986585617, 0.05337705463171005)\n",
      "(-0.0041968971490859985, 0.05317488685250282)\n",
      "(-0.003806913271546364, 0.052790410816669464)\n",
      "(-0.0036582518368959427, 0.05266536399722099)\n",
      "(-0.0035601165145635605, 0.05270063504576683)\n",
      "(-0.003580193966627121, 0.052817970514297485)\n",
      "(-0.0036818403750658035, 0.052977729588747025)\n",
      "(-0.0037545207887887955, 0.053346313536167145)\n",
      "(-0.0040847137570381165, 0.053771309554576874)\n",
      "(-0.004289235919713974, 0.05403832718729973)\n",
      "(-0.004374511539936066, 0.05408073961734772)\n",
      "(-0.004373379051685333, 0.05396205931901932)\n",
      "(-0.004309266805648804, 0.05393601953983307)\n",
      "(-0.004309147596359253, 0.05399118736386299)\n",
      "(-0.004312679171562195, 0.053974449634552)\n",
      "(-0.004296824336051941, 0.05396854877471924)\n",
      "(-0.004342436790466309, 0.053963132202625275)\n",
      "(-0.004350606352090836, 0.05391954258084297)\n",
      "(-0.004339456558227539, 0.05391303449869156)\n",
      "(-0.004332114011049271, 0.05394367873668671)\n",
      "(-0.004333600401878357, 0.05397816747426987)\n",
      "(-0.00436817854642868, 0.05398966372013092)\n",
      "(-0.004384554922580719, 0.05391654372215271)\n",
      "(-0.004351343959569931, 0.05385693162679672)\n",
      "(-0.004325702786445618, 0.053879715502262115)\n",
      "(-0.004321739077568054, 0.05389522761106491)\n",
      "(-0.004339050501585007, 0.05389074608683586)\n",
      "(-0.004334256052970886, 0.05389329791069031)\n",
      "(-0.0043277740478515625, 0.053899865597486496)\n",
      "(-0.004338692873716354, 0.053890589624643326)\n",
      "(-0.0043285563588142395, 0.05389096960425377)\n",
      "(-0.004311475902795792, 0.05390226095914841)\n",
      "(-0.004326723515987396, 0.05390652269124985)\n",
      "(-0.004347652196884155, 0.05390331894159317)\n",
      "(-0.004342574626207352, 0.053910031914711)\n",
      "(-0.004344142973423004, 0.05393017455935478)\n",
      "(-0.00433788076043129, 0.053926654160022736)\n",
      "(-0.004344359040260315, 0.05391211062669754)\n",
      "(-0.004354141652584076, 0.053905051201581955)\n",
      "(-0.004340760409832001, 0.05390981584787369)\n",
      "(-0.004358656704425812, 0.053892578929662704)\n",
      "(-0.0043234676122665405, 0.0538775697350502)\n",
      "(-0.004296351224184036, 0.05391372740268707)\n",
      "(-0.0043693482875823975, 0.05392053350806236)\n",
      "(-0.004383295774459839, 0.05386367440223694)\n",
      "(-0.004327747970819473, 0.05384877324104309)\n",
      "(-0.00432153046131134, 0.053909968584775925)\n",
      "(-0.0043462589383125305, 0.05395852029323578)\n",
      "(-0.004369810223579407, 0.05393797904253006)\n",
      "(-0.0043364837765693665, 0.053907111287117004)\n",
      "(-0.004314810037612915, 0.05393059924244881)\n",
      "(-0.004356548190116882, 0.05395815521478653)\n",
      "(-0.0043668970465660095, 0.05394493415951729)\n",
      "(-0.004343818873167038, 0.05392494797706604)\n",
      "(-0.004332393407821655, 0.053928300738334656)\n",
      "(-0.00434672087430954, 0.053926125168800354)\n",
      "(-0.004351422190666199, 0.05390843749046326)\n",
      "(-0.004347808659076691, 0.053905636072158813)\n",
      "(-0.0043346211314201355, 0.05390302464365959)\n",
      "(-0.004340857267379761, 0.053895775228738785)\n",
      "(-0.0043395161628723145, 0.05390843749046326)\n",
      "(-0.0043349489569664, 0.05394541099667549)\n",
      "(-0.004359986633062363, 0.05395980551838875)\n",
      "(-0.004356667399406433, 0.05392308533191681)\n",
      "(-0.004351586103439331, 0.05387735366821289)\n",
      "(-0.004341237246990204, 0.053869377821683884)\n",
      "(-0.004318609833717346, 0.05389299988746643)\n"
     ]
    }
   ],
   "source": [
    "scenario = h[5]\n",
    "with torch.no_grad():\n",
    "    # reset\n",
    "    env.reset()\n",
    "\n",
    "    # allow car to settle\n",
    "    for _ in range(10):\n",
    "        env.step([0,0])\n",
    "\n",
    "    st = scenario[0]\n",
    "\n",
    "    # set the initial state\n",
    "    env.vehicle.set_velocity(st.velocity)\n",
    "    env.vehicle.set_heading_theta(st.heading)\n",
    "\n",
    "\n",
    "    for i in range(len(scenario)-1):\n",
    "        st0 = scenario[i]\n",
    "        st1 = scenario[i+1]\n",
    "        action = IDMPolicy(idm)((st0, st1))\n",
    "        print(action)\n",
    "        env.step(action)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
