{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy\n",
    "\n",
    "In this notebook, we attempt to train an inverse dynamics model (IDM). In Reinforcement Learning parlance, an IDM learns to predict $a_t$ given $s_t$ and $s_{t+1}$. In our case, we have access to the MetaDrive simulator and the Waymo dataset. We want to predict the action the car should take in MetaDrive so that the successor state in the simulator is as close as possible to the successor state in the Waymo dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    heading: float\n",
    "    velocity: npt.NDArray[np.float64]\n",
    "\n",
    "\n",
    "Observation: typing.TypeAlias = tuple[State, State]\n",
    "Action: typing.TypeAlias = tuple[float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metadrive\n",
    "from metadrive import MetaDriveEnv\n",
    "import gymnasium as gym\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def deviceof(m: nn.Module) -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the device of the given module\n",
    "    \"\"\"\n",
    "    return next(m.parameters()).device\n",
    "\n",
    "def normalize_angle(angle: float) -> float:\n",
    "    \"\"\"\n",
    "    Normalize the angle to [-pi, pi)\n",
    "    \"\"\"\n",
    "    return (angle + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "def get_metadrive_state(env: MetaDriveEnv) -> State:\n",
    "    return State(heading=env.vehicle.heading_theta, velocity=env.vehicle.velocity[:2])\n",
    "\n",
    "def next_state(env: MetaDriveEnv, s: State, a: Action) -> State:\n",
    "    \"\"\"\n",
    "    runs the policy and returns the total reward\n",
    "    \"\"\"\n",
    "    # reset\n",
    "    env.reset()\n",
    "    env.vehicle.set_position(env.vehicle.position, height=0.49)\n",
    "\n",
    "    # allow car to settle\n",
    "    for _ in range(5):\n",
    "        env.step([0,0])\n",
    "\n",
    "    # set the initial state\n",
    "    env.vehicle.set_velocity(s.velocity)\n",
    "    env.vehicle.set_heading_theta(s.heading)\n",
    "    \n",
    "    # run the simulator\n",
    "    env.step(a)\n",
    "\n",
    "    # get the new state\n",
    "    s_prime = get_metadrive_state(env)\n",
    "\n",
    "    # allow car to settle (if rendering)\n",
    "    if env.config.use_render:\n",
    "        for _ in range(10):\n",
    "            env.step([0,0])\n",
    "\n",
    "    return s_prime\n",
    "\n",
    "def gen_scenario() -> tuple[State, Action]:\n",
    "    \"\"\"\n",
    "    Generates a random scenario\n",
    "    \"\"\"\n",
    "    # generate a random state\n",
    "    velocity = np.random.multivariate_normal([0, 0], np.eye(2) * 100)\n",
    "    heading = normalize_angle(np.arctan2(velocity[1], velocity[0]) + np.random.normal(0, np.pi/4))\n",
    "\n",
    "    s = State(heading=heading, velocity=velocity)\n",
    "\n",
    "    # generate a random action\n",
    "    steer = np.random.uniform(-1, 1)\n",
    "    throttle = np.random.uniform(-1, 1)\n",
    "    a = (steer, throttle)\n",
    "\n",
    "    return s, a\n",
    "\n",
    "def state_batch_to_tensor(states: list[State], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the state from State to a tensor of shape (batch_size, 3)\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.tensor(np.stack([\n",
    "        [st.velocity[0], st.velocity[1], st.heading] for st in states\n",
    "    ]), dtype=torch.float32, device=device)\n",
    "\n",
    "def action_batch_to_tensor(actions: list[Action], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the action from Action to a tensor of shape (batch_size, 2)\n",
    "    \"\"\"\n",
    "    return torch.tensor(np.stack(actions), dtype=torch.float32, device=device)\n",
    "\n",
    "def obs_batch_to_tensor(obs: list[Observation], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reshape the observation from tuple[State, State] to a tensor of shape (batch_size, 3, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    for st0, st1 in obs:\n",
    "        observations.append(np.array([\n",
    "            [st0.velocity[0], st1.velocity[0]], \n",
    "            [st0.velocity[1], st1.velocity[1]],\n",
    "            [st0.heading, st1.heading]\n",
    "        ]))\n",
    "\n",
    "    return torch.tensor(np.stack(observations), dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task may take a few minutes to run the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task((warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      ":task:task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n",
      "(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from metadrive import MetaDriveEnv\n",
    "\n",
    "MAX_WORKERS = 16\n",
    "DATASET_SIZE = 10000\n",
    "\n",
    "def generate_data(n_scenarios: int) -> list[tuple[State, Action, State]]:\n",
    "    env = MetaDriveEnv(config={\"on_continuous_line_done\": False, \"use_render\": False})\n",
    "    dataset: list[tuple[State, Action, State]] = []\n",
    "    for _ in range(n_scenarios):\n",
    "        s0, a = gen_scenario()\n",
    "        s1 = next_state(env, s0, a)\n",
    "        dataset.append((s0, a, s1))\n",
    "    env.close()\n",
    "    return dataset\n",
    "\n",
    "dataset: list[tuple[State, Action, State]] = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    batch_size, leftover_size = divmod(DATASET_SIZE, MAX_WORKERS)\n",
    "    for batch in executor.map(generate_data, [*[batch_size]*MAX_WORKERS, leftover_size]):\n",
    "        dataset.extend(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":task(warning): Creating implicit AsyncTaskChain default for AsyncTaskManager TaskManager\n"
     ]
    }
   ],
   "source": [
    "validation_data = generate_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# create a model that attempts to predict the next state given the current state and the action: (throttle and steering)\n",
    "# each state contains: velocity_x, velocity_y, and heading\n",
    "class MetadriveModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input shape: (batch_size, 3) + (batch_size, 2) = (batch_size, 5)\n",
    "        # output shape: (batch_size, 3)\n",
    "        self.fc1 = nn.Linear(5, 512) # Bx5 -> Bx512\n",
    "        self.fc2 = nn.Linear(512, 512) # Bx512 -> Bx256\n",
    "        self.fc3 = nn.Linear(512, 3) # Bx256 -> Bx3\n",
    "    \n",
    "    def forward(self, states: torch.Tensor, actions: torch.Tensor):\n",
    "        x = torch.cat([states, actions], dim=1) # Bx5\n",
    "        x = F.relu(self.fc1(x)) # Bx512\n",
    "        x = F.relu(self.fc2(x)) # Bx512\n",
    "        x = self.fc3(x) # Bx3\n",
    "        return x\n",
    "\n",
    "def train_metadrive_model_step(\n",
    "    mm: MetadriveModel,\n",
    "    mm_optimizer: torch.optim.Optimizer,\n",
    "    s0_batch: list[State],\n",
    "    a_batch: list[Action],\n",
    "    s1_batch: list[State],\n",
    ") -> float: \n",
    "    device = deviceof(mm)\n",
    "\n",
    "    s0_tensor = state_batch_to_tensor(s0_batch, device) \n",
    "    a_tensor = action_batch_to_tensor(a_batch, device)\n",
    "    s1_tensor = state_batch_to_tensor(s1_batch, device)\n",
    "\n",
    "    mm_optimizer.zero_grad()\n",
    "    s1_pred_tensor = mm(s0_tensor, a_tensor)\n",
    "    loss = F.mse_loss(s1_pred_tensor, s1_tensor)\n",
    "    loss.backward()\n",
    "    mm_optimizer.step()\n",
    "    return float(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optimizer: torch.optim.Optimizer, lr: float) -> None:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mm = MetadriveModel().to(device)\n",
    "\n",
    "mm_optimizer = torch.optim.Adam(mm.parameters())\n",
    "\n",
    "mm_step = 0\n",
    "mm_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we don't run out of data\n",
    "dataset_iter = iter(dataset*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable logging from metadrive\n",
    "import logging\n",
    "import inspect\n",
    "import metadrive.envs.base_env\n",
    "logging.getLogger(inspect.getfile(metadrive.envs.base_env)).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1953, Loss: 0.105\n",
      "Step: 1954, Loss: 0.104\n",
      "Step: 1955, Loss: 0.105\n",
      "Step: 1956, Loss: 0.105\n",
      "Step: 1957, Loss: 0.105\n",
      "Step: 1958, Loss: 0.104\n",
      "Step: 1959, Loss: 0.104\n",
      "Step: 1960, Loss: 0.105\n",
      "Step: 1961, Loss: 0.105\n",
      "Step: 1962, Loss: 0.104\n",
      "Step: 1963, Loss: 0.105\n",
      "Step: 1964, Loss: 0.105\n",
      "Step: 1965, Loss: 0.104\n",
      "Step: 1966, Loss: 0.104\n",
      "Step: 1967, Loss: 0.105\n",
      "Step: 1968, Loss: 0.105\n",
      "Step: 1969, Loss: 0.104\n",
      "Step: 1970, Loss: 0.104\n",
      "Step: 1971, Loss: 0.104\n",
      "Step: 1972, Loss: 0.104\n",
      "Step: 1973, Loss: 0.104\n",
      "Step: 1974, Loss: 0.104\n",
      "Step: 1975, Loss: 0.105\n",
      "Step: 1976, Loss: 0.104\n",
      "Step: 1977, Loss: 0.104\n",
      "Step: 1978, Loss: 0.104\n",
      "Step: 1979, Loss: 0.104\n",
      "Step: 1980, Loss: 0.104\n",
      "Step: 1981, Loss: 0.104\n",
      "Step: 1982, Loss: 0.104\n",
      "Step: 1983, Loss: 0.103\n",
      "Step: 1984, Loss: 0.104\n",
      "Step: 1985, Loss: 0.104\n",
      "Step: 1986, Loss: 0.104\n",
      "Step: 1987, Loss: 0.103\n",
      "Step: 1988, Loss: 0.104\n",
      "Step: 1989, Loss: 0.104\n",
      "Step: 1990, Loss: 0.104\n",
      "Step: 1991, Loss: 0.104\n",
      "Step: 1992, Loss: 0.104\n",
      "Step: 1993, Loss: 0.104\n",
      "Step: 1994, Loss: 0.103\n",
      "Step: 1995, Loss: 0.103\n",
      "Step: 1996, Loss: 0.104\n",
      "Step: 1997, Loss: 0.104\n",
      "Step: 1998, Loss: 0.104\n",
      "Step: 1999, Loss: 0.103\n",
      "Step: 2000, Loss: 0.104\n",
      "Step: 2001, Loss: 0.103\n",
      "Step: 2002, Loss: 0.103\n",
      "Step: 2003, Loss: 0.103\n",
      "Step: 2004, Loss: 0.104\n",
      "Step: 2005, Loss: 0.103\n",
      "Step: 2006, Loss: 0.103\n",
      "Step: 2007, Loss: 0.103\n",
      "Step: 2008, Loss: 0.103\n",
      "Step: 2009, Loss: 0.103\n",
      "Step: 2010, Loss: 0.103\n",
      "Step: 2011, Loss: 0.103\n",
      "Step: 2012, Loss: 0.103\n",
      "Step: 2013, Loss: 0.103\n",
      "Step: 2014, Loss: 0.103\n",
      "Step: 2015, Loss: 0.103\n",
      "Step: 2016, Loss: 0.103\n",
      "Step: 2017, Loss: 0.103\n",
      "Step: 2018, Loss: 0.103\n",
      "Step: 2019, Loss: 0.102\n",
      "Step: 2020, Loss: 0.103\n",
      "Step: 2021, Loss: 0.103\n",
      "Step: 2022, Loss: 0.103\n",
      "Step: 2023, Loss: 0.102\n",
      "Step: 2024, Loss: 0.103\n",
      "Step: 2025, Loss: 0.103\n",
      "Step: 2026, Loss: 0.103\n",
      "Step: 2027, Loss: 0.103\n",
      "Step: 2028, Loss: 0.103\n",
      "Step: 2029, Loss: 0.103\n",
      "Step: 2030, Loss: 0.102\n",
      "Step: 2031, Loss: 0.103\n",
      "Step: 2032, Loss: 0.103\n",
      "Step: 2033, Loss: 0.103\n",
      "Step: 2034, Loss: 0.102\n",
      "Step: 2035, Loss: 0.103\n",
      "Step: 2036, Loss: 0.102\n",
      "Step: 2037, Loss: 0.102\n",
      "Step: 2038, Loss: 0.102\n",
      "Step: 2039, Loss: 0.102\n",
      "Step: 2040, Loss: 0.103\n",
      "Step: 2041, Loss: 0.102\n",
      "Step: 2042, Loss: 0.102\n",
      "Step: 2043, Loss: 0.102\n",
      "Step: 2044, Loss: 0.103\n",
      "Step: 2045, Loss: 0.102\n",
      "Step: 2046, Loss: 0.102\n",
      "Step: 2047, Loss: 0.102\n",
      "Step: 2048, Loss: 0.102\n",
      "Step: 2049, Loss: 0.102\n",
      "Step: 2050, Loss: 0.102\n",
      "Step: 2051, Loss: 0.103\n",
      "Step: 2052, Loss: 0.102\n",
      "Step: 2053, Loss: 0.102\n",
      "Step: 2054, Loss: 0.102\n",
      "Step: 2055, Loss: 0.102\n",
      "Step: 2056, Loss: 0.102\n",
      "Step: 2057, Loss: 0.102\n",
      "Step: 2058, Loss: 0.102\n",
      "Step: 2059, Loss: 0.102\n",
      "Step: 2060, Loss: 0.102\n",
      "Step: 2061, Loss: 0.102\n",
      "Step: 2062, Loss: 0.102\n",
      "Step: 2063, Loss: 0.102\n",
      "Step: 2064, Loss: 0.102\n",
      "Step: 2065, Loss: 0.102\n",
      "Step: 2066, Loss: 0.101\n",
      "Step: 2067, Loss: 0.102\n",
      "Step: 2068, Loss: 0.102\n",
      "Step: 2069, Loss: 0.102\n",
      "Step: 2070, Loss: 0.101\n",
      "Step: 2071, Loss: 0.101\n",
      "Step: 2072, Loss: 0.102\n",
      "Step: 2073, Loss: 0.102\n",
      "Step: 2074, Loss: 0.101\n",
      "Step: 2075, Loss: 0.101\n",
      "Step: 2076, Loss: 0.102\n",
      "Step: 2077, Loss: 0.101\n",
      "Step: 2078, Loss: 0.101\n",
      "Step: 2079, Loss: 0.102\n",
      "Step: 2080, Loss: 0.102\n",
      "Step: 2081, Loss: 0.101\n",
      "Step: 2082, Loss: 0.101\n",
      "Step: 2083, Loss: 0.101\n",
      "Step: 2084, Loss: 0.101\n",
      "Step: 2085, Loss: 0.101\n",
      "Step: 2086, Loss: 0.101\n",
      "Step: 2087, Loss: 0.102\n",
      "Step: 2088, Loss: 0.101\n",
      "Step: 2089, Loss: 0.101\n",
      "Step: 2090, Loss: 0.101\n",
      "Step: 2091, Loss: 0.101\n",
      "Step: 2092, Loss: 0.101\n",
      "Step: 2093, Loss: 0.101\n",
      "Step: 2094, Loss: 0.101\n",
      "Step: 2095, Loss: 0.100\n",
      "Step: 2096, Loss: 0.101\n",
      "Step: 2097, Loss: 0.101\n",
      "Step: 2098, Loss: 0.101\n",
      "Step: 2099, Loss: 0.100\n",
      "Step: 2100, Loss: 0.101\n",
      "Step: 2101, Loss: 0.101\n",
      "Step: 2102, Loss: 0.101\n",
      "Step: 2103, Loss: 0.101\n",
      "Step: 2104, Loss: 0.101\n",
      "Step: 2105, Loss: 0.101\n",
      "Step: 2106, Loss: 0.100\n",
      "Step: 2107, Loss: 0.100\n",
      "Step: 2108, Loss: 0.101\n",
      "Step: 2109, Loss: 0.101\n",
      "Step: 2110, Loss: 0.101\n",
      "Step: 2111, Loss: 0.100\n",
      "Step: 2112, Loss: 0.101\n",
      "Step: 2113, Loss: 0.100\n",
      "Step: 2114, Loss: 0.101\n",
      "Step: 2115, Loss: 0.101\n",
      "Step: 2116, Loss: 0.101\n",
      "Step: 2117, Loss: 0.100\n",
      "Step: 2118, Loss: 0.100\n",
      "Step: 2119, Loss: 0.100\n",
      "Step: 2120, Loss: 0.100\n",
      "Step: 2121, Loss: 0.100\n",
      "Step: 2122, Loss: 0.100\n",
      "Step: 2123, Loss: 0.100\n",
      "Step: 2124, Loss: 0.100\n",
      "Step: 2125, Loss: 0.100\n",
      "Step: 2126, Loss: 0.100\n",
      "Step: 2127, Loss: 0.100\n",
      "Step: 2128, Loss: 0.100\n",
      "Step: 2129, Loss: 0.100\n",
      "Step: 2130, Loss: 0.100\n",
      "Step: 2131, Loss: 0.100\n",
      "Step: 2132, Loss: 0.100\n",
      "Step: 2133, Loss: 0.100\n",
      "Step: 2134, Loss: 0.101\n",
      "Step: 2135, Loss: 0.100\n",
      "Step: 2136, Loss: 0.100\n",
      "Step: 2137, Loss: 0.100\n",
      "Step: 2138, Loss: 0.100\n",
      "Step: 2139, Loss: 0.100\n",
      "Step: 2140, Loss: 0.100\n",
      "Step: 2141, Loss: 0.100\n",
      "Step: 2142, Loss: 0.099\n",
      "Step: 2143, Loss: 0.100\n",
      "Step: 2144, Loss: 0.100\n",
      "Step: 2145, Loss: 0.100\n",
      "Step: 2146, Loss: 0.099\n",
      "Step: 2147, Loss: 0.100\n",
      "Step: 2148, Loss: 0.100\n",
      "Step: 2149, Loss: 0.100\n",
      "Step: 2150, Loss: 0.100\n",
      "Step: 2151, Loss: 0.100\n",
      "Step: 2152, Loss: 0.100\n",
      "Step: 2153, Loss: 0.099\n",
      "Step: 2154, Loss: 0.099\n",
      "Step: 2155, Loss: 0.100\n",
      "Step: 2156, Loss: 0.100\n",
      "Step: 2157, Loss: 0.099\n",
      "Step: 2158, Loss: 0.099\n",
      "Step: 2159, Loss: 0.100\n",
      "Step: 2160, Loss: 0.099\n",
      "Step: 2161, Loss: 0.100\n",
      "Step: 2162, Loss: 0.099\n",
      "Step: 2163, Loss: 0.100\n",
      "Step: 2164, Loss: 0.099\n",
      "Step: 2165, Loss: 0.099\n",
      "Step: 2166, Loss: 0.099\n",
      "Step: 2167, Loss: 0.099\n",
      "Step: 2168, Loss: 0.099\n",
      "Step: 2169, Loss: 0.099\n",
      "Step: 2170, Loss: 0.099\n",
      "Step: 2171, Loss: 0.099\n",
      "Step: 2172, Loss: 0.099\n",
      "Step: 2173, Loss: 0.099\n",
      "Step: 2174, Loss: 0.099\n",
      "Step: 2175, Loss: 0.099\n",
      "Step: 2176, Loss: 0.099\n",
      "Step: 2177, Loss: 0.099\n",
      "Step: 2178, Loss: 0.099\n",
      "Step: 2179, Loss: 0.099\n",
      "Step: 2180, Loss: 0.099\n",
      "Step: 2181, Loss: 0.099\n",
      "Step: 2182, Loss: 0.099\n",
      "Step: 2183, Loss: 0.099\n",
      "Step: 2184, Loss: 0.099\n",
      "Step: 2185, Loss: 0.099\n",
      "Step: 2186, Loss: 0.099\n",
      "Step: 2187, Loss: 0.099\n",
      "Step: 2188, Loss: 0.099\n",
      "Step: 2189, Loss: 0.098\n",
      "Step: 2190, Loss: 0.098\n",
      "Step: 2191, Loss: 0.099\n",
      "Step: 2192, Loss: 0.099\n",
      "Step: 2193, Loss: 0.098\n",
      "Step: 2194, Loss: 0.099\n",
      "Step: 2195, Loss: 0.099\n",
      "Step: 2196, Loss: 0.098\n",
      "Step: 2197, Loss: 0.099\n",
      "Step: 2198, Loss: 0.098\n",
      "Step: 2199, Loss: 0.099\n",
      "Step: 2200, Loss: 0.098\n",
      "Step: 2201, Loss: 0.098\n",
      "Step: 2202, Loss: 0.098\n",
      "Step: 2203, Loss: 0.099\n",
      "Step: 2204, Loss: 0.098\n",
      "Step: 2205, Loss: 0.098\n",
      "Step: 2206, Loss: 0.098\n",
      "Step: 2207, Loss: 0.098\n",
      "Step: 2208, Loss: 0.098\n",
      "Step: 2209, Loss: 0.098\n",
      "Step: 2210, Loss: 0.099\n",
      "Step: 2211, Loss: 0.098\n",
      "Step: 2212, Loss: 0.098\n",
      "Step: 2213, Loss: 0.098\n",
      "Step: 2214, Loss: 0.098\n",
      "Step: 2215, Loss: 0.098\n",
      "Step: 2216, Loss: 0.098\n",
      "Step: 2217, Loss: 0.098\n",
      "Step: 2218, Loss: 0.098\n",
      "Step: 2219, Loss: 0.098\n",
      "Step: 2220, Loss: 0.098\n",
      "Step: 2221, Loss: 0.098\n",
      "Step: 2222, Loss: 0.098\n",
      "Step: 2223, Loss: 0.098\n",
      "Step: 2224, Loss: 0.098\n",
      "Step: 2225, Loss: 0.097\n",
      "Step: 2226, Loss: 0.098\n",
      "Step: 2227, Loss: 0.098\n",
      "Step: 2228, Loss: 0.098\n",
      "Step: 2229, Loss: 0.097\n",
      "Step: 2230, Loss: 0.098\n",
      "Step: 2231, Loss: 0.098\n",
      "Step: 2232, Loss: 0.098\n",
      "Step: 2233, Loss: 0.098\n",
      "Step: 2234, Loss: 0.098\n",
      "Step: 2235, Loss: 0.098\n",
      "Step: 2236, Loss: 0.097\n",
      "Step: 2237, Loss: 0.097\n",
      "Step: 2238, Loss: 0.098\n",
      "Step: 2239, Loss: 0.098\n",
      "Step: 2240, Loss: 0.097\n",
      "Step: 2241, Loss: 0.097\n",
      "Step: 2242, Loss: 0.098\n",
      "Step: 2243, Loss: 0.097\n",
      "Step: 2244, Loss: 0.098\n",
      "Step: 2245, Loss: 0.097\n",
      "Step: 2246, Loss: 0.098\n",
      "Step: 2247, Loss: 0.097\n",
      "Step: 2248, Loss: 0.097\n",
      "Step: 2249, Loss: 0.097\n",
      "Step: 2250, Loss: 0.098\n",
      "Step: 2251, Loss: 0.097\n",
      "Step: 2252, Loss: 0.097\n",
      "Step: 2253, Loss: 0.097\n",
      "Step: 2254, Loss: 0.097\n",
      "Step: 2255, Loss: 0.097\n",
      "Step: 2256, Loss: 0.097\n",
      "Step: 2257, Loss: 0.098\n",
      "Step: 2258, Loss: 0.097\n",
      "Step: 2259, Loss: 0.097\n",
      "Step: 2260, Loss: 0.097\n",
      "Step: 2261, Loss: 0.097\n",
      "Step: 2262, Loss: 0.097\n",
      "Step: 2263, Loss: 0.097\n",
      "Step: 2264, Loss: 0.097\n",
      "Step: 2265, Loss: 0.097\n",
      "Step: 2266, Loss: 0.096\n",
      "Step: 2267, Loss: 0.097\n",
      "Step: 2268, Loss: 0.097\n",
      "Step: 2269, Loss: 0.097\n",
      "Step: 2270, Loss: 0.097\n",
      "Step: 2271, Loss: 0.097\n",
      "Step: 2272, Loss: 0.096\n",
      "Step: 2273, Loss: 0.097\n",
      "Step: 2274, Loss: 0.097\n",
      "Step: 2275, Loss: 0.097\n",
      "Step: 2276, Loss: 0.096\n",
      "Step: 2277, Loss: 0.097\n",
      "Step: 2278, Loss: 0.097\n",
      "Step: 2279, Loss: 0.097\n",
      "Step: 2280, Loss: 0.096\n",
      "Step: 2281, Loss: 0.097\n",
      "Step: 2282, Loss: 0.097\n",
      "Step: 2283, Loss: 0.096\n",
      "Step: 2284, Loss: 0.096\n",
      "Step: 2285, Loss: 0.097\n",
      "Step: 2286, Loss: 0.097\n",
      "Step: 2287, Loss: 0.096\n",
      "Step: 2288, Loss: 0.096\n",
      "Step: 2289, Loss: 0.097\n",
      "Step: 2290, Loss: 0.096\n",
      "Step: 2291, Loss: 0.096\n",
      "Step: 2292, Loss: 0.096\n",
      "Step: 2293, Loss: 0.097\n",
      "Step: 2294, Loss: 0.096\n",
      "Step: 2295, Loss: 0.096\n",
      "Step: 2296, Loss: 0.096\n",
      "Step: 2297, Loss: 0.096\n",
      "Step: 2298, Loss: 0.096\n",
      "Step: 2299, Loss: 0.096\n",
      "Step: 2300, Loss: 0.096\n",
      "Step: 2301, Loss: 0.096\n",
      "Step: 2302, Loss: 0.096\n",
      "Step: 2303, Loss: 0.096\n",
      "Step: 2304, Loss: 0.096\n",
      "Step: 2305, Loss: 0.096\n",
      "Step: 2306, Loss: 0.096\n",
      "Step: 2307, Loss: 0.096\n",
      "Step: 2308, Loss: 0.096\n",
      "Step: 2309, Loss: 0.096\n",
      "Step: 2310, Loss: 0.096\n",
      "Step: 2311, Loss: 0.096\n",
      "Step: 2312, Loss: 0.096\n",
      "Step: 2313, Loss: 0.096\n",
      "Step: 2314, Loss: 0.096\n",
      "Step: 2315, Loss: 0.096\n",
      "Step: 2316, Loss: 0.096\n",
      "Step: 2317, Loss: 0.096\n",
      "Step: 2318, Loss: 0.096\n",
      "Step: 2319, Loss: 0.095\n",
      "Step: 2320, Loss: 0.096\n",
      "Step: 2321, Loss: 0.096\n",
      "Step: 2322, Loss: 0.096\n",
      "Step: 2323, Loss: 0.095\n",
      "Step: 2324, Loss: 0.096\n",
      "Step: 2325, Loss: 0.096\n",
      "Step: 2326, Loss: 0.096\n",
      "Step: 2327, Loss: 0.096\n",
      "Step: 2328, Loss: 0.096\n",
      "Step: 2329, Loss: 0.096\n",
      "Step: 2330, Loss: 0.095\n",
      "Step: 2331, Loss: 0.095\n",
      "Step: 2332, Loss: 0.096\n",
      "Step: 2333, Loss: 0.096\n",
      "Step: 2334, Loss: 0.095\n",
      "Step: 2335, Loss: 0.095\n",
      "Step: 2336, Loss: 0.096\n",
      "Step: 2337, Loss: 0.095\n",
      "Step: 2338, Loss: 0.095\n",
      "Step: 2339, Loss: 0.095\n",
      "Step: 2340, Loss: 0.096\n",
      "Step: 2341, Loss: 0.095\n",
      "Step: 2342, Loss: 0.095\n",
      "Step: 2343, Loss: 0.095\n",
      "Step: 2344, Loss: 0.095\n",
      "Step: 2345, Loss: 0.095\n",
      "Step: 2346, Loss: 0.095\n",
      "Step: 2347, Loss: 0.095\n",
      "Step: 2348, Loss: 0.095\n",
      "Step: 2349, Loss: 0.095\n",
      "Step: 2350, Loss: 0.095\n",
      "Step: 2351, Loss: 0.095\n",
      "Step: 2352, Loss: 0.095\n",
      "Step: 2353, Loss: 0.095\n",
      "Step: 2354, Loss: 0.095\n",
      "Step: 2355, Loss: 0.094\n",
      "Step: 2356, Loss: 0.095\n",
      "Step: 2357, Loss: 0.095\n",
      "Step: 2358, Loss: 0.095\n",
      "Step: 2359, Loss: 0.095\n",
      "Step: 2360, Loss: 0.095\n",
      "Step: 2361, Loss: 0.095\n",
      "Step: 2362, Loss: 0.095\n",
      "Step: 2363, Loss: 0.095\n",
      "Step: 2364, Loss: 0.095\n",
      "Step: 2365, Loss: 0.095\n",
      "Step: 2366, Loss: 0.094\n",
      "Step: 2367, Loss: 0.095\n",
      "Step: 2368, Loss: 0.095\n",
      "Step: 2369, Loss: 0.095\n",
      "Step: 2370, Loss: 0.094\n",
      "Step: 2371, Loss: 0.095\n",
      "Step: 2372, Loss: 0.095\n",
      "Step: 2373, Loss: 0.095\n",
      "Step: 2374, Loss: 0.095\n",
      "Step: 2375, Loss: 0.095\n",
      "Step: 2376, Loss: 0.095\n",
      "Step: 2377, Loss: 0.094\n",
      "Step: 2378, Loss: 0.094\n",
      "Step: 2379, Loss: 0.095\n",
      "Step: 2380, Loss: 0.095\n",
      "Step: 2381, Loss: 0.094\n",
      "Step: 2382, Loss: 0.094\n",
      "Step: 2383, Loss: 0.095\n",
      "Step: 2384, Loss: 0.094\n",
      "Step: 2385, Loss: 0.095\n",
      "Step: 2386, Loss: 0.094\n",
      "Step: 2387, Loss: 0.095\n",
      "Step: 2388, Loss: 0.094\n",
      "Step: 2389, Loss: 0.094\n",
      "Step: 2390, Loss: 0.094\n",
      "Step: 2391, Loss: 0.094\n",
      "Step: 2392, Loss: 0.094\n",
      "Step: 2393, Loss: 0.095\n",
      "Step: 2394, Loss: 0.094\n",
      "Step: 2395, Loss: 0.094\n",
      "Step: 2396, Loss: 0.094\n",
      "Step: 2397, Loss: 0.094\n",
      "Step: 2398, Loss: 0.094\n",
      "Step: 2399, Loss: 0.094\n",
      "Step: 2400, Loss: 0.094\n",
      "Step: 2401, Loss: 0.094\n",
      "Step: 2402, Loss: 0.094\n",
      "Step: 2403, Loss: 0.094\n",
      "Step: 2404, Loss: 0.094\n",
      "Step: 2405, Loss: 0.094\n",
      "Step: 2406, Loss: 0.094\n",
      "Step: 2407, Loss: 0.094\n",
      "Step: 2408, Loss: 0.094\n",
      "Step: 2409, Loss: 0.094\n",
      "Step: 2410, Loss: 0.094\n",
      "Step: 2411, Loss: 0.094\n",
      "Step: 2412, Loss: 0.094\n",
      "Step: 2413, Loss: 0.093\n",
      "Step: 2414, Loss: 0.094\n",
      "Step: 2415, Loss: 0.094\n",
      "Step: 2416, Loss: 0.094\n",
      "Step: 2417, Loss: 0.093\n",
      "Step: 2418, Loss: 0.094\n",
      "Step: 2419, Loss: 0.094\n",
      "Step: 2420, Loss: 0.093\n",
      "Step: 2421, Loss: 0.094\n",
      "Step: 2422, Loss: 0.094\n",
      "Step: 2423, Loss: 0.094\n",
      "Step: 2424, Loss: 0.093\n",
      "Step: 2425, Loss: 0.093\n",
      "Step: 2426, Loss: 0.094\n",
      "Step: 2427, Loss: 0.094\n",
      "Step: 2428, Loss: 0.093\n",
      "Step: 2429, Loss: 0.094\n",
      "Step: 2430, Loss: 0.094\n",
      "Step: 2431, Loss: 0.093\n",
      "Step: 2432, Loss: 0.093\n",
      "Step: 2433, Loss: 0.094\n",
      "Step: 2434, Loss: 0.094\n",
      "Step: 2435, Loss: 0.093\n",
      "Step: 2436, Loss: 0.093\n",
      "Step: 2437, Loss: 0.093\n",
      "Step: 2438, Loss: 0.093\n",
      "Step: 2439, Loss: 0.093\n",
      "Step: 2440, Loss: 0.093\n",
      "Step: 2441, Loss: 0.094\n",
      "Step: 2442, Loss: 0.093\n",
      "Step: 2443, Loss: 0.093\n",
      "Step: 2444, Loss: 0.093\n",
      "Step: 2445, Loss: 0.094\n",
      "Step: 2446, Loss: 0.093\n",
      "Step: 2447, Loss: 0.093\n",
      "Step: 2448, Loss: 0.093\n",
      "Step: 2449, Loss: 0.093\n",
      "Step: 2450, Loss: 0.093\n",
      "Step: 2451, Loss: 0.093\n",
      "Step: 2452, Loss: 0.093\n",
      "Step: 2453, Loss: 0.093\n",
      "Step: 2454, Loss: 0.093\n",
      "Step: 2455, Loss: 0.093\n",
      "Step: 2456, Loss: 0.093\n",
      "Step: 2457, Loss: 0.093\n",
      "Step: 2458, Loss: 0.093\n",
      "Step: 2459, Loss: 0.093\n",
      "Step: 2460, Loss: 0.093\n",
      "Step: 2461, Loss: 0.093\n",
      "Step: 2462, Loss: 0.093\n",
      "Step: 2463, Loss: 0.093\n",
      "Step: 2464, Loss: 0.093\n",
      "Step: 2465, Loss: 0.093\n",
      "Step: 2466, Loss: 0.093\n",
      "Step: 2467, Loss: 0.092\n",
      "Step: 2468, Loss: 0.093\n",
      "Step: 2469, Loss: 0.093\n",
      "Step: 2470, Loss: 0.093\n",
      "Step: 2471, Loss: 0.092\n",
      "Step: 2472, Loss: 0.092\n",
      "Step: 2473, Loss: 0.093\n",
      "Step: 2474, Loss: 0.093\n",
      "Step: 2475, Loss: 0.092\n",
      "Step: 2476, Loss: 0.093\n",
      "Step: 2477, Loss: 0.093\n",
      "Step: 2478, Loss: 0.092\n",
      "Step: 2479, Loss: 0.092\n",
      "Step: 2480, Loss: 0.093\n",
      "Step: 2481, Loss: 0.093\n",
      "Step: 2482, Loss: 0.092\n",
      "Step: 2483, Loss: 0.092\n",
      "Step: 2484, Loss: 0.092\n",
      "Step: 2485, Loss: 0.092\n",
      "Step: 2486, Loss: 0.092\n",
      "Step: 2487, Loss: 0.092\n",
      "Step: 2488, Loss: 0.093\n",
      "Step: 2489, Loss: 0.092\n",
      "Step: 2490, Loss: 0.092\n",
      "Step: 2491, Loss: 0.092\n",
      "Step: 2492, Loss: 0.093\n",
      "Step: 2493, Loss: 0.092\n",
      "Step: 2494, Loss: 0.092\n",
      "Step: 2495, Loss: 0.092\n",
      "Step: 2496, Loss: 0.092\n",
      "Step: 2497, Loss: 0.092\n",
      "Step: 2498, Loss: 0.092\n",
      "Step: 2499, Loss: 0.092\n",
      "Step: 2500, Loss: 0.092\n",
      "Step: 2501, Loss: 0.092\n",
      "Step: 2502, Loss: 0.092\n",
      "Step: 2503, Loss: 0.092\n",
      "Step: 2504, Loss: 0.092\n",
      "Step: 2505, Loss: 0.092\n",
      "Step: 2506, Loss: 0.092\n",
      "Step: 2507, Loss: 0.092\n",
      "Step: 2508, Loss: 0.092\n",
      "Step: 2509, Loss: 0.092\n",
      "Step: 2510, Loss: 0.092\n",
      "Step: 2511, Loss: 0.092\n",
      "Step: 2512, Loss: 0.092\n",
      "Step: 2513, Loss: 0.092\n",
      "Step: 2514, Loss: 0.091\n",
      "Step: 2515, Loss: 0.092\n",
      "Step: 2516, Loss: 0.092\n",
      "Step: 2517, Loss: 0.092\n",
      "Step: 2518, Loss: 0.091\n",
      "Step: 2519, Loss: 0.092\n",
      "Step: 2520, Loss: 0.092\n",
      "Step: 2521, Loss: 0.092\n",
      "Step: 2522, Loss: 0.092\n",
      "Step: 2523, Loss: 0.092\n",
      "Step: 2524, Loss: 0.092\n",
      "Step: 2525, Loss: 0.091\n",
      "Step: 2526, Loss: 0.091\n",
      "Step: 2527, Loss: 0.092\n",
      "Step: 2528, Loss: 0.092\n",
      "Step: 2529, Loss: 0.091\n",
      "Step: 2530, Loss: 0.092\n",
      "Step: 2531, Loss: 0.092\n",
      "Step: 2532, Loss: 0.091\n",
      "Step: 2533, Loss: 0.092\n",
      "Step: 2534, Loss: 0.092\n",
      "Step: 2535, Loss: 0.092\n",
      "Step: 2536, Loss: 0.091\n",
      "Step: 2537, Loss: 0.091\n",
      "Step: 2538, Loss: 0.091\n",
      "Step: 2539, Loss: 0.092\n",
      "Step: 2540, Loss: 0.091\n",
      "Step: 2541, Loss: 0.091\n",
      "Step: 2542, Loss: 0.091\n",
      "Step: 2543, Loss: 0.091\n",
      "Step: 2544, Loss: 0.091\n",
      "Step: 2545, Loss: 0.091\n",
      "Step: 2546, Loss: 0.092\n",
      "Step: 2547, Loss: 0.091\n",
      "Step: 2548, Loss: 0.091\n",
      "Step: 2549, Loss: 0.091\n",
      "Step: 2550, Loss: 0.091\n",
      "Step: 2551, Loss: 0.091\n",
      "Step: 2552, Loss: 0.091\n",
      "Step: 2553, Loss: 0.091\n",
      "Step: 2554, Loss: 0.091\n",
      "Step: 2555, Loss: 0.091\n",
      "Step: 2556, Loss: 0.091\n",
      "Step: 2557, Loss: 0.091\n",
      "Step: 2558, Loss: 0.091\n",
      "Step: 2559, Loss: 0.091\n",
      "Step: 2560, Loss: 0.091\n",
      "Step: 2561, Loss: 0.090\n",
      "Step: 2562, Loss: 0.091\n",
      "Step: 2563, Loss: 0.091\n",
      "Step: 2564, Loss: 0.091\n",
      "Step: 2565, Loss: 0.090\n",
      "Step: 2566, Loss: 0.091\n",
      "Step: 2567, Loss: 0.091\n",
      "Step: 2568, Loss: 0.091\n",
      "Step: 2569, Loss: 0.091\n",
      "Step: 2570, Loss: 0.091\n",
      "Step: 2571, Loss: 0.091\n",
      "Step: 2572, Loss: 0.090\n",
      "Step: 2573, Loss: 0.090\n",
      "Step: 2574, Loss: 0.091\n",
      "Step: 2575, Loss: 0.091\n",
      "Step: 2576, Loss: 0.090\n",
      "Step: 2577, Loss: 0.091\n",
      "Step: 2578, Loss: 0.091\n",
      "Step: 2579, Loss: 0.090\n",
      "Step: 2580, Loss: 0.091\n",
      "Step: 2581, Loss: 0.091\n",
      "Step: 2582, Loss: 0.091\n",
      "Step: 2583, Loss: 0.090\n",
      "Step: 2584, Loss: 0.090\n",
      "Step: 2585, Loss: 0.090\n",
      "Step: 2586, Loss: 0.091\n",
      "Step: 2587, Loss: 0.090\n",
      "Step: 2588, Loss: 0.091\n",
      "Step: 2589, Loss: 0.091\n",
      "Step: 2590, Loss: 0.090\n",
      "Step: 2591, Loss: 0.090\n",
      "Step: 2592, Loss: 0.091\n",
      "Step: 2593, Loss: 0.091\n",
      "Step: 2594, Loss: 0.090\n",
      "Step: 2595, Loss: 0.090\n",
      "Step: 2596, Loss: 0.090\n",
      "Step: 2597, Loss: 0.090\n",
      "Step: 2598, Loss: 0.090\n",
      "Step: 2599, Loss: 0.090\n",
      "Step: 2600, Loss: 0.091\n",
      "Step: 2601, Loss: 0.090\n",
      "Step: 2602, Loss: 0.090\n",
      "Step: 2603, Loss: 0.090\n",
      "Step: 2604, Loss: 0.090\n",
      "Step: 2605, Loss: 0.090\n",
      "Step: 2606, Loss: 0.090\n",
      "Step: 2607, Loss: 0.090\n",
      "Step: 2608, Loss: 0.089\n",
      "Step: 2609, Loss: 0.090\n",
      "Step: 2610, Loss: 0.090\n",
      "Step: 2611, Loss: 0.090\n",
      "Step: 2612, Loss: 0.089\n",
      "Step: 2613, Loss: 0.090\n",
      "Step: 2614, Loss: 0.090\n",
      "Step: 2615, Loss: 0.090\n",
      "Step: 2616, Loss: 0.090\n",
      "Step: 2617, Loss: 0.090\n",
      "Step: 2618, Loss: 0.090\n",
      "Step: 2619, Loss: 0.089\n",
      "Step: 2620, Loss: 0.090\n",
      "Step: 2621, Loss: 0.090\n",
      "Step: 2622, Loss: 0.090\n",
      "Step: 2623, Loss: 0.090\n",
      "Step: 2624, Loss: 0.090\n",
      "Step: 2625, Loss: 0.090\n",
      "Step: 2626, Loss: 0.089\n",
      "Step: 2627, Loss: 0.090\n",
      "Step: 2628, Loss: 0.090\n",
      "Step: 2629, Loss: 0.090\n",
      "Step: 2630, Loss: 0.089\n",
      "Step: 2631, Loss: 0.089\n",
      "Step: 2632, Loss: 0.090\n",
      "Step: 2633, Loss: 0.090\n",
      "Step: 2634, Loss: 0.089\n",
      "Step: 2635, Loss: 0.090\n",
      "Step: 2636, Loss: 0.090\n",
      "Step: 2637, Loss: 0.089\n",
      "Step: 2638, Loss: 0.089\n",
      "Step: 2639, Loss: 0.090\n",
      "Step: 2640, Loss: 0.090\n",
      "Step: 2641, Loss: 0.089\n",
      "Step: 2642, Loss: 0.089\n",
      "Step: 2643, Loss: 0.089\n",
      "Step: 2644, Loss: 0.089\n",
      "Step: 2645, Loss: 0.089\n",
      "Step: 2646, Loss: 0.089\n",
      "Step: 2647, Loss: 0.090\n",
      "Step: 2648, Loss: 0.089\n",
      "Step: 2649, Loss: 0.089\n",
      "Step: 2650, Loss: 0.089\n",
      "Step: 2651, Loss: 0.090\n",
      "Step: 2652, Loss: 0.089\n",
      "Step: 2653, Loss: 0.089\n",
      "Step: 2654, Loss: 0.089\n",
      "Step: 2655, Loss: 0.089\n",
      "Step: 2656, Loss: 0.089\n",
      "Step: 2657, Loss: 0.089\n",
      "Step: 2658, Loss: 0.090\n",
      "Step: 2659, Loss: 0.089\n",
      "Step: 2660, Loss: 0.089\n",
      "Step: 2661, Loss: 0.089\n",
      "Step: 2662, Loss: 0.089\n",
      "Step: 2663, Loss: 0.089\n",
      "Step: 2664, Loss: 0.089\n",
      "Step: 2665, Loss: 0.089\n",
      "Step: 2666, Loss: 0.089\n",
      "Step: 2667, Loss: 0.089\n",
      "Step: 2668, Loss: 0.089\n",
      "Step: 2669, Loss: 0.089\n",
      "Step: 2670, Loss: 0.089\n",
      "Step: 2671, Loss: 0.089\n",
      "Step: 2672, Loss: 0.089\n",
      "Step: 2673, Loss: 0.088\n",
      "Step: 2674, Loss: 0.089\n",
      "Step: 2675, Loss: 0.089\n",
      "Step: 2676, Loss: 0.089\n",
      "Step: 2677, Loss: 0.088\n",
      "Step: 2678, Loss: 0.089\n",
      "Step: 2679, Loss: 0.089\n",
      "Step: 2680, Loss: 0.089\n",
      "Step: 2681, Loss: 0.088\n",
      "Step: 2682, Loss: 0.089\n",
      "Step: 2683, Loss: 0.089\n",
      "Step: 2684, Loss: 0.088\n",
      "Step: 2685, Loss: 0.088\n",
      "Step: 2686, Loss: 0.089\n",
      "Step: 2687, Loss: 0.089\n",
      "Step: 2688, Loss: 0.088\n",
      "Step: 2689, Loss: 0.088\n",
      "Step: 2690, Loss: 0.089\n",
      "Step: 2691, Loss: 0.088\n",
      "Step: 2692, Loss: 0.088\n",
      "Step: 2693, Loss: 0.089\n",
      "Step: 2694, Loss: 0.089\n",
      "Step: 2695, Loss: 0.088\n",
      "Step: 2696, Loss: 0.088\n",
      "Step: 2697, Loss: 0.088\n",
      "Step: 2698, Loss: 0.089\n",
      "Step: 2699, Loss: 0.088\n",
      "Step: 2700, Loss: 0.088\n",
      "Step: 2701, Loss: 0.088\n",
      "Step: 2702, Loss: 0.088\n",
      "Step: 2703, Loss: 0.088\n",
      "Step: 2704, Loss: 0.089\n",
      "Step: 2705, Loss: 0.089\n",
      "Step: 2706, Loss: 0.088\n",
      "Step: 2707, Loss: 0.088\n",
      "Step: 2708, Loss: 0.088\n",
      "Step: 2709, Loss: 0.088\n",
      "Step: 2710, Loss: 0.088\n",
      "Step: 2711, Loss: 0.088\n",
      "Step: 2712, Loss: 0.089\n",
      "Step: 2713, Loss: 0.088\n",
      "Step: 2714, Loss: 0.088\n",
      "Step: 2715, Loss: 0.088\n",
      "Step: 2716, Loss: 0.088\n",
      "Step: 2717, Loss: 0.088\n",
      "Step: 2718, Loss: 0.088\n",
      "Step: 2719, Loss: 0.088\n",
      "Step: 2720, Loss: 0.087\n",
      "Step: 2721, Loss: 0.088\n",
      "Step: 2722, Loss: 0.088\n",
      "Step: 2723, Loss: 0.088\n",
      "Step: 2724, Loss: 0.088\n",
      "Step: 2725, Loss: 0.088\n",
      "Step: 2726, Loss: 0.088\n",
      "Step: 2727, Loss: 0.088\n",
      "Step: 2728, Loss: 0.088\n",
      "Step: 2729, Loss: 0.088\n",
      "Step: 2730, Loss: 0.088\n",
      "Step: 2731, Loss: 0.087\n",
      "Step: 2732, Loss: 0.088\n",
      "Step: 2733, Loss: 0.088\n",
      "Step: 2734, Loss: 0.088\n",
      "Step: 2735, Loss: 0.088\n",
      "Step: 2736, Loss: 0.088\n",
      "Step: 2737, Loss: 0.088\n",
      "Step: 2738, Loss: 0.087\n",
      "Step: 2739, Loss: 0.088\n",
      "Step: 2740, Loss: 0.088\n",
      "Step: 2741, Loss: 0.088\n",
      "Step: 2742, Loss: 0.087\n",
      "Step: 2743, Loss: 0.088\n",
      "Step: 2744, Loss: 0.088\n",
      "Step: 2745, Loss: 0.088\n",
      "Step: 2746, Loss: 0.087\n",
      "Step: 2747, Loss: 0.088\n",
      "Step: 2748, Loss: 0.088\n",
      "Step: 2749, Loss: 0.087\n",
      "Step: 2750, Loss: 0.087\n",
      "Step: 2751, Loss: 0.088\n",
      "Step: 2752, Loss: 0.088\n",
      "Step: 2753, Loss: 0.087\n",
      "Step: 2754, Loss: 0.088\n",
      "Step: 2755, Loss: 0.088\n",
      "Step: 2756, Loss: 0.087\n",
      "Step: 2757, Loss: 0.087\n",
      "Step: 2758, Loss: 0.087\n",
      "Step: 2759, Loss: 0.088\n",
      "Step: 2760, Loss: 0.087\n",
      "Step: 2761, Loss: 0.087\n",
      "Step: 2762, Loss: 0.087\n",
      "Step: 2763, Loss: 0.088\n",
      "Step: 2764, Loss: 0.087\n",
      "Step: 2765, Loss: 0.087\n",
      "Step: 2766, Loss: 0.087\n",
      "Step: 2767, Loss: 0.087\n",
      "Step: 2768, Loss: 0.087\n",
      "Step: 2769, Loss: 0.087\n",
      "Step: 2770, Loss: 0.088\n",
      "Step: 2771, Loss: 0.087\n",
      "Step: 2772, Loss: 0.087\n",
      "Step: 2773, Loss: 0.087\n",
      "Step: 2774, Loss: 0.087\n",
      "Step: 2775, Loss: 0.087\n",
      "Step: 2776, Loss: 0.087\n",
      "Step: 2777, Loss: 0.088\n",
      "Step: 2778, Loss: 0.087\n",
      "Step: 2779, Loss: 0.087\n",
      "Step: 2780, Loss: 0.087\n",
      "Step: 2781, Loss: 0.087\n",
      "Step: 2782, Loss: 0.087\n",
      "Step: 2783, Loss: 0.087\n",
      "Step: 2784, Loss: 0.087\n",
      "Step: 2785, Loss: 0.086\n",
      "Step: 2786, Loss: 0.087\n",
      "Step: 2787, Loss: 0.087\n",
      "Step: 2788, Loss: 0.087\n",
      "Step: 2789, Loss: 0.086\n",
      "Step: 2790, Loss: 0.087\n",
      "Step: 2791, Loss: 0.087\n",
      "Step: 2792, Loss: 0.087\n",
      "Step: 2793, Loss: 0.087\n",
      "Step: 2794, Loss: 0.087\n",
      "Step: 2795, Loss: 0.087\n",
      "Step: 2796, Loss: 0.086\n",
      "Step: 2797, Loss: 0.087\n",
      "Step: 2798, Loss: 0.087\n",
      "Step: 2799, Loss: 0.087\n",
      "Step: 2800, Loss: 0.087\n",
      "Step: 2801, Loss: 0.087\n",
      "Step: 2802, Loss: 0.087\n",
      "Step: 2803, Loss: 0.086\n",
      "Step: 2804, Loss: 0.087\n",
      "Step: 2805, Loss: 0.087\n",
      "Step: 2806, Loss: 0.087\n",
      "Step: 2807, Loss: 0.086\n",
      "Step: 2808, Loss: 0.086\n",
      "Step: 2809, Loss: 0.087\n",
      "Step: 2810, Loss: 0.087\n",
      "Step: 2811, Loss: 0.086\n",
      "Step: 2812, Loss: 0.087\n",
      "Step: 2813, Loss: 0.087\n",
      "Step: 2814, Loss: 0.086\n",
      "Step: 2815, Loss: 0.086\n",
      "Step: 2816, Loss: 0.087\n",
      "Step: 2817, Loss: 0.087\n",
      "Step: 2818, Loss: 0.086\n",
      "Step: 2819, Loss: 0.086\n",
      "Step: 2820, Loss: 0.086\n",
      "Step: 2821, Loss: 0.086\n",
      "Step: 2822, Loss: 0.086\n",
      "Step: 2823, Loss: 0.086\n",
      "Step: 2824, Loss: 0.087\n",
      "Step: 2825, Loss: 0.086\n",
      "Step: 2826, Loss: 0.086\n",
      "Step: 2827, Loss: 0.086\n",
      "Step: 2828, Loss: 0.087\n",
      "Step: 2829, Loss: 0.086\n",
      "Step: 2830, Loss: 0.086\n",
      "Step: 2831, Loss: 0.086\n",
      "Step: 2832, Loss: 0.086\n",
      "Step: 2833, Loss: 0.086\n",
      "Step: 2834, Loss: 0.086\n",
      "Step: 2835, Loss: 0.086\n",
      "Step: 2836, Loss: 0.086\n",
      "Step: 2837, Loss: 0.086\n",
      "Step: 2838, Loss: 0.086\n",
      "Step: 2839, Loss: 0.086\n",
      "Step: 2840, Loss: 0.086\n",
      "Step: 2841, Loss: 0.086\n",
      "Step: 2842, Loss: 0.086\n",
      "Step: 2843, Loss: 0.086\n",
      "Step: 2844, Loss: 0.086\n",
      "Step: 2845, Loss: 0.086\n",
      "Step: 2846, Loss: 0.086\n",
      "Step: 2847, Loss: 0.086\n",
      "Step: 2848, Loss: 0.086\n",
      "Step: 2849, Loss: 0.086\n",
      "Step: 2850, Loss: 0.085\n",
      "Step: 2851, Loss: 0.086\n",
      "Step: 2852, Loss: 0.086\n",
      "Step: 2853, Loss: 0.086\n",
      "Step: 2854, Loss: 0.085\n",
      "Step: 2855, Loss: 0.086\n",
      "Step: 2856, Loss: 0.086\n",
      "Step: 2857, Loss: 0.086\n",
      "Step: 2858, Loss: 0.086\n",
      "Step: 2859, Loss: 0.086\n",
      "Step: 2860, Loss: 0.086\n",
      "Step: 2861, Loss: 0.085\n",
      "Step: 2862, Loss: 0.085\n",
      "Step: 2863, Loss: 0.086\n",
      "Step: 2864, Loss: 0.086\n",
      "Step: 2865, Loss: 0.085\n",
      "Step: 2866, Loss: 0.086\n",
      "Step: 2867, Loss: 0.086\n",
      "Step: 2868, Loss: 0.085\n",
      "Step: 2869, Loss: 0.086\n",
      "Step: 2870, Loss: 0.086\n",
      "Step: 2871, Loss: 0.086\n",
      "Step: 2872, Loss: 0.085\n",
      "Step: 2873, Loss: 0.085\n",
      "Step: 2874, Loss: 0.085\n",
      "Step: 2875, Loss: 0.086\n",
      "Step: 2876, Loss: 0.085\n",
      "Step: 2877, Loss: 0.085\n",
      "Step: 2878, Loss: 0.086\n",
      "Step: 2879, Loss: 0.085\n",
      "Step: 2880, Loss: 0.085\n",
      "Step: 2881, Loss: 0.086\n",
      "Step: 2882, Loss: 0.086\n",
      "Step: 2883, Loss: 0.085\n",
      "Step: 2884, Loss: 0.085\n",
      "Step: 2885, Loss: 0.085\n",
      "Step: 2886, Loss: 0.085\n",
      "Step: 2887, Loss: 0.085\n",
      "Step: 2888, Loss: 0.085\n",
      "Step: 2889, Loss: 0.086\n",
      "Step: 2890, Loss: 0.085\n",
      "Step: 2891, Loss: 0.085\n",
      "Step: 2892, Loss: 0.085\n",
      "Step: 2893, Loss: 0.086\n",
      "Step: 2894, Loss: 0.085\n",
      "Step: 2895, Loss: 0.085\n",
      "Step: 2896, Loss: 0.085\n",
      "Step: 2897, Loss: 0.084\n",
      "Step: 2898, Loss: 0.085\n",
      "Step: 2899, Loss: 0.085\n",
      "Step: 2900, Loss: 0.085\n",
      "Step: 2901, Loss: 0.085\n",
      "Step: 2902, Loss: 0.085\n",
      "Step: 2903, Loss: 0.085\n",
      "Step: 2904, Loss: 0.085\n",
      "Step: 2905, Loss: 0.085\n",
      "Step: 2906, Loss: 0.085\n",
      "Step: 2907, Loss: 0.085\n",
      "Step: 2908, Loss: 0.085\n",
      "Step: 2909, Loss: 0.085\n",
      "Step: 2910, Loss: 0.085\n",
      "Step: 2911, Loss: 0.085\n",
      "Step: 2912, Loss: 0.085\n",
      "Step: 2913, Loss: 0.085\n",
      "Step: 2914, Loss: 0.085\n",
      "Step: 2915, Loss: 0.084\n",
      "Step: 2916, Loss: 0.085\n",
      "Step: 2917, Loss: 0.085\n",
      "Step: 2918, Loss: 0.085\n",
      "Step: 2919, Loss: 0.084\n",
      "Step: 2920, Loss: 0.085\n",
      "Step: 2921, Loss: 0.085\n",
      "Step: 2922, Loss: 0.085\n",
      "Step: 2923, Loss: 0.084\n",
      "Step: 2924, Loss: 0.085\n",
      "Step: 2925, Loss: 0.085\n",
      "Step: 2926, Loss: 0.084\n",
      "Step: 2927, Loss: 0.085\n",
      "Step: 2928, Loss: 0.085\n",
      "Step: 2929, Loss: 0.085\n",
      "Step: 2930, Loss: 0.084\n",
      "Step: 2931, Loss: 0.085\n",
      "Step: 2932, Loss: 0.085\n",
      "Step: 2933, Loss: 0.084\n",
      "Step: 2934, Loss: 0.085\n",
      "Step: 2935, Loss: 0.085\n",
      "Step: 2936, Loss: 0.085\n",
      "Step: 2937, Loss: 0.084\n",
      "Step: 2938, Loss: 0.084\n",
      "Step: 2939, Loss: 0.084\n",
      "Step: 2940, Loss: 0.085\n",
      "Step: 2941, Loss: 0.084\n",
      "Step: 2942, Loss: 0.084\n",
      "Step: 2943, Loss: 0.084\n",
      "Step: 2944, Loss: 0.084\n",
      "Step: 2945, Loss: 0.084\n",
      "Step: 2946, Loss: 0.084\n",
      "Step: 2947, Loss: 0.085\n",
      "Step: 2948, Loss: 0.084\n",
      "Step: 2949, Loss: 0.084\n",
      "Step: 2950, Loss: 0.084\n",
      "Step: 2951, Loss: 0.084\n",
      "Step: 2952, Loss: 0.084\n",
      "Step: 2953, Loss: 0.084\n",
      "Step: 2954, Loss: 0.085\n",
      "Step: 2955, Loss: 0.084\n",
      "Step: 2956, Loss: 0.084\n",
      "Step: 2957, Loss: 0.084\n",
      "Step: 2958, Loss: 0.084\n",
      "Step: 2959, Loss: 0.084\n",
      "Step: 2960, Loss: 0.084\n",
      "Step: 2961, Loss: 0.084\n",
      "Step: 2962, Loss: 0.083\n",
      "Step: 2963, Loss: 0.084\n",
      "Step: 2964, Loss: 0.084\n",
      "Step: 2965, Loss: 0.084\n",
      "Step: 2966, Loss: 0.084\n",
      "Step: 2967, Loss: 0.084\n",
      "Step: 2968, Loss: 0.084\n",
      "Step: 2969, Loss: 0.084\n",
      "Step: 2970, Loss: 0.084\n",
      "Step: 2971, Loss: 0.084\n",
      "Step: 2972, Loss: 0.084\n",
      "Step: 2973, Loss: 0.084\n",
      "Step: 2974, Loss: 0.084\n",
      "Step: 2975, Loss: 0.084\n",
      "Step: 2976, Loss: 0.084\n",
      "Step: 2977, Loss: 0.084\n",
      "Step: 2978, Loss: 0.084\n",
      "Step: 2979, Loss: 0.084\n",
      "Step: 2980, Loss: 0.083\n",
      "Step: 2981, Loss: 0.084\n",
      "Step: 2982, Loss: 0.084\n",
      "Step: 2983, Loss: 0.084\n",
      "Step: 2984, Loss: 0.083\n",
      "Step: 2985, Loss: 0.084\n",
      "Step: 2986, Loss: 0.084\n",
      "Step: 2987, Loss: 0.084\n",
      "Step: 2988, Loss: 0.083\n",
      "Step: 2989, Loss: 0.084\n",
      "Step: 2990, Loss: 0.084\n",
      "Step: 2991, Loss: 0.083\n",
      "Step: 2992, Loss: 0.083\n",
      "Step: 2993, Loss: 0.084\n",
      "Step: 2994, Loss: 0.084\n",
      "Step: 2995, Loss: 0.083\n",
      "Step: 2996, Loss: 0.084\n",
      "Step: 2997, Loss: 0.083\n",
      "Step: 2998, Loss: 0.083\n",
      "Step: 2999, Loss: 0.083\n",
      "Step: 3000, Loss: 0.084\n",
      "Step: 3001, Loss: 0.084\n",
      "Step: 3002, Loss: 0.083\n",
      "Step: 3003, Loss: 0.083\n",
      "Step: 3004, Loss: 0.083\n",
      "Step: 3005, Loss: 0.084\n",
      "Step: 3006, Loss: 0.083\n",
      "Step: 3007, Loss: 0.083\n",
      "Step: 3008, Loss: 0.084\n",
      "Step: 3009, Loss: 0.083\n",
      "Step: 3010, Loss: 0.083\n",
      "Step: 3011, Loss: 0.083\n",
      "Step: 3012, Loss: 0.084\n",
      "Step: 3013, Loss: 0.083\n",
      "Step: 3014, Loss: 0.083\n",
      "Step: 3015, Loss: 0.083\n",
      "Step: 3016, Loss: 0.083\n",
      "Step: 3017, Loss: 0.083\n",
      "Step: 3018, Loss: 0.084\n",
      "Step: 3019, Loss: 0.083\n",
      "Step: 3020, Loss: 0.083\n",
      "Step: 3021, Loss: 0.083\n",
      "Step: 3022, Loss: 0.083\n",
      "Step: 3023, Loss: 0.083\n",
      "Step: 3024, Loss: 0.083\n",
      "Step: 3025, Loss: 0.083\n",
      "Step: 3026, Loss: 0.083\n",
      "Step: 3027, Loss: 0.083\n",
      "Step: 3028, Loss: 0.083\n",
      "Step: 3029, Loss: 0.083\n",
      "Step: 3030, Loss: 0.083\n",
      "Step: 3031, Loss: 0.083\n",
      "Step: 3032, Loss: 0.083\n",
      "Step: 3033, Loss: 0.083\n",
      "Step: 3034, Loss: 0.083\n",
      "Step: 3035, Loss: 0.083\n",
      "Step: 3036, Loss: 0.083\n",
      "Step: 3037, Loss: 0.083\n",
      "Step: 3038, Loss: 0.082\n",
      "Step: 3039, Loss: 0.083\n",
      "Step: 3040, Loss: 0.083\n",
      "Step: 3041, Loss: 0.083\n",
      "Step: 3042, Loss: 0.083\n",
      "Step: 3043, Loss: 0.083\n",
      "Step: 3044, Loss: 0.083\n",
      "Step: 3045, Loss: 0.082\n",
      "Step: 3046, Loss: 0.083\n",
      "Step: 3047, Loss: 0.083\n",
      "Step: 3048, Loss: 0.083\n",
      "Step: 3049, Loss: 0.082\n",
      "Step: 3050, Loss: 0.082\n",
      "Step: 3051, Loss: 0.083\n",
      "Step: 3052, Loss: 0.083\n",
      "Step: 3053, Loss: 0.083\n",
      "Step: 3054, Loss: 0.083\n",
      "Step: 3055, Loss: 0.083\n",
      "Step: 3056, Loss: 0.082\n",
      "Step: 3057, Loss: 0.083\n",
      "Step: 3058, Loss: 0.083\n",
      "Step: 3059, Loss: 0.083\n",
      "Step: 3060, Loss: 0.082\n",
      "Step: 3061, Loss: 0.083\n",
      "Step: 3062, Loss: 0.083\n",
      "Step: 3063, Loss: 0.082\n",
      "Step: 3064, Loss: 0.082\n",
      "Step: 3065, Loss: 0.083\n",
      "Step: 3066, Loss: 0.083\n",
      "Step: 3067, Loss: 0.082\n",
      "Step: 3068, Loss: 0.082\n",
      "Step: 3069, Loss: 0.083\n",
      "Step: 3070, Loss: 0.083\n",
      "Step: 3071, Loss: 0.082\n",
      "Step: 3072, Loss: 0.082\n",
      "Step: 3073, Loss: 0.083\n",
      "Step: 3074, Loss: 0.082\n",
      "Step: 3075, Loss: 0.082\n",
      "Step: 3076, Loss: 0.082\n",
      "Step: 3077, Loss: 0.083\n",
      "Step: 3078, Loss: 0.082\n",
      "Step: 3079, Loss: 0.082\n",
      "Step: 3080, Loss: 0.082\n",
      "Step: 3081, Loss: 0.082\n",
      "Step: 3082, Loss: 0.082\n",
      "Step: 3083, Loss: 0.082\n",
      "Step: 3084, Loss: 0.082\n",
      "Step: 3085, Loss: 0.082\n",
      "Step: 3086, Loss: 0.082\n",
      "Step: 3087, Loss: 0.082\n",
      "Step: 3088, Loss: 0.082\n",
      "Step: 3089, Loss: 0.082\n",
      "Step: 3090, Loss: 0.082\n",
      "Step: 3091, Loss: 0.082\n",
      "Step: 3092, Loss: 0.082\n",
      "Step: 3093, Loss: 0.082\n",
      "Step: 3094, Loss: 0.082\n",
      "Step: 3095, Loss: 0.083\n",
      "Step: 3096, Loss: 0.082\n",
      "Step: 3097, Loss: 0.082\n",
      "Step: 3098, Loss: 0.082\n",
      "Step: 3099, Loss: 0.082\n",
      "Step: 3100, Loss: 0.082\n",
      "Step: 3101, Loss: 0.082\n",
      "Step: 3102, Loss: 0.082\n",
      "Step: 3103, Loss: 0.082\n",
      "Step: 3104, Loss: 0.082\n",
      "Step: 3105, Loss: 0.082\n",
      "Step: 3106, Loss: 0.082\n",
      "Step: 3107, Loss: 0.081\n",
      "Step: 3108, Loss: 0.082\n",
      "Step: 3109, Loss: 0.082\n",
      "Step: 3110, Loss: 0.082\n",
      "Step: 3111, Loss: 0.082\n",
      "Step: 3112, Loss: 0.082\n",
      "Step: 3113, Loss: 0.082\n",
      "Step: 3114, Loss: 0.081\n",
      "Step: 3115, Loss: 0.082\n",
      "Step: 3116, Loss: 0.082\n",
      "Step: 3117, Loss: 0.082\n",
      "Step: 3118, Loss: 0.082\n",
      "Step: 3119, Loss: 0.082\n",
      "Step: 3120, Loss: 0.082\n",
      "Step: 3121, Loss: 0.081\n",
      "Step: 3122, Loss: 0.082\n",
      "Step: 3123, Loss: 0.082\n",
      "Step: 3124, Loss: 0.082\n",
      "Step: 3125, Loss: 0.081\n",
      "Step: 3126, Loss: 0.082\n",
      "Step: 3127, Loss: 0.082\n",
      "Step: 3128, Loss: 0.082\n",
      "Step: 3129, Loss: 0.081\n",
      "Step: 3130, Loss: 0.082\n",
      "Step: 3131, Loss: 0.082\n",
      "Step: 3132, Loss: 0.081\n",
      "Step: 3133, Loss: 0.081\n",
      "Step: 3134, Loss: 0.082\n",
      "Step: 3135, Loss: 0.082\n",
      "Step: 3136, Loss: 0.081\n",
      "Step: 3137, Loss: 0.081\n",
      "Step: 3138, Loss: 0.082\n",
      "Step: 3139, Loss: 0.081\n",
      "Step: 3140, Loss: 0.081\n",
      "Step: 3141, Loss: 0.082\n",
      "Step: 3142, Loss: 0.082\n",
      "Step: 3143, Loss: 0.081\n",
      "Step: 3144, Loss: 0.081\n",
      "Step: 3145, Loss: 0.081\n",
      "Step: 3146, Loss: 0.081\n",
      "Step: 3147, Loss: 0.081\n",
      "Step: 3148, Loss: 0.081\n",
      "Step: 3149, Loss: 0.082\n",
      "Step: 3150, Loss: 0.081\n",
      "Step: 3151, Loss: 0.081\n",
      "Step: 3152, Loss: 0.081\n",
      "Step: 3153, Loss: 0.081\n",
      "Step: 3154, Loss: 0.081\n",
      "Step: 3155, Loss: 0.081\n",
      "Step: 3156, Loss: 0.081\n",
      "Step: 3157, Loss: 0.081\n",
      "Step: 3158, Loss: 0.081\n",
      "Step: 3159, Loss: 0.081\n",
      "Step: 3160, Loss: 0.082\n",
      "Step: 3161, Loss: 0.081\n",
      "Step: 3162, Loss: 0.081\n",
      "Step: 3163, Loss: 0.081\n",
      "Step: 3164, Loss: 0.081\n",
      "Step: 3165, Loss: 0.081\n",
      "Step: 3166, Loss: 0.081\n",
      "Step: 3167, Loss: 0.081\n",
      "Step: 3168, Loss: 0.080\n",
      "Step: 3169, Loss: 0.081\n",
      "Step: 3170, Loss: 0.081\n",
      "Step: 3171, Loss: 0.081\n",
      "Step: 3172, Loss: 0.080\n",
      "Step: 3173, Loss: 0.081\n",
      "Step: 3174, Loss: 0.081\n",
      "Step: 3175, Loss: 0.081\n",
      "Step: 3176, Loss: 0.081\n",
      "Step: 3177, Loss: 0.081\n",
      "Step: 3178, Loss: 0.081\n",
      "Step: 3179, Loss: 0.080\n",
      "Step: 3180, Loss: 0.081\n",
      "Step: 3181, Loss: 0.081\n",
      "Step: 3182, Loss: 0.081\n",
      "Step: 3183, Loss: 0.081\n",
      "Step: 3184, Loss: 0.081\n",
      "Step: 3185, Loss: 0.081\n",
      "Step: 3186, Loss: 0.080\n",
      "Step: 3187, Loss: 0.081\n",
      "Step: 3188, Loss: 0.081\n",
      "Step: 3189, Loss: 0.081\n",
      "Step: 3190, Loss: 0.080\n",
      "Step: 3191, Loss: 0.081\n",
      "Step: 3192, Loss: 0.081\n",
      "Step: 3193, Loss: 0.081\n",
      "Step: 3194, Loss: 0.080\n",
      "Step: 3195, Loss: 0.081\n",
      "Step: 3196, Loss: 0.081\n",
      "Step: 3197, Loss: 0.080\n",
      "Step: 3198, Loss: 0.080\n",
      "Step: 3199, Loss: 0.081\n",
      "Step: 3200, Loss: 0.081\n",
      "Step: 3201, Loss: 0.080\n",
      "Step: 3202, Loss: 0.081\n",
      "Step: 3203, Loss: 0.081\n",
      "Step: 3204, Loss: 0.080\n",
      "Step: 3205, Loss: 0.080\n",
      "Step: 3206, Loss: 0.081\n",
      "Step: 3207, Loss: 0.081\n",
      "Step: 3208, Loss: 0.080\n",
      "Step: 3209, Loss: 0.080\n",
      "Step: 3210, Loss: 0.080\n",
      "Step: 3211, Loss: 0.081\n",
      "Step: 3212, Loss: 0.080\n",
      "Step: 3213, Loss: 0.081\n",
      "Step: 3214, Loss: 0.081\n",
      "Step: 3215, Loss: 0.080\n",
      "Step: 3216, Loss: 0.080\n",
      "Step: 3217, Loss: 0.081\n",
      "Step: 3218, Loss: 0.081\n",
      "Step: 3219, Loss: 0.080\n",
      "Step: 3220, Loss: 0.080\n",
      "Step: 3221, Loss: 0.080\n",
      "Step: 3222, Loss: 0.080\n",
      "Step: 3223, Loss: 0.080\n",
      "Step: 3224, Loss: 0.080\n",
      "Step: 3225, Loss: 0.081\n",
      "Step: 3226, Loss: 0.080\n",
      "Step: 3227, Loss: 0.080\n",
      "Step: 3228, Loss: 0.080\n",
      "Step: 3229, Loss: 0.081\n",
      "Step: 3230, Loss: 0.080\n",
      "Step: 3231, Loss: 0.080\n",
      "Step: 3232, Loss: 0.080\n",
      "Step: 3233, Loss: 0.079\n",
      "Step: 3234, Loss: 0.080\n",
      "Step: 3235, Loss: 0.080\n",
      "Step: 3236, Loss: 0.080\n",
      "Step: 3237, Loss: 0.080\n",
      "Step: 3238, Loss: 0.080\n",
      "Step: 3239, Loss: 0.080\n",
      "Step: 3240, Loss: 0.080\n",
      "Step: 3241, Loss: 0.080\n",
      "Step: 3242, Loss: 0.080\n",
      "Step: 3243, Loss: 0.080\n",
      "Step: 3244, Loss: 0.080\n",
      "Step: 3245, Loss: 0.080\n",
      "Step: 3246, Loss: 0.080\n",
      "Step: 3247, Loss: 0.080\n",
      "Step: 3248, Loss: 0.080\n",
      "Step: 3249, Loss: 0.080\n",
      "Step: 3250, Loss: 0.080\n",
      "Step: 3251, Loss: 0.079\n",
      "Step: 3252, Loss: 0.080\n",
      "Step: 3253, Loss: 0.080\n",
      "Step: 3254, Loss: 0.080\n",
      "Step: 3255, Loss: 0.079\n",
      "Step: 3256, Loss: 0.080\n",
      "Step: 3257, Loss: 0.080\n",
      "Step: 3258, Loss: 0.080\n",
      "Step: 3259, Loss: 0.079\n",
      "Step: 3260, Loss: 0.080\n",
      "Step: 3261, Loss: 0.080\n",
      "Step: 3262, Loss: 0.079\n",
      "Step: 3263, Loss: 0.080\n",
      "Step: 3264, Loss: 0.080\n",
      "Step: 3265, Loss: 0.080\n",
      "Step: 3266, Loss: 0.079\n",
      "Step: 3267, Loss: 0.080\n",
      "Step: 3268, Loss: 0.080\n",
      "Step: 3269, Loss: 0.079\n",
      "Step: 3270, Loss: 0.080\n",
      "Step: 3271, Loss: 0.080\n",
      "Step: 3272, Loss: 0.080\n",
      "Step: 3273, Loss: 0.079\n",
      "Step: 3274, Loss: 0.079\n",
      "Step: 3275, Loss: 0.080\n",
      "Step: 3276, Loss: 0.080\n",
      "Step: 3277, Loss: 0.079\n",
      "Step: 3278, Loss: 0.080\n",
      "Step: 3279, Loss: 0.080\n",
      "Step: 3280, Loss: 0.079\n",
      "Step: 3281, Loss: 0.079\n",
      "Step: 3282, Loss: 0.080\n",
      "Step: 3283, Loss: 0.080\n",
      "Step: 3284, Loss: 0.079\n",
      "Step: 3285, Loss: 0.080\n",
      "Step: 3286, Loss: 0.079\n",
      "Step: 3287, Loss: 0.079\n",
      "Step: 3288, Loss: 0.079\n",
      "Step: 3289, Loss: 0.079\n",
      "Step: 3290, Loss: 0.080\n",
      "Step: 3291, Loss: 0.079\n",
      "Step: 3292, Loss: 0.079\n",
      "Step: 3293, Loss: 0.079\n",
      "Step: 3294, Loss: 0.080\n",
      "Step: 3295, Loss: 0.079\n",
      "Step: 3296, Loss: 0.079\n",
      "Step: 3297, Loss: 0.080\n",
      "Step: 3298, Loss: 0.079\n",
      "Step: 3299, Loss: 0.079\n",
      "Step: 3300, Loss: 0.079\n",
      "Step: 3301, Loss: 0.080\n",
      "Step: 3302, Loss: 0.079\n",
      "Step: 3303, Loss: 0.079\n",
      "Step: 3304, Loss: 0.079\n",
      "Step: 3305, Loss: 0.079\n",
      "Step: 3306, Loss: 0.079\n",
      "Step: 3307, Loss: 0.079\n",
      "Step: 3308, Loss: 0.079\n",
      "Step: 3309, Loss: 0.079\n",
      "Step: 3310, Loss: 0.079\n",
      "Step: 3311, Loss: 0.079\n",
      "Step: 3312, Loss: 0.079\n",
      "Step: 3313, Loss: 0.079\n",
      "Step: 3314, Loss: 0.079\n",
      "Step: 3315, Loss: 0.079\n",
      "Step: 3316, Loss: 0.079\n",
      "Step: 3317, Loss: 0.079\n",
      "Step: 3318, Loss: 0.079\n",
      "Step: 3319, Loss: 0.079\n",
      "Step: 3320, Loss: 0.079\n",
      "Step: 3321, Loss: 0.079\n",
      "Step: 3322, Loss: 0.079\n",
      "Step: 3323, Loss: 0.079\n",
      "Step: 3324, Loss: 0.079\n",
      "Step: 3325, Loss: 0.079\n",
      "Step: 3326, Loss: 0.079\n",
      "Step: 3327, Loss: 0.078\n",
      "Step: 3328, Loss: 0.079\n",
      "Step: 3329, Loss: 0.079\n",
      "Step: 3330, Loss: 0.079\n",
      "Step: 3331, Loss: 0.078\n",
      "Step: 3332, Loss: 0.079\n",
      "Step: 3333, Loss: 0.079\n",
      "Step: 3334, Loss: 0.079\n",
      "Step: 3335, Loss: 0.079\n",
      "Step: 3336, Loss: 0.079\n",
      "Step: 3337, Loss: 0.079\n",
      "Step: 3338, Loss: 0.078\n",
      "Step: 3339, Loss: 0.078\n",
      "Step: 3340, Loss: 0.079\n",
      "Step: 3341, Loss: 0.079\n",
      "Step: 3342, Loss: 0.078\n",
      "Step: 3343, Loss: 0.079\n",
      "Step: 3344, Loss: 0.079\n",
      "Step: 3345, Loss: 0.078\n",
      "Step: 3346, Loss: 0.079\n",
      "Step: 3347, Loss: 0.079\n",
      "Step: 3348, Loss: 0.079\n",
      "Step: 3349, Loss: 0.078\n",
      "Step: 3350, Loss: 0.079\n",
      "Step: 3351, Loss: 0.079\n",
      "Step: 3352, Loss: 0.078\n",
      "Step: 3353, Loss: 0.078\n",
      "Step: 3354, Loss: 0.079\n",
      "Step: 3355, Loss: 0.079\n",
      "Step: 3356, Loss: 0.078\n",
      "Step: 3357, Loss: 0.078\n",
      "Step: 3358, Loss: 0.079\n",
      "Step: 3359, Loss: 0.079\n",
      "Step: 3360, Loss: 0.078\n",
      "Step: 3361, Loss: 0.079\n",
      "Step: 3362, Loss: 0.079\n",
      "Step: 3363, Loss: 0.078\n",
      "Step: 3364, Loss: 0.078\n",
      "Step: 3365, Loss: 0.079\n",
      "Step: 3366, Loss: 0.079\n",
      "Step: 3367, Loss: 0.078\n",
      "Step: 3368, Loss: 0.078\n",
      "Step: 3369, Loss: 0.078\n",
      "Step: 3370, Loss: 0.078\n",
      "Step: 3371, Loss: 0.078\n",
      "Step: 3372, Loss: 0.079\n",
      "Step: 3373, Loss: 0.078\n",
      "Step: 3374, Loss: 0.078\n",
      "Step: 3375, Loss: 0.078\n",
      "Step: 3376, Loss: 0.078\n",
      "Step: 3377, Loss: 0.078\n",
      "Step: 3378, Loss: 0.078\n",
      "Step: 3379, Loss: 0.078\n",
      "Step: 3380, Loss: 0.078\n",
      "Step: 3381, Loss: 0.078\n",
      "Step: 3382, Loss: 0.078\n",
      "Step: 3383, Loss: 0.078\n",
      "Step: 3384, Loss: 0.079\n",
      "Step: 3385, Loss: 0.078\n",
      "Step: 3386, Loss: 0.078\n",
      "Step: 3387, Loss: 0.078\n",
      "Step: 3388, Loss: 0.078\n",
      "Step: 3389, Loss: 0.078\n",
      "Step: 3390, Loss: 0.078\n",
      "Step: 3391, Loss: 0.078\n",
      "Step: 3392, Loss: 0.078\n",
      "Step: 3393, Loss: 0.078\n",
      "Step: 3394, Loss: 0.078\n",
      "Step: 3395, Loss: 0.078\n",
      "Step: 3396, Loss: 0.077\n",
      "Step: 3397, Loss: 0.078\n",
      "Step: 3398, Loss: 0.078\n",
      "Step: 3399, Loss: 0.078\n",
      "Step: 3400, Loss: 0.078\n",
      "Step: 3401, Loss: 0.078\n",
      "Step: 3402, Loss: 0.078\n",
      "Step: 3403, Loss: 0.078\n",
      "Step: 3404, Loss: 0.078\n",
      "Step: 3405, Loss: 0.078\n",
      "Step: 3406, Loss: 0.078\n",
      "Step: 3407, Loss: 0.078\n",
      "Step: 3408, Loss: 0.078\n",
      "Step: 3409, Loss: 0.078\n",
      "Step: 3410, Loss: 0.077\n",
      "Step: 3411, Loss: 0.078\n",
      "Step: 3412, Loss: 0.078\n",
      "Step: 3413, Loss: 0.078\n",
      "Step: 3414, Loss: 0.077\n",
      "Step: 3415, Loss: 0.078\n",
      "Step: 3416, Loss: 0.078\n",
      "Step: 3417, Loss: 0.078\n",
      "Step: 3418, Loss: 0.077\n",
      "Step: 3419, Loss: 0.078\n",
      "Step: 3420, Loss: 0.078\n",
      "Step: 3421, Loss: 0.077\n",
      "Step: 3422, Loss: 0.078\n",
      "Step: 3423, Loss: 0.078\n",
      "Step: 3424, Loss: 0.078\n",
      "Step: 3425, Loss: 0.077\n",
      "Step: 3426, Loss: 0.078\n",
      "Step: 3427, Loss: 0.078\n",
      "Step: 3428, Loss: 0.077\n",
      "Step: 3429, Loss: 0.078\n",
      "Step: 3430, Loss: 0.078\n",
      "Step: 3431, Loss: 0.078\n",
      "Step: 3432, Loss: 0.077\n",
      "Step: 3433, Loss: 0.077\n",
      "Step: 3434, Loss: 0.078\n",
      "Step: 3435, Loss: 0.078\n",
      "Step: 3436, Loss: 0.077\n",
      "Step: 3437, Loss: 0.078\n",
      "Step: 3438, Loss: 0.078\n",
      "Step: 3439, Loss: 0.077\n",
      "Step: 3440, Loss: 0.077\n",
      "Step: 3441, Loss: 0.078\n",
      "Step: 3442, Loss: 0.078\n",
      "Step: 3443, Loss: 0.077\n",
      "Step: 3444, Loss: 0.078\n",
      "Step: 3445, Loss: 0.077\n",
      "Step: 3446, Loss: 0.077\n",
      "Step: 3447, Loss: 0.077\n",
      "Step: 3448, Loss: 0.077\n",
      "Step: 3449, Loss: 0.078\n",
      "Step: 3450, Loss: 0.077\n",
      "Step: 3451, Loss: 0.077\n",
      "Step: 3452, Loss: 0.077\n",
      "Step: 3453, Loss: 0.078\n",
      "Step: 3454, Loss: 0.077\n",
      "Step: 3455, Loss: 0.077\n",
      "Step: 3456, Loss: 0.077\n",
      "Step: 3457, Loss: 0.077\n",
      "Step: 3458, Loss: 0.077\n",
      "Step: 3459, Loss: 0.077\n",
      "Step: 3460, Loss: 0.078\n",
      "Step: 3461, Loss: 0.077\n",
      "Step: 3462, Loss: 0.077\n",
      "Step: 3463, Loss: 0.077\n",
      "Step: 3464, Loss: 0.077\n",
      "Step: 3465, Loss: 0.077\n",
      "Step: 3466, Loss: 0.077\n",
      "Step: 3467, Loss: 0.077\n",
      "Step: 3468, Loss: 0.077\n",
      "Step: 3469, Loss: 0.077\n",
      "Step: 3470, Loss: 0.077\n",
      "Step: 3471, Loss: 0.077\n",
      "Step: 3472, Loss: 0.077\n",
      "Step: 3473, Loss: 0.077\n",
      "Step: 3474, Loss: 0.077\n",
      "Step: 3475, Loss: 0.076\n",
      "Step: 3476, Loss: 0.077\n",
      "Step: 3477, Loss: 0.077\n",
      "Step: 3478, Loss: 0.077\n",
      "Step: 3479, Loss: 0.077\n",
      "Step: 3480, Loss: 0.077\n",
      "Step: 3481, Loss: 0.077\n",
      "Step: 3482, Loss: 0.077\n",
      "Step: 3483, Loss: 0.077\n",
      "Step: 3484, Loss: 0.077\n",
      "Step: 3485, Loss: 0.077\n",
      "Step: 3486, Loss: 0.077\n",
      "Step: 3487, Loss: 0.077\n",
      "Step: 3488, Loss: 0.077\n",
      "Step: 3489, Loss: 0.077\n",
      "Step: 3490, Loss: 0.077\n",
      "Step: 3491, Loss: 0.077\n",
      "Step: 3492, Loss: 0.077\n",
      "Step: 3493, Loss: 0.076\n",
      "Step: 3494, Loss: 0.077\n",
      "Step: 3495, Loss: 0.077\n",
      "Step: 3496, Loss: 0.077\n",
      "Step: 3497, Loss: 0.076\n",
      "Step: 3498, Loss: 0.077\n",
      "Step: 3499, Loss: 0.077\n",
      "Step: 3500, Loss: 0.077\n",
      "Step: 3501, Loss: 0.076\n",
      "Step: 3502, Loss: 0.077\n",
      "Step: 3503, Loss: 0.077\n",
      "Step: 3504, Loss: 0.076\n",
      "Step: 3505, Loss: 0.077\n",
      "Step: 3506, Loss: 0.077\n",
      "Step: 3507, Loss: 0.077\n",
      "Step: 3508, Loss: 0.076\n",
      "Step: 3509, Loss: 0.077\n",
      "Step: 3510, Loss: 0.076\n",
      "Step: 3511, Loss: 0.076\n",
      "Step: 3512, Loss: 0.076\n",
      "Step: 3513, Loss: 0.077\n",
      "Step: 3514, Loss: 0.077\n",
      "Step: 3515, Loss: 0.076\n",
      "Step: 3516, Loss: 0.076\n",
      "Step: 3517, Loss: 0.076\n",
      "Step: 3518, Loss: 0.077\n",
      "Step: 3519, Loss: 0.076\n",
      "Step: 3520, Loss: 0.077\n",
      "Step: 3521, Loss: 0.077\n",
      "Step: 3522, Loss: 0.076\n",
      "Step: 3523, Loss: 0.076\n",
      "Step: 3524, Loss: 0.076\n",
      "Step: 3525, Loss: 0.077\n",
      "Step: 3526, Loss: 0.076\n",
      "Step: 3527, Loss: 0.076\n",
      "Step: 3528, Loss: 0.076\n",
      "Step: 3529, Loss: 0.076\n",
      "Step: 3530, Loss: 0.076\n",
      "Step: 3531, Loss: 0.077\n",
      "Step: 3532, Loss: 0.077\n",
      "Step: 3533, Loss: 0.076\n",
      "Step: 3534, Loss: 0.076\n",
      "Step: 3535, Loss: 0.076\n",
      "Step: 3536, Loss: 0.077\n",
      "Step: 3537, Loss: 0.076\n",
      "Step: 3538, Loss: 0.076\n",
      "Step: 3539, Loss: 0.076\n",
      "Step: 3540, Loss: 0.076\n",
      "Step: 3541, Loss: 0.076\n",
      "Step: 3542, Loss: 0.076\n",
      "Step: 3543, Loss: 0.077\n",
      "Step: 3544, Loss: 0.076\n",
      "Step: 3545, Loss: 0.076\n",
      "Step: 3546, Loss: 0.076\n",
      "Step: 3547, Loss: 0.076\n",
      "Step: 3548, Loss: 0.076\n",
      "Step: 3549, Loss: 0.076\n",
      "Step: 3550, Loss: 0.076\n",
      "Step: 3551, Loss: 0.076\n",
      "Step: 3552, Loss: 0.076\n",
      "Step: 3553, Loss: 0.076\n",
      "Step: 3554, Loss: 0.076\n",
      "Step: 3555, Loss: 0.076\n",
      "Step: 3556, Loss: 0.076\n",
      "Step: 3557, Loss: 0.076\n",
      "Step: 3558, Loss: 0.076\n",
      "Step: 3559, Loss: 0.076\n",
      "Step: 3560, Loss: 0.076\n",
      "Step: 3561, Loss: 0.076\n",
      "Step: 3562, Loss: 0.076\n",
      "Step: 3563, Loss: 0.076\n",
      "Step: 3564, Loss: 0.076\n",
      "Step: 3565, Loss: 0.076\n",
      "Step: 3566, Loss: 0.076\n",
      "Step: 3567, Loss: 0.076\n",
      "Step: 3568, Loss: 0.076\n",
      "Step: 3569, Loss: 0.075\n",
      "Step: 3570, Loss: 0.076\n",
      "Step: 3571, Loss: 0.076\n",
      "Step: 3572, Loss: 0.076\n",
      "Step: 3573, Loss: 0.075\n",
      "Step: 3574, Loss: 0.076\n",
      "Step: 3575, Loss: 0.076\n",
      "Step: 3576, Loss: 0.076\n",
      "Step: 3577, Loss: 0.076\n",
      "Step: 3578, Loss: 0.076\n",
      "Step: 3579, Loss: 0.076\n",
      "Step: 3580, Loss: 0.075\n",
      "Step: 3581, Loss: 0.076\n",
      "Step: 3582, Loss: 0.076\n",
      "Step: 3583, Loss: 0.076\n",
      "Step: 3584, Loss: 0.075\n",
      "Step: 3585, Loss: 0.076\n",
      "Step: 3586, Loss: 0.076\n",
      "Step: 3587, Loss: 0.075\n",
      "Step: 3588, Loss: 0.076\n",
      "Step: 3589, Loss: 0.076\n",
      "Step: 3590, Loss: 0.076\n",
      "Step: 3591, Loss: 0.075\n",
      "Step: 3592, Loss: 0.076\n",
      "Step: 3593, Loss: 0.076\n",
      "Step: 3594, Loss: 0.075\n",
      "Step: 3595, Loss: 0.075\n",
      "Step: 3596, Loss: 0.076\n",
      "Step: 3597, Loss: 0.076\n",
      "Step: 3598, Loss: 0.075\n",
      "Step: 3599, Loss: 0.075\n",
      "Step: 3600, Loss: 0.076\n",
      "Step: 3601, Loss: 0.076\n",
      "Step: 3602, Loss: 0.075\n",
      "Step: 3603, Loss: 0.076\n",
      "Step: 3604, Loss: 0.076\n",
      "Step: 3605, Loss: 0.075\n",
      "Step: 3606, Loss: 0.075\n",
      "Step: 3607, Loss: 0.076\n",
      "Step: 3608, Loss: 0.076\n",
      "Step: 3609, Loss: 0.075\n",
      "Step: 3610, Loss: 0.075\n",
      "Step: 3611, Loss: 0.075\n",
      "Step: 3612, Loss: 0.076\n",
      "Step: 3613, Loss: 0.075\n",
      "Step: 3614, Loss: 0.075\n",
      "Step: 3615, Loss: 0.075\n",
      "Step: 3616, Loss: 0.075\n",
      "Step: 3617, Loss: 0.075\n",
      "Step: 3618, Loss: 0.075\n",
      "Step: 3619, Loss: 0.076\n",
      "Step: 3620, Loss: 0.075\n",
      "Step: 3621, Loss: 0.075\n",
      "Step: 3622, Loss: 0.075\n",
      "Step: 3623, Loss: 0.075\n",
      "Step: 3624, Loss: 0.075\n",
      "Step: 3625, Loss: 0.075\n",
      "Step: 3626, Loss: 0.076\n",
      "Step: 3627, Loss: 0.075\n",
      "Step: 3628, Loss: 0.075\n",
      "Step: 3629, Loss: 0.075\n",
      "Step: 3630, Loss: 0.076\n",
      "Step: 3631, Loss: 0.075\n",
      "Step: 3632, Loss: 0.075\n",
      "Step: 3633, Loss: 0.075\n",
      "Step: 3634, Loss: 0.074\n",
      "Step: 3635, Loss: 0.075\n",
      "Step: 3636, Loss: 0.075\n",
      "Step: 3637, Loss: 0.075\n",
      "Step: 3638, Loss: 0.074\n",
      "Step: 3639, Loss: 0.075\n",
      "Step: 3640, Loss: 0.075\n",
      "Step: 3641, Loss: 0.075\n",
      "Step: 3642, Loss: 0.075\n",
      "Step: 3643, Loss: 0.075\n",
      "Step: 3644, Loss: 0.075\n",
      "Step: 3645, Loss: 0.074\n",
      "Step: 3646, Loss: 0.075\n",
      "Step: 3647, Loss: 0.075\n",
      "Step: 3648, Loss: 0.075\n",
      "Step: 3649, Loss: 0.075\n",
      "Step: 3650, Loss: 0.075\n",
      "Step: 3651, Loss: 0.075\n",
      "Step: 3652, Loss: 0.074\n",
      "Step: 3653, Loss: 0.075\n",
      "Step: 3654, Loss: 0.075\n",
      "Step: 3655, Loss: 0.075\n",
      "Step: 3656, Loss: 0.074\n",
      "Step: 3657, Loss: 0.075\n",
      "Step: 3658, Loss: 0.075\n",
      "Step: 3659, Loss: 0.075\n",
      "Step: 3660, Loss: 0.074\n",
      "Step: 3661, Loss: 0.075\n",
      "Step: 3662, Loss: 0.075\n",
      "Step: 3663, Loss: 0.074\n",
      "Step: 3664, Loss: 0.075\n",
      "Step: 3665, Loss: 0.075\n",
      "Step: 3666, Loss: 0.075\n",
      "Step: 3667, Loss: 0.074\n",
      "Step: 3668, Loss: 0.075\n",
      "Step: 3669, Loss: 0.075\n",
      "Step: 3670, Loss: 0.074\n",
      "Step: 3671, Loss: 0.075\n",
      "Step: 3672, Loss: 0.075\n",
      "Step: 3673, Loss: 0.075\n",
      "Step: 3674, Loss: 0.074\n",
      "Step: 3675, Loss: 0.074\n",
      "Step: 3676, Loss: 0.075\n",
      "Step: 3677, Loss: 0.075\n",
      "Step: 3678, Loss: 0.074\n",
      "Step: 3679, Loss: 0.075\n",
      "Step: 3680, Loss: 0.075\n",
      "Step: 3681, Loss: 0.074\n",
      "Step: 3682, Loss: 0.074\n",
      "Step: 3683, Loss: 0.075\n",
      "Step: 3684, Loss: 0.075\n",
      "Step: 3685, Loss: 0.074\n",
      "Step: 3686, Loss: 0.075\n",
      "Step: 3687, Loss: 0.074\n",
      "Step: 3688, Loss: 0.074\n",
      "Step: 3689, Loss: 0.074\n",
      "Step: 3690, Loss: 0.075\n",
      "Step: 3691, Loss: 0.075\n",
      "Step: 3692, Loss: 0.074\n",
      "Step: 3693, Loss: 0.074\n",
      "Step: 3694, Loss: 0.074\n",
      "Step: 3695, Loss: 0.075\n",
      "Step: 3696, Loss: 0.074\n",
      "Step: 3697, Loss: 0.074\n",
      "Step: 3698, Loss: 0.075\n",
      "Step: 3699, Loss: 0.074\n",
      "Step: 3700, Loss: 0.074\n",
      "Step: 3701, Loss: 0.074\n",
      "Step: 3702, Loss: 0.075\n",
      "Step: 3703, Loss: 0.074\n",
      "Step: 3704, Loss: 0.074\n",
      "Step: 3705, Loss: 0.074\n",
      "Step: 3706, Loss: 0.074\n",
      "Step: 3707, Loss: 0.074\n",
      "Step: 3708, Loss: 0.074\n",
      "Step: 3709, Loss: 0.074\n",
      "Step: 3710, Loss: 0.074\n",
      "Step: 3711, Loss: 0.074\n",
      "Step: 3712, Loss: 0.074\n",
      "Step: 3713, Loss: 0.074\n",
      "Step: 3714, Loss: 0.074\n",
      "Step: 3715, Loss: 0.074\n",
      "Step: 3716, Loss: 0.074\n",
      "Step: 3717, Loss: 0.074\n",
      "Step: 3718, Loss: 0.074\n",
      "Step: 3719, Loss: 0.074\n",
      "Step: 3720, Loss: 0.075\n",
      "Step: 3721, Loss: 0.074\n",
      "Step: 3722, Loss: 0.074\n",
      "Step: 3723, Loss: 0.074\n",
      "Step: 3724, Loss: 0.074\n",
      "Step: 3725, Loss: 0.074\n",
      "Step: 3726, Loss: 0.074\n",
      "Step: 3727, Loss: 0.074\n",
      "Step: 3728, Loss: 0.073\n",
      "Step: 3729, Loss: 0.074\n",
      "Step: 3730, Loss: 0.074\n",
      "Step: 3731, Loss: 0.074\n",
      "Step: 3732, Loss: 0.073\n",
      "Step: 3733, Loss: 0.074\n",
      "Step: 3734, Loss: 0.074\n",
      "Step: 3735, Loss: 0.074\n",
      "Step: 3736, Loss: 0.074\n",
      "Step: 3737, Loss: 0.074\n",
      "Step: 3738, Loss: 0.074\n",
      "Step: 3739, Loss: 0.073\n",
      "Step: 3740, Loss: 0.074\n",
      "Step: 3741, Loss: 0.074\n",
      "Step: 3742, Loss: 0.074\n",
      "Step: 3743, Loss: 0.074\n",
      "Step: 3744, Loss: 0.074\n",
      "Step: 3745, Loss: 0.074\n",
      "Step: 3746, Loss: 0.073\n",
      "Step: 3747, Loss: 0.074\n",
      "Step: 3748, Loss: 0.074\n",
      "Step: 3749, Loss: 0.074\n",
      "Step: 3750, Loss: 0.073\n",
      "Step: 3751, Loss: 0.074\n",
      "Step: 3752, Loss: 0.074\n",
      "Step: 3753, Loss: 0.074\n",
      "Step: 3754, Loss: 0.073\n",
      "Step: 3755, Loss: 0.074\n",
      "Step: 3756, Loss: 0.074\n",
      "Step: 3757, Loss: 0.073\n",
      "Step: 3758, Loss: 0.074\n",
      "Step: 3759, Loss: 0.074\n",
      "Step: 3760, Loss: 0.074\n",
      "Step: 3761, Loss: 0.073\n",
      "Step: 3762, Loss: 0.074\n",
      "Step: 3763, Loss: 0.074\n",
      "Step: 3764, Loss: 0.073\n",
      "Step: 3765, Loss: 0.074\n",
      "Step: 3766, Loss: 0.074\n",
      "Step: 3767, Loss: 0.074\n",
      "Step: 3768, Loss: 0.073\n",
      "Step: 3769, Loss: 0.074\n",
      "Step: 3770, Loss: 0.074\n",
      "Step: 3771, Loss: 0.074\n",
      "Step: 3772, Loss: 0.073\n",
      "Step: 3773, Loss: 0.074\n",
      "Step: 3774, Loss: 0.074\n",
      "Step: 3775, Loss: 0.073\n",
      "Step: 3776, Loss: 0.073\n",
      "Step: 3777, Loss: 0.074\n",
      "Step: 3778, Loss: 0.074\n",
      "Step: 3779, Loss: 0.073\n",
      "Step: 3780, Loss: 0.074\n",
      "Step: 3781, Loss: 0.074\n",
      "Step: 3782, Loss: 0.073\n",
      "Step: 3783, Loss: 0.073\n",
      "Step: 3784, Loss: 0.073\n",
      "Step: 3785, Loss: 0.074\n",
      "Step: 3786, Loss: 0.073\n",
      "Step: 3787, Loss: 0.073\n",
      "Step: 3788, Loss: 0.073\n",
      "Step: 3789, Loss: 0.074\n",
      "Step: 3790, Loss: 0.073\n",
      "Step: 3791, Loss: 0.073\n",
      "Step: 3792, Loss: 0.073\n",
      "Step: 3793, Loss: 0.073\n",
      "Step: 3794, Loss: 0.073\n",
      "Step: 3795, Loss: 0.073\n",
      "Step: 3796, Loss: 0.074\n",
      "Step: 3797, Loss: 0.073\n",
      "Step: 3798, Loss: 0.073\n",
      "Step: 3799, Loss: 0.073\n",
      "Step: 3800, Loss: 0.073\n",
      "Step: 3801, Loss: 0.073\n",
      "Step: 3802, Loss: 0.073\n",
      "Step: 3803, Loss: 0.074\n",
      "Step: 3804, Loss: 0.073\n",
      "Step: 3805, Loss: 0.073\n",
      "Step: 3806, Loss: 0.073\n",
      "Step: 3807, Loss: 0.074\n",
      "Step: 3808, Loss: 0.073\n",
      "Step: 3809, Loss: 0.073\n",
      "Step: 3810, Loss: 0.073\n",
      "Step: 3811, Loss: 0.072\n",
      "Step: 3812, Loss: 0.073\n",
      "Step: 3813, Loss: 0.073\n",
      "Step: 3814, Loss: 0.073\n",
      "Step: 3815, Loss: 0.073\n",
      "Step: 3816, Loss: 0.073\n",
      "Step: 3817, Loss: 0.073\n",
      "Step: 3818, Loss: 0.073\n",
      "Step: 3819, Loss: 0.073\n",
      "Step: 3820, Loss: 0.073\n",
      "Step: 3821, Loss: 0.073\n",
      "Step: 3822, Loss: 0.072\n",
      "Step: 3823, Loss: 0.073\n",
      "Step: 3824, Loss: 0.073\n",
      "Step: 3825, Loss: 0.073\n",
      "Step: 3826, Loss: 0.073\n",
      "Step: 3827, Loss: 0.073\n",
      "Step: 3828, Loss: 0.073\n",
      "Step: 3829, Loss: 0.072\n",
      "Step: 3830, Loss: 0.073\n",
      "Step: 3831, Loss: 0.073\n",
      "Step: 3832, Loss: 0.073\n",
      "Step: 3833, Loss: 0.072\n",
      "Step: 3834, Loss: 0.073\n",
      "Step: 3835, Loss: 0.073\n",
      "Step: 3836, Loss: 0.073\n",
      "Step: 3837, Loss: 0.072\n",
      "Step: 3838, Loss: 0.073\n",
      "Step: 3839, Loss: 0.073\n",
      "Step: 3840, Loss: 0.072\n",
      "Step: 3841, Loss: 0.073\n",
      "Step: 3842, Loss: 0.073\n",
      "Step: 3843, Loss: 0.073\n",
      "Step: 3844, Loss: 0.072\n",
      "Step: 3845, Loss: 0.073\n",
      "Step: 3846, Loss: 0.073\n",
      "Step: 3847, Loss: 0.072\n",
      "Step: 3848, Loss: 0.073\n",
      "Step: 3849, Loss: 0.073\n",
      "Step: 3850, Loss: 0.073\n",
      "Step: 3851, Loss: 0.072\n",
      "Step: 3852, Loss: 0.072\n",
      "Step: 3853, Loss: 0.073\n",
      "Step: 3854, Loss: 0.073\n",
      "Step: 3855, Loss: 0.072\n",
      "Step: 3856, Loss: 0.073\n",
      "Step: 3857, Loss: 0.073\n",
      "Step: 3858, Loss: 0.072\n",
      "Step: 3859, Loss: 0.073\n",
      "Step: 3860, Loss: 0.073\n",
      "Step: 3861, Loss: 0.073\n",
      "Step: 3862, Loss: 0.072\n",
      "Step: 3863, Loss: 0.073\n",
      "Step: 3864, Loss: 0.073\n",
      "Step: 3865, Loss: 0.072\n",
      "Step: 3866, Loss: 0.072\n",
      "Step: 3867, Loss: 0.073\n",
      "Step: 3868, Loss: 0.073\n",
      "Step: 3869, Loss: 0.072\n",
      "Step: 3870, Loss: 0.072\n",
      "Step: 3871, Loss: 0.073\n",
      "Step: 3872, Loss: 0.073\n",
      "Step: 3873, Loss: 0.072\n",
      "Step: 3874, Loss: 0.072\n",
      "Step: 3875, Loss: 0.073\n",
      "Step: 3876, Loss: 0.072\n",
      "Step: 3877, Loss: 0.072\n",
      "Step: 3878, Loss: 0.073\n",
      "Step: 3879, Loss: 0.073\n",
      "Step: 3880, Loss: 0.072\n",
      "Step: 3881, Loss: 0.072\n",
      "Step: 3882, Loss: 0.072\n",
      "Step: 3883, Loss: 0.072\n",
      "Step: 3884, Loss: 0.072\n",
      "Step: 3885, Loss: 0.073\n",
      "Step: 3886, Loss: 0.072\n",
      "Step: 3887, Loss: 0.072\n",
      "Step: 3888, Loss: 0.072\n",
      "Step: 3889, Loss: 0.072\n",
      "Step: 3890, Loss: 0.072\n",
      "Step: 3891, Loss: 0.072\n",
      "Step: 3892, Loss: 0.072\n",
      "Step: 3893, Loss: 0.072\n",
      "Step: 3894, Loss: 0.072\n",
      "Step: 3895, Loss: 0.072\n",
      "Step: 3896, Loss: 0.072\n",
      "Step: 3897, Loss: 0.073\n",
      "Step: 3898, Loss: 0.072\n",
      "Step: 3899, Loss: 0.072\n",
      "Step: 3900, Loss: 0.072\n",
      "Step: 3901, Loss: 0.072\n",
      "Step: 3902, Loss: 0.072\n",
      "Step: 3903, Loss: 0.072\n",
      "Step: 3904, Loss: 0.072\n",
      "Step: 3905, Loss: 0.071\n",
      "Step: 3906, Loss: 0.072\n",
      "Step: 3907, Loss: 0.072\n",
      "Step: 3908, Loss: 0.072\n",
      "Step: 3909, Loss: 0.071\n",
      "Step: 3910, Loss: 0.072\n",
      "Step: 3911, Loss: 0.072\n",
      "Step: 3912, Loss: 0.072\n",
      "Step: 3913, Loss: 0.072\n",
      "Step: 3914, Loss: 0.072\n",
      "Step: 3915, Loss: 0.072\n",
      "Step: 3916, Loss: 0.071\n",
      "Step: 3917, Loss: 0.072\n",
      "Step: 3918, Loss: 0.072\n",
      "Step: 3919, Loss: 0.072\n",
      "Step: 3920, Loss: 0.072\n",
      "Step: 3921, Loss: 0.072\n",
      "Step: 3922, Loss: 0.072\n",
      "Step: 3923, Loss: 0.071\n",
      "Step: 3924, Loss: 0.072\n",
      "Step: 3925, Loss: 0.072\n",
      "Step: 3926, Loss: 0.072\n",
      "Step: 3927, Loss: 0.071\n",
      "Step: 3928, Loss: 0.072\n",
      "Step: 3929, Loss: 0.072\n",
      "Step: 3930, Loss: 0.072\n",
      "Step: 3931, Loss: 0.071\n",
      "Step: 3932, Loss: 0.072\n",
      "Step: 3933, Loss: 0.072\n",
      "Step: 3934, Loss: 0.071\n",
      "Step: 3935, Loss: 0.072\n",
      "Step: 3936, Loss: 0.072\n",
      "Step: 3937, Loss: 0.072\n",
      "Step: 3938, Loss: 0.071\n",
      "Step: 3939, Loss: 0.072\n",
      "Step: 3940, Loss: 0.072\n",
      "Step: 3941, Loss: 0.071\n",
      "Step: 3942, Loss: 0.072\n",
      "Step: 3943, Loss: 0.072\n",
      "Step: 3944, Loss: 0.072\n",
      "Step: 3945, Loss: 0.071\n",
      "Step: 3946, Loss: 0.072\n",
      "Step: 3947, Loss: 0.072\n",
      "Step: 3948, Loss: 0.072\n",
      "Step: 3949, Loss: 0.071\n",
      "Step: 3950, Loss: 0.072\n",
      "Step: 3951, Loss: 0.072\n",
      "Step: 3952, Loss: 0.071\n",
      "Step: 3953, Loss: 0.071\n",
      "Step: 3954, Loss: 0.072\n",
      "Step: 3955, Loss: 0.072\n",
      "Step: 3956, Loss: 0.071\n",
      "Step: 3957, Loss: 0.072\n",
      "Step: 3958, Loss: 0.072\n",
      "Step: 3959, Loss: 0.071\n",
      "Step: 3960, Loss: 0.071\n",
      "Step: 3961, Loss: 0.072\n",
      "Step: 3962, Loss: 0.072\n",
      "Step: 3963, Loss: 0.071\n",
      "Step: 3964, Loss: 0.071\n",
      "Step: 3965, Loss: 0.071\n",
      "Step: 3966, Loss: 0.072\n",
      "Step: 3967, Loss: 0.071\n",
      "Step: 3968, Loss: 0.072\n",
      "Step: 3969, Loss: 0.072\n",
      "Step: 3970, Loss: 0.071\n",
      "Step: 3971, Loss: 0.071\n",
      "Step: 3972, Loss: 0.071\n",
      "Step: 3973, Loss: 0.072\n",
      "Step: 3974, Loss: 0.071\n",
      "Step: 3975, Loss: 0.072\n",
      "Step: 3976, Loss: 0.071\n",
      "Step: 3977, Loss: 0.071\n",
      "Step: 3978, Loss: 0.071\n",
      "Step: 3979, Loss: 0.072\n",
      "Step: 3980, Loss: 0.072\n",
      "Step: 3981, Loss: 0.071\n",
      "Step: 3982, Loss: 0.071\n",
      "Step: 3983, Loss: 0.071\n",
      "Step: 3984, Loss: 0.071\n",
      "Step: 3985, Loss: 0.071\n",
      "Step: 3986, Loss: 0.071\n",
      "Step: 3987, Loss: 0.071\n",
      "Step: 3988, Loss: 0.071\n",
      "Step: 3989, Loss: 0.071\n",
      "Step: 3990, Loss: 0.071\n",
      "Step: 3991, Loss: 0.071\n",
      "Step: 3992, Loss: 0.071\n",
      "Step: 3993, Loss: 0.071\n",
      "Step: 3994, Loss: 0.071\n",
      "Step: 3995, Loss: 0.071\n",
      "Step: 3996, Loss: 0.071\n",
      "Step: 3997, Loss: 0.071\n",
      "Step: 3998, Loss: 0.071\n",
      "Step: 3999, Loss: 0.071\n",
      "Step: 4000, Loss: 0.071\n"
     ]
    }
   ],
   "source": [
    "set_lr(mm_optimizer, 1e-5)\n",
    "METADRIVE_MODEL_TRAIN_EPOCHS = 4000\n",
    "METADRIVE_MODEL_TRAIN_BATCH_SIZE = 2048\n",
    "\n",
    "while mm_step < METADRIVE_MODEL_TRAIN_EPOCHS:\n",
    "    # take up to n from the data buffer\n",
    "    data_batch = [next(dataset_iter) for _ in range(METADRIVE_MODEL_TRAIN_BATCH_SIZE)]\n",
    "    # unpack the batch\n",
    "    s0_batch = [s0 for s0, _, _ in data_batch]\n",
    "    a_batch = [a for _, a, _ in data_batch]\n",
    "    s1_batch = [s1 for _, _, s1 in data_batch]\n",
    "    loss = train_metadrive_model_step(mm, mm_optimizer, s0_batch, a_batch, s1_batch)\n",
    "    mm_losses.append(loss)\n",
    "    mm_step += 1\n",
    "    print(f\"Step: {mm_step}, Loss: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the losses over training\n",
    "plt.plot(list(range(len(mm_losses))), mm_losses, label='MM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0765836089849472\n"
     ]
    }
   ],
   "source": [
    "s0_batch = [s0 for s0, _, _ in validation_data]\n",
    "s1_batch = [s1 for _, _, s1 in validation_data]\n",
    "a_batch = [a for _, a, _ in validation_data]\n",
    "\n",
    "s0_tensor = state_batch_to_tensor(s0_batch, device)\n",
    "s1_tensor = state_batch_to_tensor(s1_batch, device)\n",
    "a_tensor = action_batch_to_tensor(a_batch, device)\n",
    "\n",
    "s1_pred = mm(s0_tensor, a_tensor)\n",
    "\n",
    "loss = (s1_pred - s1_tensor)**2\n",
    "loss_x = loss[:, 0]\n",
    "loss_y = loss[:, 1]\n",
    "loss_theta = loss[:, 2]\n",
    "\n",
    "print(loss.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s0_batch = [s0 for s0, _, _ in dataset]\n",
    "s1_batch = [s1 for _, _, s1 in dataset]\n",
    "a_batch = [a for _, a, _ in dataset]\n",
    "\n",
    "s0_tensor = state_batch_to_tensor(s0_batch, device)\n",
    "s1_tensor = state_batch_to_tensor(s1_batch, device)\n",
    "a_tensor = action_batch_to_tensor(a_batch, device)\n",
    "\n",
    "s1_pred = mm(s0_tensor, a_tensor)\n",
    "\n",
    "loss = (s1_pred - s1_tensor)**2\n",
    "loss_x = loss[:, 0]\n",
    "loss_y = loss[:, 1]\n",
    "loss_theta = loss[:, 2]\n",
    "\n",
    "print(loss.mean())\n",
    "\n",
    "plt.hist(loss_x.cpu().detach().numpy(), bins=100)\n",
    "plt.title('Loss in x')\n",
    "plt.show()\n",
    "plt.hist(loss_y.cpu().detach().numpy(), bins=100)\n",
    "plt.title('Loss in y')\n",
    "plt.show()\n",
    "plt.hist(loss_theta.cpu().detach().numpy(), bins=100)\n",
    "plt.title('Loss in theta')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1768e-01,  1.8888e-01,  1.2054e-02],\n",
       "        [ 6.2519e-02, -8.3508e-01, -3.2485e-06],\n",
       "        [-4.5734e-01,  7.3288e-01, -1.0580e-04],\n",
       "        [ 5.0727e-01,  4.4587e-01,  1.0980e-03],\n",
       "        [-6.3876e-01,  5.0682e-01,  6.3348e-04],\n",
       "        [ 6.9418e-01,  4.7359e-01, -2.2006e-04],\n",
       "        [ 8.0162e-01,  1.3291e-01,  7.8046e-04],\n",
       "        [-4.9212e-01, -6.5377e-01, -1.0452e-03],\n",
       "        [-8.5614e-01,  9.2770e-02,  7.5459e-05],\n",
       "        [ 8.3968e-01,  6.7720e-02,  5.2452e-06]], device='cuda:0')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s1_tensor - s0_tensor)[10:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5685, -0.0314,  0.0084],\n",
       "        [-0.3019, -0.5145, -0.0263],\n",
       "        [-0.3720,  0.2469,  0.0752],\n",
       "        [-0.1301,  0.1892, -0.0452],\n",
       "        [-0.2383,  0.5546,  0.0334],\n",
       "        [ 0.5607,  0.4675, -0.0051],\n",
       "        [ 0.0705, -0.3087,  0.0113],\n",
       "        [-0.4600, -0.6457,  0.0402],\n",
       "        [-0.6262,  0.4614,  0.0460],\n",
       "        [ 0.5090,  0.4448, -0.0358]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s1_pred - s0_tensor)[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3508, -0.2203, -0.0036],\n",
       "        [-0.3644,  0.3205, -0.0263],\n",
       "        [ 0.0854, -0.4860,  0.0753],\n",
       "        [-0.6374, -0.2566, -0.0463],\n",
       "        [ 0.4004,  0.0478,  0.0327],\n",
       "        [-0.1334, -0.0061, -0.0049],\n",
       "        [-0.7312, -0.4416,  0.0106],\n",
       "        [ 0.0321,  0.0081,  0.0412],\n",
       "        [ 0.2300,  0.3686,  0.0459],\n",
       "        [-0.3307,  0.3771, -0.0358]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s1_pred - s1_tensor)[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a neural network model of Metadrive through which we can backprop, training the IDM is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a model that attempts to predict the action given the current state and the next state\n",
    "class InverseDynamicsModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input shape: (batch_size, 3, 2)\n",
    "        # output shape: (batch_size, 2)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(3, 512, 2) # Bx3x2 -> Bx512x1\n",
    "        self.fc1 = nn.Linear(512, 256) # Bx512 -> Bx256\n",
    "        self.fc2 = nn.Linear(256, 256) # Bx256 -> Bx256\n",
    "        self.fc3 = nn.Linear(256, 2) # Bx256 -> Bx2\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = F.relu(self.conv1(x)) # Bx3x2 -> Bx512x1\n",
    "        x = torch.flatten(x, 1) # Bx512x1 -> Bx512\n",
    "        x = F.relu(self.fc1(x)) # Bx512 -> Bx256\n",
    "        x = F.relu(self.fc2(x)) # Bx256 -> Bx256\n",
    "        x = self.fc3(x) # Bx256 -> Bx2\n",
    "        return x\n",
    "\n",
    "def train_idm_step(\n",
    "        mm: MetadriveModel,\n",
    "        idm: InverseDynamicsModel,\n",
    "        idm_optimizer: torch.optim.Optimizer,\n",
    "        obs_batch: list[Observation],\n",
    ") -> float:\n",
    "    device = deviceof(mm)\n",
    "\n",
    "    assert deviceof(idm) == device\n",
    "\n",
    "    obs_tensor = obs_batch_to_tensor(obs_batch, device)\n",
    "    s0_batch = state_batch_to_tensor([s0 for s0, _ in obs_batch], device)\n",
    "    s1_batch = state_batch_to_tensor([s1 for _, s1 in obs_batch], device)\n",
    "\n",
    "    idm_optimizer.zero_grad()\n",
    "\n",
    "    pred_action = idm(obs_tensor)\n",
    "    pred_s1 = mm(s0_batch, pred_action)\n",
    "\n",
    "    loss = F.mse_loss(pred_s1, s1_batch)\n",
    "    loss.backward()\n",
    "\n",
    "    idm_optimizer.step()\n",
    "\n",
    "    return float(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 19:36:32.198575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 19:36:33.251851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]2023-08-10 19:36:34.065985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.085071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.085294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.087637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.087862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.088026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.647045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.647254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.647427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-10 19:36:34.647561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21543 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1d:00.0, compute capability: 8.6\n",
      "  2%|▏         | 2/100 [01:27<1:11:10, 43.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m TFRecordDataset(file_path, compression_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mas_numpy_iterator():\n\u001b[1;32m     26\u001b[0m     scenario \u001b[39m=\u001b[39m scenario_pb2\u001b[39m.\u001b[39mScenario()\n\u001b[0;32m---> 27\u001b[0m     scenario\u001b[39m.\u001b[39;49mParseFromString(data)\n\u001b[1;32m     28\u001b[0m     h\u001b[39m.\u001b[39mappend(parse_scenario(scenario))\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/message.py:202\u001b[0m, in \u001b[0;36mMessage.ParseFromString\u001b[0;34m(self, serialized)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse serialized protocol buffer data into this message.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \n\u001b[1;32m    196\u001b[0m \u001b[39mLike :func:`MergeFromString()`, except we clear the object first.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m  message.DecodeError if the input cannot be parsed.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mClear()\n\u001b[0;32m--> 202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMergeFromString(serialized)\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1128\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.MergeFromString\u001b[0;34m(self, serialized)\u001b[0m\n\u001b[1;32m   1126\u001b[0m length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(serialized)\n\u001b[1;32m   1127\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_InternalParse(serialized, \u001b[39m0\u001b[39;49m, length) \u001b[39m!=\u001b[39m length:\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m message_mod\u001b[39m.\u001b[39mDecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1132\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m   1133\u001b[0m   \u001b[39m# Now ord(buf[p:p+1]) == ord('') gets TypeError.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1193\u001b[0m   pos \u001b[39m=\u001b[39m new_pos\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m   pos \u001b[39m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[39mself\u001b[39;49m, field_dict)\n\u001b[1;32m   1196\u001b[0m   \u001b[39mif\u001b[39;00m field_desc:\n\u001b[1;32m   1197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/decoder.py:705\u001b[0m, in \u001b[0;36mMessageDecoder.<locals>.DecodeRepeatedField\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mTruncated message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    704\u001b[0m \u001b[39m# Read sub-message.\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39;49madd()\u001b[39m.\u001b[39;49m_InternalParse(buffer, pos, new_pos) \u001b[39m!=\u001b[39m new_pos:\n\u001b[1;32m    706\u001b[0m   \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m    707\u001b[0m   \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    709\u001b[0m \u001b[39m# Predict that the next tag is another copy of the same repeated field.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1193\u001b[0m   pos \u001b[39m=\u001b[39m new_pos\n\u001b[1;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m   pos \u001b[39m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[39mself\u001b[39;49m, field_dict)\n\u001b[1;32m   1196\u001b[0m   \u001b[39mif\u001b[39;00m field_desc:\n\u001b[1;32m   1197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/decoder.py:705\u001b[0m, in \u001b[0;36mMessageDecoder.<locals>.DecodeRepeatedField\u001b[0;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mTruncated message.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    704\u001b[0m \u001b[39m# Read sub-message.\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39;49madd()\u001b[39m.\u001b[39;49m_InternalParse(buffer, pos, new_pos) \u001b[39m!=\u001b[39m new_pos:\n\u001b[1;32m    706\u001b[0m   \u001b[39m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[1;32m    707\u001b[0m   \u001b[39m# encountered an end-group tag.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m   \u001b[39mraise\u001b[39;00m _DecodeError(\u001b[39m'\u001b[39m\u001b[39mUnexpected end-group tag.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    709\u001b[0m \u001b[39m# Predict that the next tag is another copy of the same repeated field.\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1164\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[0;34m(self, buffer, pos, end)\u001b[0m\n\u001b[1;32m   1162\u001b[0m unknown_field_set \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_field_set\n\u001b[1;32m   1163\u001b[0m \u001b[39mwhile\u001b[39;00m pos \u001b[39m!=\u001b[39m end:\n\u001b[0;32m-> 1164\u001b[0m   (tag_bytes, new_pos) \u001b[39m=\u001b[39m local_ReadTag(buffer, pos)\n\u001b[1;32m   1165\u001b[0m   field_decoder, field_desc \u001b[39m=\u001b[39m decoders_by_tag\u001b[39m.\u001b[39mget(tag_bytes, (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m   1166\u001b[0m   \u001b[39mif\u001b[39;00m field_decoder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/metadrive/lib/python3.11/site-packages/google/protobuf/internal/decoder.py:178\u001b[0m, in \u001b[0;36mReadTag\u001b[0;34m(buffer, pos)\u001b[0m\n\u001b[1;32m    175\u001b[0m   pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    176\u001b[0m pos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 178\u001b[0m tag_bytes \u001b[39m=\u001b[39m buffer[start:pos]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m tag_bytes, pos\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from protos import scenario_pb2\n",
    "from tensorflow.data import TFRecordDataset\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "def getFiles(path: str) -> list[str]:\n",
    "    path = os.path.expanduser(path)\n",
    "    files = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "    return [f for f in files if os.path.isfile(f)]\n",
    "\n",
    "\n",
    "files = getFiles('~/data/waymo/')\n",
    "\n",
    "def parse_scenario(scenario: scenario_pb2.Scenario) -> list[State]:\n",
    "    states = []\n",
    "    for s in scenario.tracks[scenario.sdc_track_index].states:\n",
    "        if s.valid:\n",
    "            states.append(State(s.heading, np.array([s.velocity_x, s.velocity_y], dtype=np.float32)))\n",
    "    return states\n",
    "\n",
    "\n",
    "h: list[list[State]] = []\n",
    "\n",
    "for file_path in tqdm.tqdm(files):\n",
    "    for data in TFRecordDataset(file_path, compression_type=\"\").as_numpy_iterator():\n",
    "        scenario = scenario_pb2.Scenario()\n",
    "        scenario.ParseFromString(data)\n",
    "        h.append(parse_scenario(scenario))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: list[Observation] = []\n",
    "for states in h:\n",
    "    for i in range(len(states)-1):\n",
    "        dataset.append((states[i], states[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idm = InverseDynamicsModel().to(device)\n",
    "\n",
    "idm_optimizer = torch.optim.Adam(idm.parameters())\n",
    "\n",
    "idm_step = 0\n",
    "idm_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lr(idm_optimizer, 1e-4)\n",
    "INVERSE_DYNAMICS_MODEL_TRAIN_EPOCHS = 1000\n",
    "INVERSE_DYNAMICS_MODEL_TRAIN_BATCH_SIZE = 128\n",
    "\n",
    "while idm_step < INVERSE_DYNAMICS_MODEL_TRAIN_EPOCHS:\n",
    "    idm_step += 1\n",
    "    idm_batch = idm_train_ds.sample(INVERSE_DYNAMICS_MODEL_TRAIN_BATCH_SIZE)\n",
    "    idm_loss = idm_train_step(idm_batch)\n",
    "    if idm_step % 50 == 0:\n",
    "        print(f\"IDM Step {idm_step}: {idm_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
